<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.utils.utils API documentation</title>
<meta name="description" content="General utilities for all pipelines." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.utils.utils</code></h1>
</header>
<section id="section-intro">
<p>General utilities for all pipelines.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
General utilities for all pipelines.
&#34;&#34;&#34;

import base64
from datetime import datetime
import json
import logging
from os import getenv, walk
from os.path import join
from pathlib import Path
import re
from typing import Any, Dict, List, Optional, Tuple, Union
from uuid import uuid4

import basedosdados as bd
import croniter
from google.cloud import storage
from google.cloud.storage.blob import Blob
from google.oauth2 import service_account
import hvac
import numpy as np
import pandas as pd
import prefect
from prefect.client import Client
from prefect.engine.state import State
from prefect.run_configs import KubernetesRun
from redis_pal import RedisPal
import requests
import telegram

from pipelines.constants import constants


def log(msg: Any, level: str = &#34;info&#34;) -&gt; None:
    &#34;&#34;&#34;
    Logs a message to prefect&#39;s logger.
    &#34;&#34;&#34;
    levels = {
        &#34;debug&#34;: logging.DEBUG,
        &#34;info&#34;: logging.INFO,
        &#34;warning&#34;: logging.WARNING,
        &#34;error&#34;: logging.ERROR,
        &#34;critical&#34;: logging.CRITICAL,
    }

    blank_spaces = 8 * &#34; &#34;
    msg = blank_spaces + &#34;----\n&#34; + str(msg)
    msg = &#34;\n&#34;.join([blank_spaces + line for line in msg.split(&#34;\n&#34;)]) + &#34;\n\n&#34;

    if level not in levels:
        raise ValueError(f&#34;Invalid log level: {level}&#34;)
    prefect.context.logger.log(levels[level], msg)  # pylint: disable=E1101


###############
#
# Datetime utils
#
###############


def determine_whether_to_execute_or_not(
    cron_expression: str, datetime_now: datetime, datetime_last_execution: datetime
) -&gt; bool:
    &#34;&#34;&#34;
    Determines whether the cron expression is currently valid.

    Args:
        cron_expression: The cron expression to check.
        datetime_now: The current datetime.
        datetime_last_execution: The last datetime the cron expression was executed.

    Returns:
        True if the cron expression should trigger, False otherwise.
    &#34;&#34;&#34;
    cron_expression_iterator = croniter.croniter(
        cron_expression, datetime_last_execution
    )
    next_cron_expression_time = cron_expression_iterator.get_next(datetime)
    if next_cron_expression_time &lt;= datetime_now:
        return True
    return False


###############
#
# Redis
#
###############


def get_redis_client(
    host: str = &#34;redis.redis.svc.cluster.local&#34;,
    port: int = 6379,
    db: int = 0,  # pylint: disable=C0103
    password: str = None,
) -&gt; RedisPal:
    &#34;&#34;&#34;
    Returns a Redis client.
    &#34;&#34;&#34;
    return RedisPal(
        host=host,
        port=port,
        db=db,
        password=password,
    )


def get_vault_client() -&gt; hvac.Client:
    &#34;&#34;&#34;
    Returns a Vault client.
    &#34;&#34;&#34;
    return hvac.Client(
        url=getenv(&#34;VAULT_ADDRESS&#34;).strip(),
        token=getenv(&#34;VAULT_TOKEN&#34;).strip(),
    )


def get_vault_secret(secret_path: str, client: hvac.Client = None) -&gt; dict:
    &#34;&#34;&#34;
    Returns a secret from Vault.
    &#34;&#34;&#34;
    vault_client = client or get_vault_client()
    return vault_client.secrets.kv.read_secret_version(secret_path)[&#34;data&#34;]


def get_username_and_password_from_secret(
    secret_path: str,
    client: hvac.Client = None,
) -&gt; Tuple[str, str]:
    &#34;&#34;&#34;
    Returns a username and password from a secret in Vault.
    &#34;&#34;&#34;
    secret = get_vault_secret(secret_path, client)
    return (
        secret[&#34;data&#34;][&#34;username&#34;],
        secret[&#34;data&#34;][&#34;password&#34;],
    )


def notify_discord_on_failure(
    flow: prefect.Flow,
    state: State,
    secret_path: str,
    code_owners: Optional[List[str]] = None,
):
    &#34;&#34;&#34;
    Notifies a Discord channel when a flow fails.
    &#34;&#34;&#34;
    url = get_vault_secret(secret_path)[&#34;data&#34;][&#34;url&#34;]
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    code_owners = code_owners or constants.DEFAULT_CODE_OWNERS.value
    code_owner_dict = constants.OWNERS_DISCORD_MENTIONS.value
    at_code_owners = []
    for code_owner in code_owners:
        code_owner_id = code_owner_dict[code_owner][&#34;user_id&#34;]
        code_owner_type = code_owner_dict[code_owner][&#34;type&#34;]

        if code_owner_type == &#34;user&#34;:
            at_code_owners.append(f&#34;    - &lt;@{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;user_nickname&#34;:
            at_code_owners.append(f&#34;    - &lt;@!{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;channel&#34;:
            at_code_owners.append(f&#34;    - &lt;#{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;role&#34;:
            at_code_owners.append(f&#34;    - &lt;@&amp;{code_owner_id}&gt;\n&#34;)

    message = (
        f&#34;:man_facepalming: Flow **{flow.name}** has failed.&#34;
        + f&#39;\n  - State message: *&#34;{state.message}&#34;*&#39;
        + &#34;\n  - Link to the failed flow: &#34;
        + f&#34;https://prefect.dados.rio/flow-run/{flow_run_id}&#34;
        + &#34;\n  - Extra attention:\n&#34;
        + &#34;&#34;.join(at_code_owners)
    )
    send_discord_message(
        message=message,
        webhook_url=url,
    )


def set_default_parameters(
    flow: prefect.Flow, default_parameters: dict
) -&gt; prefect.Flow:
    &#34;&#34;&#34;
    Sets default parameters for a flow.
    &#34;&#34;&#34;
    for parameter in flow.parameters():
        if parameter.name in default_parameters:
            parameter.default = default_parameters[parameter.name]
    return flow


def run_local(flow: prefect.Flow, parameters: Dict[str, Any] = None):
    &#34;&#34;&#34;
    Runs a flow locally.
    &#34;&#34;&#34;
    # Setup for local run
    flow.storage = None
    flow.run_config = None
    flow.schedule = None

    # Run flow
    if parameters:
        return flow.run(parameters=parameters)
    return flow.run()


def run_cloud(
    flow: prefect.Flow,
    labels: List[str],
    parameters: Dict[str, Any] = None,
    run_description: str = &#34;&#34;,
):
    &#34;&#34;&#34;
    Runs a flow on Prefect Server (must have VPN configured).
    &#34;&#34;&#34;
    # Setup no schedule
    flow.schedule = None

    # Change flow name for development and register
    flow.name = f&#34;{flow.name} (development)&#34;
    flow.run_config = KubernetesRun(image=&#34;ghcr.io/prefeitura-rio/prefect-flows:latest&#34;)
    flow_id = flow.register(project_name=&#34;main&#34;, labels=[])

    # Get Prefect Client and submit flow run
    client = Client()
    flow_run_id = client.create_flow_run(
        flow_id=flow_id,
        run_name=f&#34;TEST RUN - {run_description} - {flow.name}&#34;,
        labels=labels,
        parameters=parameters,
    )

    # Print flow run link so user can check it
    print(f&#34;Run submitted: TEST RUN - {run_description} - {flow.name}&#34;)
    print(f&#34;Please check at: https://prefect.dados.rio/flow-run/{flow_run_id}&#34;)


def run_registered(
    flow_id: str,
    labels: List[str],
    parameters: Dict[str, Any] = None,
    run_description: str = &#34;&#34;,
):
    &#34;&#34;&#34;
    Runs an already registered flow on Prefect Server (must have credentials configured).
    &#34;&#34;&#34;
    # Get Prefect Client and submit flow run
    client = Client()
    flow_run_id = client.create_flow_run(
        flow_id=flow_id,
        run_name=f&#34;SUBMITTED REMOTELY - {run_description}&#34;,
        labels=labels,
        parameters=parameters,
    )

    # Print flow run link so user can check it
    print(f&#34;Run submitted: SUBMITTED REMOTELY - {run_description}&#34;)
    print(f&#34;Please check at: https://prefect.dados.rio/flow-run/{flow_run_id}&#34;)


def query_to_line(query: str) -&gt; str:
    &#34;&#34;&#34;
    Converts a query to a line.
    &#34;&#34;&#34;
    return &#34; &#34;.join([line.strip() for line in query.split(&#34;\n&#34;)])


def send_discord_message(
    message: str,
    webhook_url: str,
) -&gt; None:
    &#34;&#34;&#34;
    Sends a message to a Discord channel.
    &#34;&#34;&#34;
    requests.post(
        webhook_url,
        data={&#34;content&#34;: message},
    )


def send_telegram_message(
    message: str,
    token: str,
    chat_id: int,
    parse_mode: str = telegram.ParseMode.HTML,
):
    &#34;&#34;&#34;
    Sends a message to a Telegram chat.
    &#34;&#34;&#34;
    bot = telegram.Bot(token=token)
    bot.send_message(
        chat_id=chat_id,
        text=message,
        parse_mode=parse_mode,
    )


def smart_split(
    text: str,
    max_length: int,
    separator: str = &#34; &#34;,
) -&gt; List[str]:
    &#34;&#34;&#34;
    Splits a string into a list of strings.
    &#34;&#34;&#34;
    if len(text) &lt;= max_length:
        return [text]

    separator_index = text.rfind(separator, 0, max_length)
    if (separator_index &gt;= max_length) or (separator_index == -1):
        raise ValueError(
            f&#39;Cannot split text &#34;{text}&#34; into {max_length}&#39;
            f&#39;characters using separator &#34;{separator}&#34;&#39;
        )

    return [
        text[:separator_index],
        *smart_split(
            text[separator_index + len(separator) :],
            max_length,
            separator,
        ),
    ]


def untuple_clocks(clocks):
    &#34;&#34;&#34;
    Converts a list of tuples to a list of clocks.
    &#34;&#34;&#34;
    return [clock[0] if isinstance(clock, tuple) else clock for clock in clocks]


###############
#
# Text formatting
#
###############


def human_readable(
    value: Union[int, float],
    unit: str = &#34;&#34;,
    unit_prefixes: List[str] = None,
    unit_divider: int = 1000,
    decimal_places: int = 2,
):
    &#34;&#34;&#34;
    Formats a value in a human readable way.
    &#34;&#34;&#34;
    if unit_prefixes is None:
        unit_prefixes = [&#34;&#34;, &#34;k&#34;, &#34;M&#34;, &#34;G&#34;, &#34;T&#34;, &#34;P&#34;, &#34;E&#34;, &#34;Z&#34;, &#34;Y&#34;]
    if value == 0:
        return f&#34;{value}{unit}&#34;
    unit_prefix = unit_prefixes[0]
    for prefix in unit_prefixes[1:]:
        if value &lt; unit_divider:
            break
        unit_prefix = prefix
        value /= unit_divider
    return f&#34;{value:.{decimal_places}f}{unit_prefix}{unit}&#34;


###############
#
# Dataframe
#
###############


def dataframe_to_csv(
    dataframe: pd.DataFrame,
    path: Union[str, Path],
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Writes a dataframe to CSV file.
    &#34;&#34;&#34;
    if build_json_dataframe:
        dataframe = to_json_dataframe(dataframe, key_column=dataframe_key_column)

    # Remove filename from path
    path = Path(path)
    # Create directory if it doesn&#39;t exist
    path.parent.mkdir(parents=True, exist_ok=True)

    # Write dataframe to CSV
    dataframe.to_csv(path, index=False, encoding=&#34;utf-8&#34;)


def dataframe_to_parquet(
    dataframe: pd.DataFrame,
    path: Union[str, Path],
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
):
    &#34;&#34;&#34;
    Writes a dataframe to Parquet file with Schema as STRING.
    &#34;&#34;&#34;
    # Code adapted from
    # https://stackoverflow.com/a/70817689/9944075

    if build_json_dataframe:
        dataframe = to_json_dataframe(dataframe, key_column=dataframe_key_column)

    # If the file already exists, we:
    # - Load it
    # - Merge the new dataframe with the existing one
    if Path(path).exists():
        # Load it
        original_df = pd.read_parquet(path)
        # Merge the new dataframe with the existing one
        dataframe = pd.concat([original_df, dataframe], sort=False)

    # Write dataframe to Parquet
    dataframe.to_parquet(path, engine=&#34;pyarrow&#34;)


def batch_to_dataframe(batch: Tuple[Tuple], columns: List[str]) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Converts a batch of rows to a dataframe.
    &#34;&#34;&#34;
    return pd.DataFrame(batch, columns=columns)


def clean_dataframe(dataframe: pd.DataFrame) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Cleans a dataframe.
    &#34;&#34;&#34;
    for col in dataframe.columns.tolist():
        if dataframe[col].dtype == object:
            try:
                dataframe[col] = (
                    dataframe[col]
                    .astype(str)
                    .str.replace(&#34;\x00&#34;, &#34;&#34;)
                    .replace(&#34;None&#34;, np.nan)
                )
            except Exception as exc:
                print(
                    &#34;Column: &#34;,
                    col,
                    &#34;\nData: &#34;,
                    dataframe[col].tolist(),
                    &#34;\n&#34;,
                    exc,
                )
                raise
    return dataframe


def remove_columns_accents(dataframe: pd.DataFrame) -&gt; list:
    &#34;&#34;&#34;
    Remove accents from dataframe columns.
    &#34;&#34;&#34;
    columns = [str(column) for column in dataframe.columns]
    dataframe.columns = columns
    return list(
        dataframe.columns.str.normalize(&#34;NFKD&#34;)
        .str.encode(&#34;ascii&#34;, errors=&#34;ignore&#34;)
        .str.decode(&#34;utf-8&#34;)
        .map(lambda x: x.strip())
        .str.replace(&#34; &#34;, &#34;_&#34;)
        .str.replace(&#34;/&#34;, &#34;_&#34;)
        .str.replace(&#34;-&#34;, &#34;_&#34;)
        .str.replace(&#34;\a&#34;, &#34;_&#34;)
        .str.replace(&#34;\b&#34;, &#34;_&#34;)
        .str.replace(&#34;\n&#34;, &#34;_&#34;)
        .str.replace(&#34;\t&#34;, &#34;_&#34;)
        .str.replace(&#34;\v&#34;, &#34;_&#34;)
        .str.replace(&#34;\f&#34;, &#34;_&#34;)
        .str.replace(&#34;\r&#34;, &#34;_&#34;)
        .str.lower()
        .map(final_column_treatment)
    )


# pylint: disable=R0913
def to_partitions(
    data: pd.DataFrame,
    partition_columns: List[str],
    savepath: str,
    data_type: str = &#34;csv&#34;,
    suffix: str = None,
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
):  # sourcery skip: raise-specific-error
    &#34;&#34;&#34;Save data in to hive patitions schema, given a dataframe and a list of partition columns.
    Args:
        data (pandas.core.frame.DataFrame): Dataframe to be partitioned.
        partition_columns (list): List of columns to be used as partitions.
        savepath (str, pathlib.PosixPath): folder path to save the partitions
    Exemple:
        data = {
            &#34;ano&#34;: [2020, 2021, 2020, 2021, 2020, 2021, 2021,2025],
            &#34;mes&#34;: [1, 2, 3, 4, 5, 6, 6,9],
            &#34;sigla_uf&#34;: [&#34;SP&#34;, &#34;SP&#34;, &#34;RJ&#34;, &#34;RJ&#34;, &#34;PR&#34;, &#34;PR&#34;, &#34;PR&#34;,&#34;PR&#34;],
            &#34;dado&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;, &#34;e&#34;, &#34;f&#34;, &#34;g&#34;,&#39;h&#39;],
        }
        to_partitions(
            data=pd.DataFrame(data),
            partition_columns=[&#39;ano&#39;,&#39;mes&#39;,&#39;sigla_uf&#39;],
            savepath=&#39;partitions/&#39;
        )
    &#34;&#34;&#34;

    if isinstance(data, (pd.core.frame.DataFrame)):

        savepath = Path(savepath)

        # create unique combinations between partition columns
        unique_combinations = (
            data[partition_columns]
            .drop_duplicates(subset=partition_columns)
            .to_dict(orient=&#34;records&#34;)
        )

        for filter_combination in unique_combinations:
            patitions_values = [
                f&#34;{partition}={value}&#34;
                for partition, value in filter_combination.items()
            ]

            # get filtered data
            df_filter = data.loc[
                data[filter_combination.keys()]
                .isin(filter_combination.values())
                .all(axis=1),
                :,
            ]
            df_filter = df_filter.drop(columns=partition_columns).reset_index(drop=True)

            # create folder tree
            filter_save_path = Path(savepath / &#34;/&#34;.join(patitions_values))
            filter_save_path.mkdir(parents=True, exist_ok=True)
            if suffix is not None:
                file_filter_save_path = (
                    Path(filter_save_path) / f&#34;data_{suffix}.{data_type}&#34;
                )
            else:
                file_filter_save_path = Path(filter_save_path) / f&#34;data.{data_type}&#34;

            if build_json_dataframe:
                df_filter = to_json_dataframe(
                    df_filter, key_column=dataframe_key_column
                )

            if data_type == &#34;csv&#34;:
                # append data to csv
                df_filter.to_csv(
                    file_filter_save_path,
                    index=False,
                    mode=&#34;a&#34;,
                    header=not file_filter_save_path.exists(),
                )
            elif data_type == &#34;parquet&#34;:
                dataframe_to_parquet(dataframe=df_filter, path=file_filter_save_path)
            else:
                raise ValueError(f&#34;Invalid data type: {data_type}&#34;)
    else:
        raise BaseException(&#34;Data need to be a pandas DataFrame&#34;)


def to_json_dataframe(
    dataframe: pd.DataFrame = None,
    csv_path: Union[str, Path] = None,
    key_column: str = None,
    read_csv_kwargs: dict = None,
    save_to: Union[str, Path] = None,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Manipulates a dataframe by keeping key_column and moving every other column
    data to a &#34;content&#34; column in JSON format. Example:

    - Input dataframe: pd.DataFrame({&#34;key&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;], &#34;col1&#34;: [1, 2, 3], &#34;col2&#34;: [4, 5, 6]})
    - Output dataframe: pd.DataFrame({
        &#34;key&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],
        &#34;content&#34;: [{&#34;col1&#34;: 1, &#34;col2&#34;: 4}, {&#34;col1&#34;: 2, &#34;col2&#34;: 5}, {&#34;col1&#34;: 3, &#34;col2&#34;: 6}]
    })
    &#34;&#34;&#34;
    if dataframe is None and not csv_path:
        raise ValueError(&#34;dataframe or dataframe_path is required&#34;)
    if csv_path:
        dataframe = pd.read_csv(csv_path, **read_csv_kwargs)
    if key_column:
        dataframe[&#34;content&#34;] = dataframe.drop(columns=[key_column]).to_dict(
            orient=&#34;records&#34;
        )
        dataframe = dataframe[[&#34;key&#34;, &#34;content&#34;]]
    else:
        dataframe[&#34;content&#34;] = dataframe.to_dict(orient=&#34;records&#34;)
        dataframe = dataframe[[&#34;content&#34;]]
    if save_to:
        dataframe.to_csv(save_to, index=False)
    return dataframe


###############
#
# Storage utils
#
###############


def get_credentials_from_env(
    mode: str = &#34;prod&#34;, scopes: List[str] = None
) -&gt; service_account.Credentials:
    &#34;&#34;&#34;
    Gets credentials from env vars
    &#34;&#34;&#34;
    if mode not in [&#34;prod&#34;, &#34;staging&#34;]:
        raise ValueError(&#34;Mode must be &#39;prod&#39; or &#39;staging&#39;&#34;)
    env: str = getenv(f&#34;BASEDOSDADOS_CREDENTIALS_{mode.upper()}&#34;, &#34;&#34;)
    if env == &#34;&#34;:
        raise ValueError(f&#34;BASEDOSDADOS_CREDENTIALS_{mode.upper()} env var not set!&#34;)
    info: dict = json.loads(base64.b64decode(env))
    cred: service_account.Credentials = (
        service_account.Credentials.from_service_account_info(info)
    )
    if scopes:
        cred = cred.with_scopes(scopes)
    return cred


def get_storage_blobs(dataset_id: str, table_id: str) -&gt; list:
    &#34;&#34;&#34;
    Get all blobs from a table in a dataset.
    &#34;&#34;&#34;

    bd_storage = bd.Storage(dataset_id=dataset_id, table_id=table_id)
    return list(
        bd_storage.client[&#34;storage_staging&#34;]
        .bucket(bd_storage.bucket_name)
        .list_blobs(prefix=f&#34;staging/{bd_storage.dataset_id}/{bd_storage.table_id}/&#34;)
    )


def list_blobs_with_prefix(
    bucket_name: str, prefix: str, mode: str = &#34;prod&#34;
) -&gt; List[Blob]:
    &#34;&#34;&#34;
    Lists all the blobs in the bucket that begin with the prefix.
    This can be used to list all blobs in a &#34;folder&#34;, e.g. &#34;public/&#34;.
    Mode needs to be &#34;prod&#34; or &#34;staging&#34;
    &#34;&#34;&#34;

    credentials = get_credentials_from_env(mode=mode)
    storage_client = storage.Client(credentials=credentials)

    # Note: Client.list_blobs requires at least package version 1.17.0.
    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)

    return list(blobs)


def parser_blobs_to_partition_dict(blobs: list) -&gt; dict:
    &#34;&#34;&#34;
    Extracts the partition information from the blobs.
    &#34;&#34;&#34;

    partitions_dict = {}
    for blob in blobs:
        for folder in blob.name.split(&#34;/&#34;):
            if &#34;=&#34; in folder:
                key = folder.split(&#34;=&#34;)[0]
                value = folder.split(&#34;=&#34;)[1]
                try:
                    partitions_dict[key].append(value)
                except KeyError:
                    partitions_dict[key] = [value]
    return partitions_dict


def dump_header_to_file(data_path: Union[str, Path], data_type: str = &#34;csv&#34;):
    &#34;&#34;&#34;
    Writes a header to a CSV file.
    &#34;&#34;&#34;
    try:
        assert data_type in [&#34;csv&#34;, &#34;parquet&#34;]
    except AssertionError as exc:
        raise ValueError(f&#34;Invalid data type: {data_type}&#34;) from exc
    # Remove filename from path
    path = Path(data_path)
    if not path.is_dir():
        path = path.parent
    # Grab first `data_type` file found
    found: bool = False
    file: str = None
    for subdir, _, filenames in walk(str(path)):
        for fname in filenames:
            if fname.endswith(f&#34;.{data_type}&#34;):
                file = join(subdir, fname)
                log(f&#34;Found {data_type.upper()} file: {file}&#34;)
                found = True
                break
        if found:
            break

    save_header_path = f&#34;data/{uuid4()}&#34;
    # discover if it&#39;s a partitioned table
    if partition_folders := [folder for folder in file.split(&#34;/&#34;) if &#34;=&#34; in folder]:
        partition_path = &#34;/&#34;.join(partition_folders)
        save_header_file_path = Path(
            f&#34;{save_header_path}/{partition_path}/header.{data_type}&#34;
        )
        log(f&#34;Found partition path: {save_header_file_path}&#34;)

    else:
        save_header_file_path = Path(f&#34;{save_header_path}/header.{data_type}&#34;)
        log(f&#34;Do not found partition path: {save_header_file_path}&#34;)

    # Create directory if it doesn&#39;t exist
    save_header_file_path.parent.mkdir(parents=True, exist_ok=True)

    # Read just first row and write dataframe to file
    if data_type == &#34;csv&#34;:
        dataframe = pd.read_csv(file, nrows=1)
        dataframe.to_csv(save_header_file_path, index=False, encoding=&#34;utf-8&#34;)
    elif data_type == &#34;parquet&#34;:
        dataframe = pd.read_parquet(file)[:1]
        dataframe_to_parquet(dataframe=dataframe, path=save_header_file_path)

    log(f&#34;Wrote {data_type.upper()} header at {save_header_file_path}&#34;)

    return save_header_path


def parse_date_columns(
    dataframe: pd.DataFrame, partition_date_column: str
) -&gt; Tuple[pd.DataFrame, List[str]]:
    &#34;&#34;&#34;
    Parses the date columns to the partition format.
    &#34;&#34;&#34;
    ano_col = &#34;ano_particao&#34;
    mes_col = &#34;mes_particao&#34;
    data_col = &#34;data_particao&#34;
    cols = [ano_col, mes_col, data_col]
    for col in cols:
        if col in dataframe.columns:
            raise ValueError(f&#34;Column {col} already exists, please review your model.&#34;)
    dataframe[partition_date_column] = dataframe[partition_date_column].astype(str)
    dataframe[data_col] = pd.to_datetime(dataframe[partition_date_column])
    dataframe[ano_col] = dataframe[data_col].dt.year
    dataframe[mes_col] = dataframe[data_col].dt.month
    dataframe[data_col] = dataframe[data_col].dt.date

    return dataframe, [ano_col, mes_col, data_col]


def final_column_treatment(column: str) -&gt; str:
    &#34;&#34;&#34;
    Adds an underline before column name if it only has numbers or remove all non alpha numeric
    characters besides underlines (&#34;_&#34;).
    &#34;&#34;&#34;
    try:
        int(column)
        return f&#34;_{column}&#34;
    except ValueError:  # pylint: disable=bare-except
        non_alpha_removed = re.sub(r&#34;[\W]+&#34;, &#34;&#34;, column)
        return non_alpha_removed</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.utils.utils.batch_to_dataframe"><code class="name flex">
<span>def <span class="ident">batch_to_dataframe</span></span>(<span>batch: Tuple[Tuple], columns: List[str]) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a batch of rows to a dataframe.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_to_dataframe(batch: Tuple[Tuple], columns: List[str]) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Converts a batch of rows to a dataframe.
    &#34;&#34;&#34;
    return pd.DataFrame(batch, columns=columns)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.clean_dataframe"><code class="name flex">
<span>def <span class="ident">clean_dataframe</span></span>(<span>dataframe: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Cleans a dataframe.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_dataframe(dataframe: pd.DataFrame) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Cleans a dataframe.
    &#34;&#34;&#34;
    for col in dataframe.columns.tolist():
        if dataframe[col].dtype == object:
            try:
                dataframe[col] = (
                    dataframe[col]
                    .astype(str)
                    .str.replace(&#34;\x00&#34;, &#34;&#34;)
                    .replace(&#34;None&#34;, np.nan)
                )
            except Exception as exc:
                print(
                    &#34;Column: &#34;,
                    col,
                    &#34;\nData: &#34;,
                    dataframe[col].tolist(),
                    &#34;\n&#34;,
                    exc,
                )
                raise
    return dataframe</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.dataframe_to_csv"><code class="name flex">
<span>def <span class="ident">dataframe_to_csv</span></span>(<span>dataframe: pandas.core.frame.DataFrame, path: Union[str, pathlib.Path], build_json_dataframe: bool = False, dataframe_key_column: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Writes a dataframe to CSV file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataframe_to_csv(
    dataframe: pd.DataFrame,
    path: Union[str, Path],
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Writes a dataframe to CSV file.
    &#34;&#34;&#34;
    if build_json_dataframe:
        dataframe = to_json_dataframe(dataframe, key_column=dataframe_key_column)

    # Remove filename from path
    path = Path(path)
    # Create directory if it doesn&#39;t exist
    path.parent.mkdir(parents=True, exist_ok=True)

    # Write dataframe to CSV
    dataframe.to_csv(path, index=False, encoding=&#34;utf-8&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.dataframe_to_parquet"><code class="name flex">
<span>def <span class="ident">dataframe_to_parquet</span></span>(<span>dataframe: pandas.core.frame.DataFrame, path: Union[str, pathlib.Path], build_json_dataframe: bool = False, dataframe_key_column: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes a dataframe to Parquet file with Schema as STRING.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataframe_to_parquet(
    dataframe: pd.DataFrame,
    path: Union[str, Path],
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
):
    &#34;&#34;&#34;
    Writes a dataframe to Parquet file with Schema as STRING.
    &#34;&#34;&#34;
    # Code adapted from
    # https://stackoverflow.com/a/70817689/9944075

    if build_json_dataframe:
        dataframe = to_json_dataframe(dataframe, key_column=dataframe_key_column)

    # If the file already exists, we:
    # - Load it
    # - Merge the new dataframe with the existing one
    if Path(path).exists():
        # Load it
        original_df = pd.read_parquet(path)
        # Merge the new dataframe with the existing one
        dataframe = pd.concat([original_df, dataframe], sort=False)

    # Write dataframe to Parquet
    dataframe.to_parquet(path, engine=&#34;pyarrow&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.determine_whether_to_execute_or_not"><code class="name flex">
<span>def <span class="ident">determine_whether_to_execute_or_not</span></span>(<span>cron_expression: str, datetime_now: datetime.datetime, datetime_last_execution: datetime.datetime) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Determines whether the cron expression is currently valid.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cron_expression</code></strong></dt>
<dd>The cron expression to check.</dd>
<dt><strong><code>datetime_now</code></strong></dt>
<dd>The current datetime.</dd>
<dt><strong><code>datetime_last_execution</code></strong></dt>
<dd>The last datetime the cron expression was executed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if the cron expression should trigger, False otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_whether_to_execute_or_not(
    cron_expression: str, datetime_now: datetime, datetime_last_execution: datetime
) -&gt; bool:
    &#34;&#34;&#34;
    Determines whether the cron expression is currently valid.

    Args:
        cron_expression: The cron expression to check.
        datetime_now: The current datetime.
        datetime_last_execution: The last datetime the cron expression was executed.

    Returns:
        True if the cron expression should trigger, False otherwise.
    &#34;&#34;&#34;
    cron_expression_iterator = croniter.croniter(
        cron_expression, datetime_last_execution
    )
    next_cron_expression_time = cron_expression_iterator.get_next(datetime)
    if next_cron_expression_time &lt;= datetime_now:
        return True
    return False</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.dump_header_to_file"><code class="name flex">
<span>def <span class="ident">dump_header_to_file</span></span>(<span>data_path: Union[str, pathlib.Path], data_type: str = 'csv')</span>
</code></dt>
<dd>
<div class="desc"><p>Writes a header to a CSV file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_header_to_file(data_path: Union[str, Path], data_type: str = &#34;csv&#34;):
    &#34;&#34;&#34;
    Writes a header to a CSV file.
    &#34;&#34;&#34;
    try:
        assert data_type in [&#34;csv&#34;, &#34;parquet&#34;]
    except AssertionError as exc:
        raise ValueError(f&#34;Invalid data type: {data_type}&#34;) from exc
    # Remove filename from path
    path = Path(data_path)
    if not path.is_dir():
        path = path.parent
    # Grab first `data_type` file found
    found: bool = False
    file: str = None
    for subdir, _, filenames in walk(str(path)):
        for fname in filenames:
            if fname.endswith(f&#34;.{data_type}&#34;):
                file = join(subdir, fname)
                log(f&#34;Found {data_type.upper()} file: {file}&#34;)
                found = True
                break
        if found:
            break

    save_header_path = f&#34;data/{uuid4()}&#34;
    # discover if it&#39;s a partitioned table
    if partition_folders := [folder for folder in file.split(&#34;/&#34;) if &#34;=&#34; in folder]:
        partition_path = &#34;/&#34;.join(partition_folders)
        save_header_file_path = Path(
            f&#34;{save_header_path}/{partition_path}/header.{data_type}&#34;
        )
        log(f&#34;Found partition path: {save_header_file_path}&#34;)

    else:
        save_header_file_path = Path(f&#34;{save_header_path}/header.{data_type}&#34;)
        log(f&#34;Do not found partition path: {save_header_file_path}&#34;)

    # Create directory if it doesn&#39;t exist
    save_header_file_path.parent.mkdir(parents=True, exist_ok=True)

    # Read just first row and write dataframe to file
    if data_type == &#34;csv&#34;:
        dataframe = pd.read_csv(file, nrows=1)
        dataframe.to_csv(save_header_file_path, index=False, encoding=&#34;utf-8&#34;)
    elif data_type == &#34;parquet&#34;:
        dataframe = pd.read_parquet(file)[:1]
        dataframe_to_parquet(dataframe=dataframe, path=save_header_file_path)

    log(f&#34;Wrote {data_type.upper()} header at {save_header_file_path}&#34;)

    return save_header_path</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.final_column_treatment"><code class="name flex">
<span>def <span class="ident">final_column_treatment</span></span>(<span>column: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Adds an underline before column name if it only has numbers or remove all non alpha numeric
characters besides underlines ("_").</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def final_column_treatment(column: str) -&gt; str:
    &#34;&#34;&#34;
    Adds an underline before column name if it only has numbers or remove all non alpha numeric
    characters besides underlines (&#34;_&#34;).
    &#34;&#34;&#34;
    try:
        int(column)
        return f&#34;_{column}&#34;
    except ValueError:  # pylint: disable=bare-except
        non_alpha_removed = re.sub(r&#34;[\W]+&#34;, &#34;&#34;, column)
        return non_alpha_removed</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_credentials_from_env"><code class="name flex">
<span>def <span class="ident">get_credentials_from_env</span></span>(<span>mode: str = 'prod', scopes: List[str] = None) ‑> google.oauth2.service_account.Credentials</span>
</code></dt>
<dd>
<div class="desc"><p>Gets credentials from env vars</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_credentials_from_env(
    mode: str = &#34;prod&#34;, scopes: List[str] = None
) -&gt; service_account.Credentials:
    &#34;&#34;&#34;
    Gets credentials from env vars
    &#34;&#34;&#34;
    if mode not in [&#34;prod&#34;, &#34;staging&#34;]:
        raise ValueError(&#34;Mode must be &#39;prod&#39; or &#39;staging&#39;&#34;)
    env: str = getenv(f&#34;BASEDOSDADOS_CREDENTIALS_{mode.upper()}&#34;, &#34;&#34;)
    if env == &#34;&#34;:
        raise ValueError(f&#34;BASEDOSDADOS_CREDENTIALS_{mode.upper()} env var not set!&#34;)
    info: dict = json.loads(base64.b64decode(env))
    cred: service_account.Credentials = (
        service_account.Credentials.from_service_account_info(info)
    )
    if scopes:
        cred = cred.with_scopes(scopes)
    return cred</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_redis_client"><code class="name flex">
<span>def <span class="ident">get_redis_client</span></span>(<span>host: str = 'redis.redis.svc.cluster.local', port: int = 6379, db: int = 0, password: str = None) ‑> redis_pal.RedisPal.RedisPal</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Redis client.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_redis_client(
    host: str = &#34;redis.redis.svc.cluster.local&#34;,
    port: int = 6379,
    db: int = 0,  # pylint: disable=C0103
    password: str = None,
) -&gt; RedisPal:
    &#34;&#34;&#34;
    Returns a Redis client.
    &#34;&#34;&#34;
    return RedisPal(
        host=host,
        port=port,
        db=db,
        password=password,
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_storage_blobs"><code class="name flex">
<span>def <span class="ident">get_storage_blobs</span></span>(<span>dataset_id: str, table_id: str) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Get all blobs from a table in a dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_storage_blobs(dataset_id: str, table_id: str) -&gt; list:
    &#34;&#34;&#34;
    Get all blobs from a table in a dataset.
    &#34;&#34;&#34;

    bd_storage = bd.Storage(dataset_id=dataset_id, table_id=table_id)
    return list(
        bd_storage.client[&#34;storage_staging&#34;]
        .bucket(bd_storage.bucket_name)
        .list_blobs(prefix=f&#34;staging/{bd_storage.dataset_id}/{bd_storage.table_id}/&#34;)
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_username_and_password_from_secret"><code class="name flex">
<span>def <span class="ident">get_username_and_password_from_secret</span></span>(<span>secret_path: str, client: hvac.v1.Client = None) ‑> Tuple[str, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a username and password from a secret in Vault.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_username_and_password_from_secret(
    secret_path: str,
    client: hvac.Client = None,
) -&gt; Tuple[str, str]:
    &#34;&#34;&#34;
    Returns a username and password from a secret in Vault.
    &#34;&#34;&#34;
    secret = get_vault_secret(secret_path, client)
    return (
        secret[&#34;data&#34;][&#34;username&#34;],
        secret[&#34;data&#34;][&#34;password&#34;],
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_vault_client"><code class="name flex">
<span>def <span class="ident">get_vault_client</span></span>(<span>) ‑> hvac.v1.Client</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Vault client.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vault_client() -&gt; hvac.Client:
    &#34;&#34;&#34;
    Returns a Vault client.
    &#34;&#34;&#34;
    return hvac.Client(
        url=getenv(&#34;VAULT_ADDRESS&#34;).strip(),
        token=getenv(&#34;VAULT_TOKEN&#34;).strip(),
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.get_vault_secret"><code class="name flex">
<span>def <span class="ident">get_vault_secret</span></span>(<span>secret_path: str, client: hvac.v1.Client = None) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a secret from Vault.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vault_secret(secret_path: str, client: hvac.Client = None) -&gt; dict:
    &#34;&#34;&#34;
    Returns a secret from Vault.
    &#34;&#34;&#34;
    vault_client = client or get_vault_client()
    return vault_client.secrets.kv.read_secret_version(secret_path)[&#34;data&#34;]</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.human_readable"><code class="name flex">
<span>def <span class="ident">human_readable</span></span>(<span>value: Union[int, float], unit: str = '', unit_prefixes: List[str] = None, unit_divider: int = 1000, decimal_places: int = 2)</span>
</code></dt>
<dd>
<div class="desc"><p>Formats a value in a human readable way.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def human_readable(
    value: Union[int, float],
    unit: str = &#34;&#34;,
    unit_prefixes: List[str] = None,
    unit_divider: int = 1000,
    decimal_places: int = 2,
):
    &#34;&#34;&#34;
    Formats a value in a human readable way.
    &#34;&#34;&#34;
    if unit_prefixes is None:
        unit_prefixes = [&#34;&#34;, &#34;k&#34;, &#34;M&#34;, &#34;G&#34;, &#34;T&#34;, &#34;P&#34;, &#34;E&#34;, &#34;Z&#34;, &#34;Y&#34;]
    if value == 0:
        return f&#34;{value}{unit}&#34;
    unit_prefix = unit_prefixes[0]
    for prefix in unit_prefixes[1:]:
        if value &lt; unit_divider:
            break
        unit_prefix = prefix
        value /= unit_divider
    return f&#34;{value:.{decimal_places}f}{unit_prefix}{unit}&#34;</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.list_blobs_with_prefix"><code class="name flex">
<span>def <span class="ident">list_blobs_with_prefix</span></span>(<span>bucket_name: str, prefix: str, mode: str = 'prod') ‑> List[google.cloud.storage.blob.Blob]</span>
</code></dt>
<dd>
<div class="desc"><p>Lists all the blobs in the bucket that begin with the prefix.
This can be used to list all blobs in a "folder", e.g. "public/".
Mode needs to be "prod" or "staging"</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_blobs_with_prefix(
    bucket_name: str, prefix: str, mode: str = &#34;prod&#34;
) -&gt; List[Blob]:
    &#34;&#34;&#34;
    Lists all the blobs in the bucket that begin with the prefix.
    This can be used to list all blobs in a &#34;folder&#34;, e.g. &#34;public/&#34;.
    Mode needs to be &#34;prod&#34; or &#34;staging&#34;
    &#34;&#34;&#34;

    credentials = get_credentials_from_env(mode=mode)
    storage_client = storage.Client(credentials=credentials)

    # Note: Client.list_blobs requires at least package version 1.17.0.
    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)

    return list(blobs)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>msg: Any, level: str = 'info') ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Logs a message to prefect's logger.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(msg: Any, level: str = &#34;info&#34;) -&gt; None:
    &#34;&#34;&#34;
    Logs a message to prefect&#39;s logger.
    &#34;&#34;&#34;
    levels = {
        &#34;debug&#34;: logging.DEBUG,
        &#34;info&#34;: logging.INFO,
        &#34;warning&#34;: logging.WARNING,
        &#34;error&#34;: logging.ERROR,
        &#34;critical&#34;: logging.CRITICAL,
    }

    blank_spaces = 8 * &#34; &#34;
    msg = blank_spaces + &#34;----\n&#34; + str(msg)
    msg = &#34;\n&#34;.join([blank_spaces + line for line in msg.split(&#34;\n&#34;)]) + &#34;\n\n&#34;

    if level not in levels:
        raise ValueError(f&#34;Invalid log level: {level}&#34;)
    prefect.context.logger.log(levels[level], msg)  # pylint: disable=E1101</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.notify_discord_on_failure"><code class="name flex">
<span>def <span class="ident">notify_discord_on_failure</span></span>(<span>flow: prefect.core.flow.Flow, state: prefect.engine.state.State, secret_path: str, code_owners: Optional[List[str]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Notifies a Discord channel when a flow fails.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def notify_discord_on_failure(
    flow: prefect.Flow,
    state: State,
    secret_path: str,
    code_owners: Optional[List[str]] = None,
):
    &#34;&#34;&#34;
    Notifies a Discord channel when a flow fails.
    &#34;&#34;&#34;
    url = get_vault_secret(secret_path)[&#34;data&#34;][&#34;url&#34;]
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    code_owners = code_owners or constants.DEFAULT_CODE_OWNERS.value
    code_owner_dict = constants.OWNERS_DISCORD_MENTIONS.value
    at_code_owners = []
    for code_owner in code_owners:
        code_owner_id = code_owner_dict[code_owner][&#34;user_id&#34;]
        code_owner_type = code_owner_dict[code_owner][&#34;type&#34;]

        if code_owner_type == &#34;user&#34;:
            at_code_owners.append(f&#34;    - &lt;@{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;user_nickname&#34;:
            at_code_owners.append(f&#34;    - &lt;@!{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;channel&#34;:
            at_code_owners.append(f&#34;    - &lt;#{code_owner_id}&gt;\n&#34;)
        elif code_owner_type == &#34;role&#34;:
            at_code_owners.append(f&#34;    - &lt;@&amp;{code_owner_id}&gt;\n&#34;)

    message = (
        f&#34;:man_facepalming: Flow **{flow.name}** has failed.&#34;
        + f&#39;\n  - State message: *&#34;{state.message}&#34;*&#39;
        + &#34;\n  - Link to the failed flow: &#34;
        + f&#34;https://prefect.dados.rio/flow-run/{flow_run_id}&#34;
        + &#34;\n  - Extra attention:\n&#34;
        + &#34;&#34;.join(at_code_owners)
    )
    send_discord_message(
        message=message,
        webhook_url=url,
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.parse_date_columns"><code class="name flex">
<span>def <span class="ident">parse_date_columns</span></span>(<span>dataframe: pandas.core.frame.DataFrame, partition_date_column: str) ‑> Tuple[pandas.core.frame.DataFrame, List[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Parses the date columns to the partition format.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_date_columns(
    dataframe: pd.DataFrame, partition_date_column: str
) -&gt; Tuple[pd.DataFrame, List[str]]:
    &#34;&#34;&#34;
    Parses the date columns to the partition format.
    &#34;&#34;&#34;
    ano_col = &#34;ano_particao&#34;
    mes_col = &#34;mes_particao&#34;
    data_col = &#34;data_particao&#34;
    cols = [ano_col, mes_col, data_col]
    for col in cols:
        if col in dataframe.columns:
            raise ValueError(f&#34;Column {col} already exists, please review your model.&#34;)
    dataframe[partition_date_column] = dataframe[partition_date_column].astype(str)
    dataframe[data_col] = pd.to_datetime(dataframe[partition_date_column])
    dataframe[ano_col] = dataframe[data_col].dt.year
    dataframe[mes_col] = dataframe[data_col].dt.month
    dataframe[data_col] = dataframe[data_col].dt.date

    return dataframe, [ano_col, mes_col, data_col]</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.parser_blobs_to_partition_dict"><code class="name flex">
<span>def <span class="ident">parser_blobs_to_partition_dict</span></span>(<span>blobs: list) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts the partition information from the blobs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parser_blobs_to_partition_dict(blobs: list) -&gt; dict:
    &#34;&#34;&#34;
    Extracts the partition information from the blobs.
    &#34;&#34;&#34;

    partitions_dict = {}
    for blob in blobs:
        for folder in blob.name.split(&#34;/&#34;):
            if &#34;=&#34; in folder:
                key = folder.split(&#34;=&#34;)[0]
                value = folder.split(&#34;=&#34;)[1]
                try:
                    partitions_dict[key].append(value)
                except KeyError:
                    partitions_dict[key] = [value]
    return partitions_dict</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.query_to_line"><code class="name flex">
<span>def <span class="ident">query_to_line</span></span>(<span>query: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a query to a line.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_to_line(query: str) -&gt; str:
    &#34;&#34;&#34;
    Converts a query to a line.
    &#34;&#34;&#34;
    return &#34; &#34;.join([line.strip() for line in query.split(&#34;\n&#34;)])</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.remove_columns_accents"><code class="name flex">
<span>def <span class="ident">remove_columns_accents</span></span>(<span>dataframe: pandas.core.frame.DataFrame) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Remove accents from dataframe columns.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_columns_accents(dataframe: pd.DataFrame) -&gt; list:
    &#34;&#34;&#34;
    Remove accents from dataframe columns.
    &#34;&#34;&#34;
    columns = [str(column) for column in dataframe.columns]
    dataframe.columns = columns
    return list(
        dataframe.columns.str.normalize(&#34;NFKD&#34;)
        .str.encode(&#34;ascii&#34;, errors=&#34;ignore&#34;)
        .str.decode(&#34;utf-8&#34;)
        .map(lambda x: x.strip())
        .str.replace(&#34; &#34;, &#34;_&#34;)
        .str.replace(&#34;/&#34;, &#34;_&#34;)
        .str.replace(&#34;-&#34;, &#34;_&#34;)
        .str.replace(&#34;\a&#34;, &#34;_&#34;)
        .str.replace(&#34;\b&#34;, &#34;_&#34;)
        .str.replace(&#34;\n&#34;, &#34;_&#34;)
        .str.replace(&#34;\t&#34;, &#34;_&#34;)
        .str.replace(&#34;\v&#34;, &#34;_&#34;)
        .str.replace(&#34;\f&#34;, &#34;_&#34;)
        .str.replace(&#34;\r&#34;, &#34;_&#34;)
        .str.lower()
        .map(final_column_treatment)
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.run_cloud"><code class="name flex">
<span>def <span class="ident">run_cloud</span></span>(<span>flow: prefect.core.flow.Flow, labels: List[str], parameters: Dict[str, Any] = None, run_description: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Runs a flow on Prefect Server (must have VPN configured).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_cloud(
    flow: prefect.Flow,
    labels: List[str],
    parameters: Dict[str, Any] = None,
    run_description: str = &#34;&#34;,
):
    &#34;&#34;&#34;
    Runs a flow on Prefect Server (must have VPN configured).
    &#34;&#34;&#34;
    # Setup no schedule
    flow.schedule = None

    # Change flow name for development and register
    flow.name = f&#34;{flow.name} (development)&#34;
    flow.run_config = KubernetesRun(image=&#34;ghcr.io/prefeitura-rio/prefect-flows:latest&#34;)
    flow_id = flow.register(project_name=&#34;main&#34;, labels=[])

    # Get Prefect Client and submit flow run
    client = Client()
    flow_run_id = client.create_flow_run(
        flow_id=flow_id,
        run_name=f&#34;TEST RUN - {run_description} - {flow.name}&#34;,
        labels=labels,
        parameters=parameters,
    )

    # Print flow run link so user can check it
    print(f&#34;Run submitted: TEST RUN - {run_description} - {flow.name}&#34;)
    print(f&#34;Please check at: https://prefect.dados.rio/flow-run/{flow_run_id}&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.run_local"><code class="name flex">
<span>def <span class="ident">run_local</span></span>(<span>flow: prefect.core.flow.Flow, parameters: Dict[str, Any] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs a flow locally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_local(flow: prefect.Flow, parameters: Dict[str, Any] = None):
    &#34;&#34;&#34;
    Runs a flow locally.
    &#34;&#34;&#34;
    # Setup for local run
    flow.storage = None
    flow.run_config = None
    flow.schedule = None

    # Run flow
    if parameters:
        return flow.run(parameters=parameters)
    return flow.run()</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.run_registered"><code class="name flex">
<span>def <span class="ident">run_registered</span></span>(<span>flow_id: str, labels: List[str], parameters: Dict[str, Any] = None, run_description: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Runs an already registered flow on Prefect Server (must have credentials configured).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_registered(
    flow_id: str,
    labels: List[str],
    parameters: Dict[str, Any] = None,
    run_description: str = &#34;&#34;,
):
    &#34;&#34;&#34;
    Runs an already registered flow on Prefect Server (must have credentials configured).
    &#34;&#34;&#34;
    # Get Prefect Client and submit flow run
    client = Client()
    flow_run_id = client.create_flow_run(
        flow_id=flow_id,
        run_name=f&#34;SUBMITTED REMOTELY - {run_description}&#34;,
        labels=labels,
        parameters=parameters,
    )

    # Print flow run link so user can check it
    print(f&#34;Run submitted: SUBMITTED REMOTELY - {run_description}&#34;)
    print(f&#34;Please check at: https://prefect.dados.rio/flow-run/{flow_run_id}&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.send_discord_message"><code class="name flex">
<span>def <span class="ident">send_discord_message</span></span>(<span>message: str, webhook_url: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sends a message to a Discord channel.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_discord_message(
    message: str,
    webhook_url: str,
) -&gt; None:
    &#34;&#34;&#34;
    Sends a message to a Discord channel.
    &#34;&#34;&#34;
    requests.post(
        webhook_url,
        data={&#34;content&#34;: message},
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.send_telegram_message"><code class="name flex">
<span>def <span class="ident">send_telegram_message</span></span>(<span>message: str, token: str, chat_id: int, parse_mode: str = 'HTML')</span>
</code></dt>
<dd>
<div class="desc"><p>Sends a message to a Telegram chat.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_telegram_message(
    message: str,
    token: str,
    chat_id: int,
    parse_mode: str = telegram.ParseMode.HTML,
):
    &#34;&#34;&#34;
    Sends a message to a Telegram chat.
    &#34;&#34;&#34;
    bot = telegram.Bot(token=token)
    bot.send_message(
        chat_id=chat_id,
        text=message,
        parse_mode=parse_mode,
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.set_default_parameters"><code class="name flex">
<span>def <span class="ident">set_default_parameters</span></span>(<span>flow: prefect.core.flow.Flow, default_parameters: dict) ‑> prefect.core.flow.Flow</span>
</code></dt>
<dd>
<div class="desc"><p>Sets default parameters for a flow.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_default_parameters(
    flow: prefect.Flow, default_parameters: dict
) -&gt; prefect.Flow:
    &#34;&#34;&#34;
    Sets default parameters for a flow.
    &#34;&#34;&#34;
    for parameter in flow.parameters():
        if parameter.name in default_parameters:
            parameter.default = default_parameters[parameter.name]
    return flow</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.smart_split"><code class="name flex">
<span>def <span class="ident">smart_split</span></span>(<span>text: str, max_length: int, separator: str = ' ') ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Splits a string into a list of strings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smart_split(
    text: str,
    max_length: int,
    separator: str = &#34; &#34;,
) -&gt; List[str]:
    &#34;&#34;&#34;
    Splits a string into a list of strings.
    &#34;&#34;&#34;
    if len(text) &lt;= max_length:
        return [text]

    separator_index = text.rfind(separator, 0, max_length)
    if (separator_index &gt;= max_length) or (separator_index == -1):
        raise ValueError(
            f&#39;Cannot split text &#34;{text}&#34; into {max_length}&#39;
            f&#39;characters using separator &#34;{separator}&#34;&#39;
        )

    return [
        text[:separator_index],
        *smart_split(
            text[separator_index + len(separator) :],
            max_length,
            separator,
        ),
    ]</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.to_json_dataframe"><code class="name flex">
<span>def <span class="ident">to_json_dataframe</span></span>(<span>dataframe: pandas.core.frame.DataFrame = None, csv_path: Union[str, pathlib.Path] = None, key_column: str = None, read_csv_kwargs: dict = None, save_to: Union[str, pathlib.Path] = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Manipulates a dataframe by keeping key_column and moving every other column
data to a "content" column in JSON format. Example:</p>
<ul>
<li>Input dataframe: pd.DataFrame({"key": ["a", "b", "c"], "col1": [1, 2, 3], "col2": [4, 5, 6]})</li>
<li>Output dataframe: pd.DataFrame({
"key": ["a", "b", "c"],
"content": [{"col1": 1, "col2": 4}, {"col1": 2, "col2": 5}, {"col1": 3, "col2": 6}]
})</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json_dataframe(
    dataframe: pd.DataFrame = None,
    csv_path: Union[str, Path] = None,
    key_column: str = None,
    read_csv_kwargs: dict = None,
    save_to: Union[str, Path] = None,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Manipulates a dataframe by keeping key_column and moving every other column
    data to a &#34;content&#34; column in JSON format. Example:

    - Input dataframe: pd.DataFrame({&#34;key&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;], &#34;col1&#34;: [1, 2, 3], &#34;col2&#34;: [4, 5, 6]})
    - Output dataframe: pd.DataFrame({
        &#34;key&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;],
        &#34;content&#34;: [{&#34;col1&#34;: 1, &#34;col2&#34;: 4}, {&#34;col1&#34;: 2, &#34;col2&#34;: 5}, {&#34;col1&#34;: 3, &#34;col2&#34;: 6}]
    })
    &#34;&#34;&#34;
    if dataframe is None and not csv_path:
        raise ValueError(&#34;dataframe or dataframe_path is required&#34;)
    if csv_path:
        dataframe = pd.read_csv(csv_path, **read_csv_kwargs)
    if key_column:
        dataframe[&#34;content&#34;] = dataframe.drop(columns=[key_column]).to_dict(
            orient=&#34;records&#34;
        )
        dataframe = dataframe[[&#34;key&#34;, &#34;content&#34;]]
    else:
        dataframe[&#34;content&#34;] = dataframe.to_dict(orient=&#34;records&#34;)
        dataframe = dataframe[[&#34;content&#34;]]
    if save_to:
        dataframe.to_csv(save_to, index=False)
    return dataframe</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.to_partitions"><code class="name flex">
<span>def <span class="ident">to_partitions</span></span>(<span>data: pandas.core.frame.DataFrame, partition_columns: List[str], savepath: str, data_type: str = 'csv', suffix: str = None, build_json_dataframe: bool = False, dataframe_key_column: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Save data in to hive patitions schema, given a dataframe and a list of partition columns.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas.core.frame.DataFrame</code></dt>
<dd>Dataframe to be partitioned.</dd>
<dt><strong><code>partition_columns</code></strong> :&ensp;<code>list</code></dt>
<dd>List of columns to be used as partitions.</dd>
<dt><strong><code>savepath</code></strong> :&ensp;<code>str, pathlib.PosixPath</code></dt>
<dd>folder path to save the partitions</dd>
</dl>
<h2 id="exemple">Exemple</h2>
<p>data = {
"ano": [2020, 2021, 2020, 2021, 2020, 2021, 2021,2025],
"mes": [1, 2, 3, 4, 5, 6, 6,9],
"sigla_uf": ["SP", "SP", "RJ", "RJ", "PR", "PR", "PR","PR"],
"dado": ["a", "b", "c", "d", "e", "f", "g",'h'],
}
to_partitions(
data=pd.DataFrame(data),
partition_columns=['ano','mes','sigla_uf'],
savepath='partitions/'
)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_partitions(
    data: pd.DataFrame,
    partition_columns: List[str],
    savepath: str,
    data_type: str = &#34;csv&#34;,
    suffix: str = None,
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
):  # sourcery skip: raise-specific-error
    &#34;&#34;&#34;Save data in to hive patitions schema, given a dataframe and a list of partition columns.
    Args:
        data (pandas.core.frame.DataFrame): Dataframe to be partitioned.
        partition_columns (list): List of columns to be used as partitions.
        savepath (str, pathlib.PosixPath): folder path to save the partitions
    Exemple:
        data = {
            &#34;ano&#34;: [2020, 2021, 2020, 2021, 2020, 2021, 2021,2025],
            &#34;mes&#34;: [1, 2, 3, 4, 5, 6, 6,9],
            &#34;sigla_uf&#34;: [&#34;SP&#34;, &#34;SP&#34;, &#34;RJ&#34;, &#34;RJ&#34;, &#34;PR&#34;, &#34;PR&#34;, &#34;PR&#34;,&#34;PR&#34;],
            &#34;dado&#34;: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;, &#34;e&#34;, &#34;f&#34;, &#34;g&#34;,&#39;h&#39;],
        }
        to_partitions(
            data=pd.DataFrame(data),
            partition_columns=[&#39;ano&#39;,&#39;mes&#39;,&#39;sigla_uf&#39;],
            savepath=&#39;partitions/&#39;
        )
    &#34;&#34;&#34;

    if isinstance(data, (pd.core.frame.DataFrame)):

        savepath = Path(savepath)

        # create unique combinations between partition columns
        unique_combinations = (
            data[partition_columns]
            .drop_duplicates(subset=partition_columns)
            .to_dict(orient=&#34;records&#34;)
        )

        for filter_combination in unique_combinations:
            patitions_values = [
                f&#34;{partition}={value}&#34;
                for partition, value in filter_combination.items()
            ]

            # get filtered data
            df_filter = data.loc[
                data[filter_combination.keys()]
                .isin(filter_combination.values())
                .all(axis=1),
                :,
            ]
            df_filter = df_filter.drop(columns=partition_columns).reset_index(drop=True)

            # create folder tree
            filter_save_path = Path(savepath / &#34;/&#34;.join(patitions_values))
            filter_save_path.mkdir(parents=True, exist_ok=True)
            if suffix is not None:
                file_filter_save_path = (
                    Path(filter_save_path) / f&#34;data_{suffix}.{data_type}&#34;
                )
            else:
                file_filter_save_path = Path(filter_save_path) / f&#34;data.{data_type}&#34;

            if build_json_dataframe:
                df_filter = to_json_dataframe(
                    df_filter, key_column=dataframe_key_column
                )

            if data_type == &#34;csv&#34;:
                # append data to csv
                df_filter.to_csv(
                    file_filter_save_path,
                    index=False,
                    mode=&#34;a&#34;,
                    header=not file_filter_save_path.exists(),
                )
            elif data_type == &#34;parquet&#34;:
                dataframe_to_parquet(dataframe=df_filter, path=file_filter_save_path)
            else:
                raise ValueError(f&#34;Invalid data type: {data_type}&#34;)
    else:
        raise BaseException(&#34;Data need to be a pandas DataFrame&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.utils.untuple_clocks"><code class="name flex">
<span>def <span class="ident">untuple_clocks</span></span>(<span>clocks)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a list of tuples to a list of clocks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def untuple_clocks(clocks):
    &#34;&#34;&#34;
    Converts a list of tuples to a list of clocks.
    &#34;&#34;&#34;
    return [clock[0] if isinstance(clock, tuple) else clock for clock in clocks]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.utils" href="index.html">pipelines.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.utils.utils.batch_to_dataframe" href="#pipelines.utils.utils.batch_to_dataframe">batch_to_dataframe</a></code></li>
<li><code><a title="pipelines.utils.utils.clean_dataframe" href="#pipelines.utils.utils.clean_dataframe">clean_dataframe</a></code></li>
<li><code><a title="pipelines.utils.utils.dataframe_to_csv" href="#pipelines.utils.utils.dataframe_to_csv">dataframe_to_csv</a></code></li>
<li><code><a title="pipelines.utils.utils.dataframe_to_parquet" href="#pipelines.utils.utils.dataframe_to_parquet">dataframe_to_parquet</a></code></li>
<li><code><a title="pipelines.utils.utils.determine_whether_to_execute_or_not" href="#pipelines.utils.utils.determine_whether_to_execute_or_not">determine_whether_to_execute_or_not</a></code></li>
<li><code><a title="pipelines.utils.utils.dump_header_to_file" href="#pipelines.utils.utils.dump_header_to_file">dump_header_to_file</a></code></li>
<li><code><a title="pipelines.utils.utils.final_column_treatment" href="#pipelines.utils.utils.final_column_treatment">final_column_treatment</a></code></li>
<li><code><a title="pipelines.utils.utils.get_credentials_from_env" href="#pipelines.utils.utils.get_credentials_from_env">get_credentials_from_env</a></code></li>
<li><code><a title="pipelines.utils.utils.get_redis_client" href="#pipelines.utils.utils.get_redis_client">get_redis_client</a></code></li>
<li><code><a title="pipelines.utils.utils.get_storage_blobs" href="#pipelines.utils.utils.get_storage_blobs">get_storage_blobs</a></code></li>
<li><code><a title="pipelines.utils.utils.get_username_and_password_from_secret" href="#pipelines.utils.utils.get_username_and_password_from_secret">get_username_and_password_from_secret</a></code></li>
<li><code><a title="pipelines.utils.utils.get_vault_client" href="#pipelines.utils.utils.get_vault_client">get_vault_client</a></code></li>
<li><code><a title="pipelines.utils.utils.get_vault_secret" href="#pipelines.utils.utils.get_vault_secret">get_vault_secret</a></code></li>
<li><code><a title="pipelines.utils.utils.human_readable" href="#pipelines.utils.utils.human_readable">human_readable</a></code></li>
<li><code><a title="pipelines.utils.utils.list_blobs_with_prefix" href="#pipelines.utils.utils.list_blobs_with_prefix">list_blobs_with_prefix</a></code></li>
<li><code><a title="pipelines.utils.utils.log" href="#pipelines.utils.utils.log">log</a></code></li>
<li><code><a title="pipelines.utils.utils.notify_discord_on_failure" href="#pipelines.utils.utils.notify_discord_on_failure">notify_discord_on_failure</a></code></li>
<li><code><a title="pipelines.utils.utils.parse_date_columns" href="#pipelines.utils.utils.parse_date_columns">parse_date_columns</a></code></li>
<li><code><a title="pipelines.utils.utils.parser_blobs_to_partition_dict" href="#pipelines.utils.utils.parser_blobs_to_partition_dict">parser_blobs_to_partition_dict</a></code></li>
<li><code><a title="pipelines.utils.utils.query_to_line" href="#pipelines.utils.utils.query_to_line">query_to_line</a></code></li>
<li><code><a title="pipelines.utils.utils.remove_columns_accents" href="#pipelines.utils.utils.remove_columns_accents">remove_columns_accents</a></code></li>
<li><code><a title="pipelines.utils.utils.run_cloud" href="#pipelines.utils.utils.run_cloud">run_cloud</a></code></li>
<li><code><a title="pipelines.utils.utils.run_local" href="#pipelines.utils.utils.run_local">run_local</a></code></li>
<li><code><a title="pipelines.utils.utils.run_registered" href="#pipelines.utils.utils.run_registered">run_registered</a></code></li>
<li><code><a title="pipelines.utils.utils.send_discord_message" href="#pipelines.utils.utils.send_discord_message">send_discord_message</a></code></li>
<li><code><a title="pipelines.utils.utils.send_telegram_message" href="#pipelines.utils.utils.send_telegram_message">send_telegram_message</a></code></li>
<li><code><a title="pipelines.utils.utils.set_default_parameters" href="#pipelines.utils.utils.set_default_parameters">set_default_parameters</a></code></li>
<li><code><a title="pipelines.utils.utils.smart_split" href="#pipelines.utils.utils.smart_split">smart_split</a></code></li>
<li><code><a title="pipelines.utils.utils.to_json_dataframe" href="#pipelines.utils.utils.to_json_dataframe">to_json_dataframe</a></code></li>
<li><code><a title="pipelines.utils.utils.to_partitions" href="#pipelines.utils.utils.to_partitions">to_partitions</a></code></li>
<li><code><a title="pipelines.utils.utils.untuple_clocks" href="#pipelines.utils.utils.untuple_clocks">untuple_clocks</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>