<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.utils.tasks API documentation</title>
<meta name="description" content="Helper tasks that could fit any pipeline." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.utils.tasks</code></h1>
</header>
<section id="section-intro">
<p>Helper tasks that could fit any pipeline.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Helper tasks that could fit any pipeline.
&#34;&#34;&#34;
# pylint: disable=unused-argument, R0913

from datetime import timedelta

from pathlib import Path
from typing import List, Union, Any

import basedosdados as bd
import pendulum
import prefect
from prefect import task
from prefect.backend import FlowRunView
from prefect.client import Client

from pipelines.constants import constants
from pipelines.utils.utils import (
    get_username_and_password_from_secret,
    log,
    dump_header_to_file,
)

##################
#
# Utilities for flow management
#
##################


@prefect.task(checkpoint=False)
def log_task(msg: Any, level: str = &#34;info&#34;, wait=None):
    &#34;&#34;&#34;
    Logs a message to prefect&#39;s logger.
    &#34;&#34;&#34;
    log(msg, level)


@prefect.task(checkpoint=False)
def get_now_time():
    &#34;&#34;&#34;
    Returns the HH:MM.
    &#34;&#34;&#34;
    now = pendulum.now(pendulum.timezone(&#34;America/Sao_Paulo&#34;))

    return f&#34;{now.hour}:{f&#39;0{now.minute}&#39; if len(str(now.minute))==1 else now.minute}&#34;


@task
def get_current_flow_labels() -&gt; List[str]:
    &#34;&#34;&#34;
    Get the labels of the current flow.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    flow_run_view = FlowRunView.from_flow_run_id(flow_run_id)
    return flow_run_view.labels


@task
def greater_than(value, compare_to) -&gt; bool:
    &#34;&#34;&#34;
    Returns True if value is greater than compare_to.
    &#34;&#34;&#34;
    return value &gt; compare_to


@task
def rename_current_flow_run_now_time(prefix: str, now_time=None, wait=None) -&gt; None:
    &#34;&#34;&#34;
    Rename the current flow run.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    client = Client()
    return client.set_flow_run_name(flow_run_id, f&#34;{prefix}{now_time}&#34;)


@task
def rename_current_flow_run_dataset_table(
    prefix: str, dataset_id, table_id, wait=None
) -&gt; None:
    &#34;&#34;&#34;
    Rename the current flow run.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    client = Client()
    return client.set_flow_run_name(flow_run_id, f&#34;{prefix}{dataset_id}.{table_id}&#34;)


##################
#
# Hashicorp Vault
#
##################


@task(checkpoint=False, nout=2)
def get_user_and_password(secret_path: str, wait=None):
    &#34;&#34;&#34;
    Returns the user and password for the given secret path.
    &#34;&#34;&#34;
    log(f&#34;Getting user and password for secret path: {secret_path}&#34;)
    return get_username_and_password_from_secret(secret_path)


###############
#
# Upload to GCS
#
###############


@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def create_table_and_upload_to_gcs(
    data_path: Union[str, Path],
    dataset_id: str,
    table_id: str,
    dump_mode: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; None:
    &#34;&#34;&#34;
    Create table using BD+ and upload to GCS.
    &#34;&#34;&#34;
    # pylint: disable=C0103
    tb = bd.Table(dataset_id=dataset_id, table_id=table_id)
    table_staging = f&#34;{tb.table_full_name[&#39;staging&#39;]}&#34;
    # pylint: disable=C0103
    st = bd.Storage(dataset_id=dataset_id, table_id=table_id)
    storage_path = f&#34;{st.bucket_name}.staging.{dataset_id}.{table_id}&#34;
    storage_path_link = (
        f&#34;https://console.cloud.google.com/storage/browser/{st.bucket_name}&#34;
        f&#34;/staging/{dataset_id}/{table_id}&#34;
    )

    # prod datasets is public if the project is datario. staging are private im both projects
    dataset_is_public = tb.client[&#34;bigquery_prod&#34;].project == &#34;datario&#34;

    #####################################
    #
    # MANAGEMENT OF TABLE CREATION
    #
    #####################################
    log(&#34;STARTING TABLE CREATION MANAGEMENT&#34;)
    if dump_mode == &#34;append&#34;:
        if tb.table_exists(mode=&#34;staging&#34;):
            log(
                f&#34;MODE APPEND: Table ALREADY EXISTS:&#34;
                f&#34;\n{table_staging}&#34;
                f&#34;\n{storage_path_link}&#34;
            )
        else:
            # the header is needed to create a table when dosen&#39;t exist
            log(&#34;MODE APPEND: Table DOSEN&#39;T EXISTS\nStart to CREATE HEADER file&#34;)
            header_path = dump_header_to_file(data_path=data_path)
            log(&#34;MODE APPEND: Created HEADER file:\n&#34; f&#34;{header_path}&#34;)

            tb.create(
                path=header_path,
                if_storage_data_exists=&#34;replace&#34;,
                if_table_config_exists=&#34;replace&#34;,
                if_table_exists=&#34;replace&#34;,
                dataset_is_public=dataset_is_public,
            )

            log(
                &#34;MODE APPEND: Sucessfully CREATED A NEW TABLE:\n&#34;
                f&#34;{table_staging}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301

            st.delete_table(
                mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True
            )
            log(
                &#34;MODE APPEND: Sucessfully REMOVED HEADER DATA from Storage:\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
    elif dump_mode == &#34;overwrite&#34;:
        if tb.table_exists(mode=&#34;staging&#34;):
            log(
                &#34;MODE OVERWRITE: Table ALREADY EXISTS, DELETING OLD DATA!\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
            st.delete_table(
                mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True
            )
            log(
                &#34;MODE OVERWRITE: Sucessfully DELETED OLD DATA from Storage:\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
            tb.delete(mode=&#34;all&#34;)
            log(
                &#34;MODE OVERWRITE: Sucessfully DELETED TABLE:\n&#34;
                f&#34;{table_staging}\n&#34;
                f&#34;{tb.table_full_name[&#39;prod&#39;]}&#34;
            )  # pylint: disable=C0301

        # the header is needed to create a table when dosen&#39;t exist
        # in overwrite mode the header is always created
        log(&#34;MODE OVERWRITE: Table DOSEN&#39;T EXISTS\nStart to CREATE HEADER file&#34;)
        header_path = dump_header_to_file(data_path=data_path)
        log(&#34;MODE OVERWRITE: Created HEADER file:\n&#34; f&#34;{header_path}&#34;)

        tb.create(
            path=header_path,
            if_storage_data_exists=&#34;replace&#34;,
            if_table_config_exists=&#34;replace&#34;,
            if_table_exists=&#34;replace&#34;,
            dataset_is_public=dataset_is_public,
        )

        log(
            &#34;MODE OVERWRITE: Sucessfully CREATED TABLE\n&#34;
            f&#34;{table_staging}\n&#34;
            f&#34;{storage_path_link}&#34;
        )

        st.delete_table(mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True)
        log(
            f&#34;MODE OVERWRITE: Sucessfully REMOVED HEADER DATA from Storage\n:&#34;
            f&#34;{storage_path}\n&#34;
            f&#34;{storage_path_link}&#34;
        )  # pylint: disable=C0301

    #####################################
    #
    # Uploads a bunch of files using BD+
    #
    #####################################

    log(&#34;STARTING UPLOAD TO GCS&#34;)
    if tb.table_exists(mode=&#34;staging&#34;):
        # the name of the files need to be the same or the data doesn&#39;t get overwritten
        tb.append(filepath=data_path, if_exists=&#34;replace&#34;)

        log(
            f&#34;STEP UPLOAD: Successfully uploaded {data_path} to Storage:\n&#34;
            f&#34;{storage_path}\n&#34;
            f&#34;{storage_path_link}&#34;
        )
    else:
        # pylint: disable=C0301
        log(&#34;STEP UPLOAD: Table does not exist in STAGING, need to create first&#34;)


@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def check_table_exists(
    dataset_id: str,
    table_id: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; bool:
    &#34;&#34;&#34;
    Check if table exists in staging on GCP
    &#34;&#34;&#34;
    # pylint: disable=C0103
    tb = bd.Table(dataset_id=dataset_id, table_id=table_id)
    exists = tb.table_exists(mode=&#34;staging&#34;)
    log(f&#34;Table {dataset_id}.{table_id} exists in staging: {exists}&#34;)
    return exists</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.utils.tasks.check_table_exists"><code class="name flex">
<span>def <span class="ident">check_table_exists</span></span>(<span>dataset_id: str, table_id: str, wait=None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if table exists in staging on GCP</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def check_table_exists(
    dataset_id: str,
    table_id: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; bool:
    &#34;&#34;&#34;
    Check if table exists in staging on GCP
    &#34;&#34;&#34;
    # pylint: disable=C0103
    tb = bd.Table(dataset_id=dataset_id, table_id=table_id)
    exists = tb.table_exists(mode=&#34;staging&#34;)
    log(f&#34;Table {dataset_id}.{table_id} exists in staging: {exists}&#34;)
    return exists</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.create_table_and_upload_to_gcs"><code class="name flex">
<span>def <span class="ident">create_table_and_upload_to_gcs</span></span>(<span>data_path: Union[str, pathlib.Path], dataset_id: str, table_id: str, dump_mode: str, wait=None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Create table using BD+ and upload to GCS.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def create_table_and_upload_to_gcs(
    data_path: Union[str, Path],
    dataset_id: str,
    table_id: str,
    dump_mode: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; None:
    &#34;&#34;&#34;
    Create table using BD+ and upload to GCS.
    &#34;&#34;&#34;
    # pylint: disable=C0103
    tb = bd.Table(dataset_id=dataset_id, table_id=table_id)
    table_staging = f&#34;{tb.table_full_name[&#39;staging&#39;]}&#34;
    # pylint: disable=C0103
    st = bd.Storage(dataset_id=dataset_id, table_id=table_id)
    storage_path = f&#34;{st.bucket_name}.staging.{dataset_id}.{table_id}&#34;
    storage_path_link = (
        f&#34;https://console.cloud.google.com/storage/browser/{st.bucket_name}&#34;
        f&#34;/staging/{dataset_id}/{table_id}&#34;
    )

    # prod datasets is public if the project is datario. staging are private im both projects
    dataset_is_public = tb.client[&#34;bigquery_prod&#34;].project == &#34;datario&#34;

    #####################################
    #
    # MANAGEMENT OF TABLE CREATION
    #
    #####################################
    log(&#34;STARTING TABLE CREATION MANAGEMENT&#34;)
    if dump_mode == &#34;append&#34;:
        if tb.table_exists(mode=&#34;staging&#34;):
            log(
                f&#34;MODE APPEND: Table ALREADY EXISTS:&#34;
                f&#34;\n{table_staging}&#34;
                f&#34;\n{storage_path_link}&#34;
            )
        else:
            # the header is needed to create a table when dosen&#39;t exist
            log(&#34;MODE APPEND: Table DOSEN&#39;T EXISTS\nStart to CREATE HEADER file&#34;)
            header_path = dump_header_to_file(data_path=data_path)
            log(&#34;MODE APPEND: Created HEADER file:\n&#34; f&#34;{header_path}&#34;)

            tb.create(
                path=header_path,
                if_storage_data_exists=&#34;replace&#34;,
                if_table_config_exists=&#34;replace&#34;,
                if_table_exists=&#34;replace&#34;,
                dataset_is_public=dataset_is_public,
            )

            log(
                &#34;MODE APPEND: Sucessfully CREATED A NEW TABLE:\n&#34;
                f&#34;{table_staging}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301

            st.delete_table(
                mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True
            )
            log(
                &#34;MODE APPEND: Sucessfully REMOVED HEADER DATA from Storage:\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
    elif dump_mode == &#34;overwrite&#34;:
        if tb.table_exists(mode=&#34;staging&#34;):
            log(
                &#34;MODE OVERWRITE: Table ALREADY EXISTS, DELETING OLD DATA!\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
            st.delete_table(
                mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True
            )
            log(
                &#34;MODE OVERWRITE: Sucessfully DELETED OLD DATA from Storage:\n&#34;
                f&#34;{storage_path}\n&#34;
                f&#34;{storage_path_link}&#34;
            )  # pylint: disable=C0301
            tb.delete(mode=&#34;all&#34;)
            log(
                &#34;MODE OVERWRITE: Sucessfully DELETED TABLE:\n&#34;
                f&#34;{table_staging}\n&#34;
                f&#34;{tb.table_full_name[&#39;prod&#39;]}&#34;
            )  # pylint: disable=C0301

        # the header is needed to create a table when dosen&#39;t exist
        # in overwrite mode the header is always created
        log(&#34;MODE OVERWRITE: Table DOSEN&#39;T EXISTS\nStart to CREATE HEADER file&#34;)
        header_path = dump_header_to_file(data_path=data_path)
        log(&#34;MODE OVERWRITE: Created HEADER file:\n&#34; f&#34;{header_path}&#34;)

        tb.create(
            path=header_path,
            if_storage_data_exists=&#34;replace&#34;,
            if_table_config_exists=&#34;replace&#34;,
            if_table_exists=&#34;replace&#34;,
            dataset_is_public=dataset_is_public,
        )

        log(
            &#34;MODE OVERWRITE: Sucessfully CREATED TABLE\n&#34;
            f&#34;{table_staging}\n&#34;
            f&#34;{storage_path_link}&#34;
        )

        st.delete_table(mode=&#34;staging&#34;, bucket_name=st.bucket_name, not_found_ok=True)
        log(
            f&#34;MODE OVERWRITE: Sucessfully REMOVED HEADER DATA from Storage\n:&#34;
            f&#34;{storage_path}\n&#34;
            f&#34;{storage_path_link}&#34;
        )  # pylint: disable=C0301

    #####################################
    #
    # Uploads a bunch of files using BD+
    #
    #####################################

    log(&#34;STARTING UPLOAD TO GCS&#34;)
    if tb.table_exists(mode=&#34;staging&#34;):
        # the name of the files need to be the same or the data doesn&#39;t get overwritten
        tb.append(filepath=data_path, if_exists=&#34;replace&#34;)

        log(
            f&#34;STEP UPLOAD: Successfully uploaded {data_path} to Storage:\n&#34;
            f&#34;{storage_path}\n&#34;
            f&#34;{storage_path_link}&#34;
        )
    else:
        # pylint: disable=C0301
        log(&#34;STEP UPLOAD: Table does not exist in STAGING, need to create first&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.get_current_flow_labels"><code class="name flex">
<span>def <span class="ident">get_current_flow_labels</span></span>(<span>) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the labels of the current flow.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_current_flow_labels() -&gt; List[str]:
    &#34;&#34;&#34;
    Get the labels of the current flow.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    flow_run_view = FlowRunView.from_flow_run_id(flow_run_id)
    return flow_run_view.labels</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.get_now_time"><code class="name flex">
<span>def <span class="ident">get_now_time</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the HH:MM.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@prefect.task(checkpoint=False)
def get_now_time():
    &#34;&#34;&#34;
    Returns the HH:MM.
    &#34;&#34;&#34;
    now = pendulum.now(pendulum.timezone(&#34;America/Sao_Paulo&#34;))

    return f&#34;{now.hour}:{f&#39;0{now.minute}&#39; if len(str(now.minute))==1 else now.minute}&#34;</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.get_user_and_password"><code class="name flex">
<span>def <span class="ident">get_user_and_password</span></span>(<span>secret_path: str, wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the user and password for the given secret path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(checkpoint=False, nout=2)
def get_user_and_password(secret_path: str, wait=None):
    &#34;&#34;&#34;
    Returns the user and password for the given secret path.
    &#34;&#34;&#34;
    log(f&#34;Getting user and password for secret path: {secret_path}&#34;)
    return get_username_and_password_from_secret(secret_path)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.greater_than"><code class="name flex">
<span>def <span class="ident">greater_than</span></span>(<span>value, compare_to) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Returns True if value is greater than compare_to.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def greater_than(value, compare_to) -&gt; bool:
    &#34;&#34;&#34;
    Returns True if value is greater than compare_to.
    &#34;&#34;&#34;
    return value &gt; compare_to</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.log_task"><code class="name flex">
<span>def <span class="ident">log_task</span></span>(<span>msg: Any, level: str = 'info', wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Logs a message to prefect's logger.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@prefect.task(checkpoint=False)
def log_task(msg: Any, level: str = &#34;info&#34;, wait=None):
    &#34;&#34;&#34;
    Logs a message to prefect&#39;s logger.
    &#34;&#34;&#34;
    log(msg, level)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.rename_current_flow_run_dataset_table"><code class="name flex">
<span>def <span class="ident">rename_current_flow_run_dataset_table</span></span>(<span>prefix: str, dataset_id, table_id, wait=None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Rename the current flow run.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def rename_current_flow_run_dataset_table(
    prefix: str, dataset_id, table_id, wait=None
) -&gt; None:
    &#34;&#34;&#34;
    Rename the current flow run.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    client = Client()
    return client.set_flow_run_name(flow_run_id, f&#34;{prefix}{dataset_id}.{table_id}&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.tasks.rename_current_flow_run_now_time"><code class="name flex">
<span>def <span class="ident">rename_current_flow_run_now_time</span></span>(<span>prefix: str, now_time=None, wait=None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Rename the current flow run.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def rename_current_flow_run_now_time(prefix: str, now_time=None, wait=None) -&gt; None:
    &#34;&#34;&#34;
    Rename the current flow run.
    &#34;&#34;&#34;
    flow_run_id = prefect.context.get(&#34;flow_run_id&#34;)
    client = Client()
    return client.set_flow_run_name(flow_run_id, f&#34;{prefix}{now_time}&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.utils" href="index.html">pipelines.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.utils.tasks.check_table_exists" href="#pipelines.utils.tasks.check_table_exists">check_table_exists</a></code></li>
<li><code><a title="pipelines.utils.tasks.create_table_and_upload_to_gcs" href="#pipelines.utils.tasks.create_table_and_upload_to_gcs">create_table_and_upload_to_gcs</a></code></li>
<li><code><a title="pipelines.utils.tasks.get_current_flow_labels" href="#pipelines.utils.tasks.get_current_flow_labels">get_current_flow_labels</a></code></li>
<li><code><a title="pipelines.utils.tasks.get_now_time" href="#pipelines.utils.tasks.get_now_time">get_now_time</a></code></li>
<li><code><a title="pipelines.utils.tasks.get_user_and_password" href="#pipelines.utils.tasks.get_user_and_password">get_user_and_password</a></code></li>
<li><code><a title="pipelines.utils.tasks.greater_than" href="#pipelines.utils.tasks.greater_than">greater_than</a></code></li>
<li><code><a title="pipelines.utils.tasks.log_task" href="#pipelines.utils.tasks.log_task">log_task</a></code></li>
<li><code><a title="pipelines.utils.tasks.rename_current_flow_run_dataset_table" href="#pipelines.utils.tasks.rename_current_flow_run_dataset_table">rename_current_flow_run_dataset_table</a></code></li>
<li><code><a title="pipelines.utils.tasks.rename_current_flow_run_now_time" href="#pipelines.utils.tasks.rename_current_flow_run_now_time">rename_current_flow_run_now_time</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>