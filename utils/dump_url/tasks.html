<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.utils.dump_url.tasks API documentation</title>
<meta name="description" content="General purpose tasks for dumping data from URLs." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.utils.dump_url.tasks</code></h1>
</header>
<section id="section-intro">
<p>General purpose tasks for dumping data from URLs.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# pylint: disable=E1101
&#34;&#34;&#34;
General purpose tasks for dumping data from URLs.
&#34;&#34;&#34;

from datetime import datetime, timedelta
import io
from pathlib import Path
from typing import List

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload
import gspread
import pandas as pd
from prefect import task
import requests

from pipelines.constants import constants
from pipelines.utils.utils import (
    remove_columns_accents,
)
from pipelines.utils.dump_url.utils import handle_dataframe_chunk
from pipelines.utils.utils import (
    get_credentials_from_env,
    log,
)


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
# pylint: disable=R0912,R0914,R0915
def download_url(  # pylint: disable=too-many-arguments
    url: str,
    fname: str,
    url_type: str = &#34;direct&#34;,
    gsheets_sheet_order: int = 0,
    gsheets_sheet_name: str = None,
    gsheets_sheet_range: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Downloads a file from a URL and saves it to a local file.
    Try to do it without using lots of RAM.
    It is not optimized for Google Sheets downloads.

    Args:
        url: URL to download from.
        fname: Name of the file to save to.
        url_type: Type or URL that is being passed.
            `direct`-&gt; common URL to download directly;
            `google_drive`-&gt; Google Drive URL;
            `google_sheet`-&gt; Google Sheet URL.
        gsheets_sheet_order: Worksheet index, in the case you want to select it by index. \
            Worksheet indexes start from zero.
        gsheets_sheet_name: Worksheet name, in the case you want to select it by name.
        gsheets_sheet_range: Range in selected worksheet to get data from. Defaults to entire \
            worksheet.

    Returns:
        None.
    &#34;&#34;&#34;
    filepath = Path(fname)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    if url_type == &#34;google_sheet&#34;:
        url_prefix = &#34;https://docs.google.com/spreadsheets/d/&#34;
        if not url.startswith(url_prefix):
            raise ValueError(
                &#34;URL must start with https://docs.google.com/spreadsheets/d/&#34;
                f&#34;Invalid URL: {url}&#34;
            )
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is a Google Sheets URL, downloading directly&#34;)
        credentials = get_credentials_from_env(
            scopes=[
                &#34;https://www.googleapis.com/auth/spreadsheets&#34;,
                &#34;https://www.googleapis.com/auth/drive&#34;,
            ]
        )
        gspread_client = gspread.authorize(credentials)
        sheet = gspread_client.open_by_url(url)
        if gsheets_sheet_name:
            worksheet = sheet.worksheet(gsheets_sheet_name)
        else:
            worksheet = sheet.get_worksheet(gsheets_sheet_order)
        if gsheets_sheet_range:  # if range is informed, get range from worksheet
            dataframe = pd.DataFrame(worksheet.batch_get((gsheets_sheet_range,))[0])
        else:
            dataframe = pd.DataFrame(worksheet.get_values())
        new_header = dataframe.iloc[0]  # grab the first row for the header
        dataframe = dataframe[1:]  # take the data less the header row
        dataframe.columns = new_header  # set the header row as the df header
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe shape: {dataframe.shape}&#34;)
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe columns: {dataframe.columns}&#34;)
        dataframe.columns = remove_columns_accents(dataframe)
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe columns after treatment: {dataframe.columns}&#34;)
        dataframe.to_csv(filepath, index=False)
    elif url_type == &#34;direct&#34;:
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is not a Google Drive URL, downloading directly&#34;)
        req = requests.get(url, stream=True)
        with open(fname, &#34;wb&#34;) as file:
            for chunk in req.iter_content(chunk_size=1024):
                if chunk:
                    file.write(chunk)
                    file.flush()
    elif url_type == &#34;google_drive&#34;:
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is a Google Drive URL, downloading from Google Drive&#34;)
        # URL is in format
        # https://drive.google.com/file/d/&lt;FILE_ID&gt;/...
        # We want to extract the FILE_ID
        log(&#34;&gt;&gt;&gt;&gt;&gt; Extracting FILE_ID from URL&#34;)
        url_prefix = &#34;https://drive.google.com/file/d/&#34;
        if not url.startswith(url_prefix):
            raise ValueError(
                &#34;URL must start with https://drive.google.com/file/d/.&#34;
                f&#34;Invalid URL: {url}&#34;
            )
        file_id = url.removeprefix(url_prefix).split(&#34;/&#34;)[0]
        log(f&#34;&gt;&gt;&gt;&gt;&gt; FILE_ID: {file_id}&#34;)
        creds = get_credentials_from_env(
            scopes=[&#34;https://www.googleapis.com/auth/drive&#34;]
        )
        try:
            service = build(&#34;drive&#34;, &#34;v3&#34;, credentials=creds)
            request = service.files().get_media(fileId=file_id)  # pylint: disable=E1101
            fh = io.FileIO(fname, mode=&#34;wb&#34;)  # pylint: disable=C0103
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while done is False:
                status, done = downloader.next_chunk()
                log(f&#34;Downloading file... {int(status.progress() * 100)}%.&#34;)
        except HttpError as error:
            log(f&#34;HTTPError: {error}&#34;, &#34;error&#34;)
            raise error
    else:
        raise ValueError(&#34;Invalid URL type. Please set values to `url_type` parameter&#34;)


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
# pylint: disable=R0913
def dump_files(
    file_path: str,
    partition_columns: List[str],
    save_path: str = &#34;.&#34;,
    chunksize: int = 10**6,
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Dump files according to chunk size
    &#34;&#34;&#34;
    event_id = datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)
    for idx, chunk in enumerate(pd.read_csv(Path(file_path), chunksize=chunksize)):
        log(f&#34;Dumping batch {idx} with size {chunksize}&#34;)
        handle_dataframe_chunk(
            dataframe=chunk,
            save_path=save_path,
            partition_columns=partition_columns,
            event_id=event_id,
            idx=idx,
            build_json_dataframe=build_json_dataframe,
            dataframe_key_column=dataframe_key_column,
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.utils.dump_url.tasks.download_url"><code class="name flex">
<span>def <span class="ident">download_url</span></span>(<span>url: str, fname: str, url_type: str = 'direct', gsheets_sheet_order: int = 0, gsheets_sheet_name: str = None, gsheets_sheet_range: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Downloads a file from a URL and saves it to a local file.
Try to do it without using lots of RAM.
It is not optimized for Google Sheets downloads.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>URL to download from.</dd>
<dt><strong><code>fname</code></strong></dt>
<dd>Name of the file to save to.</dd>
<dt><strong><code>url_type</code></strong></dt>
<dd>Type or URL that is being passed.
<code>direct</code>-&gt; common URL to download directly;
<code>google_drive</code>-&gt; Google Drive URL;
<code>google_sheet</code>-&gt; Google Sheet URL.</dd>
<dt><strong><code>gsheets_sheet_order</code></strong></dt>
<dd>Worksheet index, in the case you want to select it by index.
Worksheet indexes start from zero.</dd>
<dt><strong><code>gsheets_sheet_name</code></strong></dt>
<dd>Worksheet name, in the case you want to select it by name.</dd>
<dt><strong><code>gsheets_sheet_range</code></strong></dt>
<dd>Range in selected worksheet to get data from. Defaults to entire
worksheet.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
# pylint: disable=R0912,R0914,R0915
def download_url(  # pylint: disable=too-many-arguments
    url: str,
    fname: str,
    url_type: str = &#34;direct&#34;,
    gsheets_sheet_order: int = 0,
    gsheets_sheet_name: str = None,
    gsheets_sheet_range: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Downloads a file from a URL and saves it to a local file.
    Try to do it without using lots of RAM.
    It is not optimized for Google Sheets downloads.

    Args:
        url: URL to download from.
        fname: Name of the file to save to.
        url_type: Type or URL that is being passed.
            `direct`-&gt; common URL to download directly;
            `google_drive`-&gt; Google Drive URL;
            `google_sheet`-&gt; Google Sheet URL.
        gsheets_sheet_order: Worksheet index, in the case you want to select it by index. \
            Worksheet indexes start from zero.
        gsheets_sheet_name: Worksheet name, in the case you want to select it by name.
        gsheets_sheet_range: Range in selected worksheet to get data from. Defaults to entire \
            worksheet.

    Returns:
        None.
    &#34;&#34;&#34;
    filepath = Path(fname)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    if url_type == &#34;google_sheet&#34;:
        url_prefix = &#34;https://docs.google.com/spreadsheets/d/&#34;
        if not url.startswith(url_prefix):
            raise ValueError(
                &#34;URL must start with https://docs.google.com/spreadsheets/d/&#34;
                f&#34;Invalid URL: {url}&#34;
            )
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is a Google Sheets URL, downloading directly&#34;)
        credentials = get_credentials_from_env(
            scopes=[
                &#34;https://www.googleapis.com/auth/spreadsheets&#34;,
                &#34;https://www.googleapis.com/auth/drive&#34;,
            ]
        )
        gspread_client = gspread.authorize(credentials)
        sheet = gspread_client.open_by_url(url)
        if gsheets_sheet_name:
            worksheet = sheet.worksheet(gsheets_sheet_name)
        else:
            worksheet = sheet.get_worksheet(gsheets_sheet_order)
        if gsheets_sheet_range:  # if range is informed, get range from worksheet
            dataframe = pd.DataFrame(worksheet.batch_get((gsheets_sheet_range,))[0])
        else:
            dataframe = pd.DataFrame(worksheet.get_values())
        new_header = dataframe.iloc[0]  # grab the first row for the header
        dataframe = dataframe[1:]  # take the data less the header row
        dataframe.columns = new_header  # set the header row as the df header
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe shape: {dataframe.shape}&#34;)
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe columns: {dataframe.columns}&#34;)
        dataframe.columns = remove_columns_accents(dataframe)
        log(f&#34;&gt;&gt;&gt;&gt;&gt; Dataframe columns after treatment: {dataframe.columns}&#34;)
        dataframe.to_csv(filepath, index=False)
    elif url_type == &#34;direct&#34;:
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is not a Google Drive URL, downloading directly&#34;)
        req = requests.get(url, stream=True)
        with open(fname, &#34;wb&#34;) as file:
            for chunk in req.iter_content(chunk_size=1024):
                if chunk:
                    file.write(chunk)
                    file.flush()
    elif url_type == &#34;google_drive&#34;:
        log(&#34;&gt;&gt;&gt;&gt;&gt; URL is a Google Drive URL, downloading from Google Drive&#34;)
        # URL is in format
        # https://drive.google.com/file/d/&lt;FILE_ID&gt;/...
        # We want to extract the FILE_ID
        log(&#34;&gt;&gt;&gt;&gt;&gt; Extracting FILE_ID from URL&#34;)
        url_prefix = &#34;https://drive.google.com/file/d/&#34;
        if not url.startswith(url_prefix):
            raise ValueError(
                &#34;URL must start with https://drive.google.com/file/d/.&#34;
                f&#34;Invalid URL: {url}&#34;
            )
        file_id = url.removeprefix(url_prefix).split(&#34;/&#34;)[0]
        log(f&#34;&gt;&gt;&gt;&gt;&gt; FILE_ID: {file_id}&#34;)
        creds = get_credentials_from_env(
            scopes=[&#34;https://www.googleapis.com/auth/drive&#34;]
        )
        try:
            service = build(&#34;drive&#34;, &#34;v3&#34;, credentials=creds)
            request = service.files().get_media(fileId=file_id)  # pylint: disable=E1101
            fh = io.FileIO(fname, mode=&#34;wb&#34;)  # pylint: disable=C0103
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while done is False:
                status, done = downloader.next_chunk()
                log(f&#34;Downloading file... {int(status.progress() * 100)}%.&#34;)
        except HttpError as error:
            log(f&#34;HTTPError: {error}&#34;, &#34;error&#34;)
            raise error
    else:
        raise ValueError(&#34;Invalid URL type. Please set values to `url_type` parameter&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_url.tasks.dump_files"><code class="name flex">
<span>def <span class="ident">dump_files</span></span>(<span>file_path: str, partition_columns: List[str], save_path: str = '.', chunksize: int = 1000000, build_json_dataframe: bool = False, dataframe_key_column: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Dump files according to chunk size</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
# pylint: disable=R0913
def dump_files(
    file_path: str,
    partition_columns: List[str],
    save_path: str = &#34;.&#34;,
    chunksize: int = 10**6,
    build_json_dataframe: bool = False,
    dataframe_key_column: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Dump files according to chunk size
    &#34;&#34;&#34;
    event_id = datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)
    for idx, chunk in enumerate(pd.read_csv(Path(file_path), chunksize=chunksize)):
        log(f&#34;Dumping batch {idx} with size {chunksize}&#34;)
        handle_dataframe_chunk(
            dataframe=chunk,
            save_path=save_path,
            partition_columns=partition_columns,
            event_id=event_id,
            idx=idx,
            build_json_dataframe=build_json_dataframe,
            dataframe_key_column=dataframe_key_column,
        )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.utils.dump_url" href="index.html">pipelines.utils.dump_url</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.utils.dump_url.tasks.download_url" href="#pipelines.utils.dump_url.tasks.download_url">download_url</a></code></li>
<li><code><a title="pipelines.utils.dump_url.tasks.dump_files" href="#pipelines.utils.dump_url.tasks.dump_files">dump_files</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>