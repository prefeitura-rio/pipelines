<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.utils.dump_db.tasks API documentation</title>
<meta name="description" content="General purpose tasks for dumping database data." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.utils.dump_db.tasks</code></h1>
</header>
<section id="section-intro">
<p>General purpose tasks for dumping database data.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# pylint: disable=R0914
&#34;&#34;&#34;
General purpose tasks for dumping database data.
&#34;&#34;&#34;
from datetime import datetime, timedelta
from queue import Empty, Queue
from pathlib import Path
from threading import Event, Thread
from time import sleep, time
from typing import Dict, List, Union
from uuid import uuid4

import basedosdados as bd
import pandas as pd
from prefect import task

from pipelines.utils.dump_db.db import (
    Database,
    MySql,
    Oracle,
    SqlServer,
)
from pipelines.utils.dump_db.utils import (
    extract_last_partition_date,
    build_query_new_columns,
)
from pipelines.utils.elasticsearch_metrics.utils import (
    format_document,
    index_document,
)
from pipelines.utils.utils import (
    batch_to_dataframe,
    dataframe_to_csv,
    dataframe_to_parquet,
    parse_date_columns,
    clean_dataframe,
    to_partitions,
    parser_blobs_to_partition_dict,
    get_storage_blobs,
    remove_columns_accents,
)
from pipelines.constants import constants
from pipelines.utils.utils import log

DATABASE_MAPPING: Dict[str, Database] = {
    &#34;mysql&#34;: MySql,
    &#34;oracle&#34;: Oracle,
    &#34;sql_server&#34;: SqlServer,
}

# pylint: disable=too-many-arguments


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_get(
    database_type: str,
    hostname: str,
    port: int,
    user: str,
    password: str,
    database: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; Database:
    &#34;&#34;&#34;
    Returns a database object.

    Args:
        database_type: The type of the database.
        hostname: The hostname of the database.
        port: The port of the database.
        user: The username of the database.
        password: The password of the database.
        database: The database name.

    Returns:
        A database object.
    &#34;&#34;&#34;
    if database_type not in DATABASE_MAPPING:
        raise ValueError(f&#34;Unknown database type: {database_type}&#34;)
    return DATABASE_MAPPING[database_type](
        hostname=hostname,
        port=port,
        user=user,
        password=password,
        database=database,
    )


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_execute(
    database: Database,
    query: str,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Executes a query on the database.

    Args:
        database: The database object.
        query: The query to execute.
    &#34;&#34;&#34;
    start_time = time()
    log(f&#34;Executing query: {query}&#34;)
    database.execute_query(query)
    time_elapsed = time() - start_time
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;db_execute&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;db_execute&#34;: time_elapsed},
    )
    index_document(doc)


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_fetch(
    database: Database,
    batch_size: str,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
):
    &#34;&#34;&#34;
    Fetches the results of a query on the database.
    &#34;&#34;&#34;
    start_time = time()
    if batch_size == &#34;all&#34;:
        log(f&#34;columns: {database.get_columns()}&#34;)
        log(f&#34;All rows: { database.fetch_all()}&#34;)
    else:
        try:
            batch_size_no = int(batch_size)
        except ValueError as error:
            raise ValueError(f&#34;Invalid batch size: {batch_size}&#34;) from error
        log(f&#34;columns: {database.get_columns()}&#34;)
        log(f&#34;{batch_size_no} rows: {database.fetch_batch(batch_size_no)}&#34;)
    time_elapsed = time() - start_time
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;db_fetch&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;db_fetch&#34;: time_elapsed},
    )
    index_document(doc)


@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def format_partitioned_query(
    query: str,
    dataset_id: str,
    table_id: str,
    database_type: str,
    partition_columns: List[str] = None,
    lower_bound_date: str = None,
    date_format: str = None,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;
    Formats a query for fetching partitioned data.
    &#34;&#34;&#34;
    # If no partition column is specified, return the query as is.
    if not partition_columns or partition_columns[0] == &#34;&#34;:
        log(&#34;NO partition column specified. Returning query as is&#34;)
        return query

    partition_column = partition_columns[0]

    # Check if the table already exists in BigQuery.
    table = bd.Table(dataset_id, table_id)

    # If it doesn&#39;t, return the query as is, so we can fetch the whole table.
    if not table.table_exists(mode=&#34;staging&#34;):
        log(&#34;NO tables was found. Returning query as is&#34;)
        return query

    blobs = get_storage_blobs(dataset_id, table_id)

    # extract only partitioned folders
    storage_partitions_dict = parser_blobs_to_partition_dict(blobs)
    # get last partition date
    last_partition_date = extract_last_partition_date(
        storage_partitions_dict, date_format
    )

    if lower_bound_date == &#34;current_year&#34;:
        lower_bound_date = datetime.now().replace(month=1, day=1).strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_year: {lower_bound_date}&#34;)
    elif lower_bound_date == &#34;current_month&#34;:
        lower_bound_date = datetime.now().replace(day=1).strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_month: {lower_bound_date}&#34;)
    elif lower_bound_date == &#34;current_day&#34;:
        lower_bound_date = datetime.now().strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_day: {lower_bound_date}&#34;)

    if lower_bound_date:
        last_date = min(str(lower_bound_date), str(last_partition_date))
        log(f&#34;Using lower_bound_date: {last_date}&#34;)

    else:
        last_date = str(last_partition_date)
        log(f&#34;Using last_date from storage: {last_date}&#34;)

    # Using the last partition date, get the partitioned query.
    # `aux_name` must be unique and start with a letter, for better compatibility with
    # multiple DBMSs.
    aux_name = f&#34;a{uuid4().hex}&#34;[:8]

    log(
        f&#34;Partitioned DETECTED: {partition_column}, retuning a NEW QUERY &#34;
        &#34;with partitioned columns and filters&#34;
    )
    if database_type == &#34;oracle&#34;:
        oracle_date_format = &#34;YYYY-MM-DD&#34; if date_format == &#34;%Y-%m-%d&#34; else date_format

        return f&#34;&#34;&#34;
        with {aux_name} as ({query})
        select * from {aux_name}
        where {partition_column} &gt;= TO_DATE(&#39;{last_date}&#39;, &#39;{oracle_date_format}&#39;)
        &#34;&#34;&#34;

    return f&#34;&#34;&#34;
    with {aux_name} as ({query})
    select * from {aux_name}
    where {partition_column} &gt;= &#39;{last_date}&#39;
    &#34;&#34;&#34;


###############
#
# File
#
###############


@task
def parse_comma_separated_string_to_list(text: str) -&gt; List[str]:
    &#34;&#34;&#34;
    Parses a comma separated string to a list.

    Args:
        text: The text to parse.

    Returns:
        A list of strings.
    &#34;&#34;&#34;
    if text is None or not text:
        return []
    # Remove extras.
    text = text.replace(&#34;\n&#34;, &#34;&#34;)
    text = text.replace(&#34;\r&#34;, &#34;&#34;)
    text = text.replace(&#34;\t&#34;, &#34;&#34;)
    while &#34;,,&#34; in text:
        text = text.replace(&#34;,,&#34;, &#34;,&#34;)
    while text.endswith(&#34;,&#34;):
        text = text[:-1]
    result = [x.strip() for x in text.split(&#34;,&#34;)]
    result = [item for item in result if item != &#34;&#34; and item is not None]
    return result


@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
    nout=2,
)
def dump_batches_to_file(  # pylint: disable=too-many-locals,too-many-statements
    database: Database,
    batch_size: int,
    prepath: Union[str, Path],
    partition_columns: List[str] = None,
    batch_data_type: str = &#34;csv&#34;,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
) -&gt; Path:
    &#34;&#34;&#34;
    Dumps batches of data to FILE.
    &#34;&#34;&#34;
    # Get columns
    columns = database.get_columns()
    log(f&#34;Got columns: {columns}&#34;)

    new_query_cols = build_query_new_columns(table_columns=columns)
    log(&#34;New query columns without accents:&#34;)
    log(f&#34;{new_query_cols}&#34;)

    prepath = Path(prepath)
    log(f&#34;Got prepath: {prepath}&#34;)

    if not partition_columns or partition_columns[0] == &#34;&#34;:
        partition_column = None
    else:
        partition_column = partition_columns[0]

    if not partition_column:
        log(&#34;NO partition column specified! Writing unique files&#34;)
    else:
        log(f&#34;Partition column: {partition_column} FOUND!! Write to partitioned files&#34;)

    # Initialize queues
    batches = Queue()
    dataframes = Queue()

    # Define thread functions
    def thread_batch_to_dataframe(
        batches: Queue,
        dataframes: Queue,
        done: Event,
        columns: List[str],
        flow_name: str,
        labels: List[str],
        dataset_id: str,
        table_id: str,
    ):
        while not done.is_set():
            try:
                batch = batches.get(timeout=1)
                start_time = time()
                dataframe = batch_to_dataframe(batch, columns)
                elapsed_time = time() - start_time
                dataframes.put(dataframe)
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=&#34;batch_to_dataframe&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={&#34;batch_to_dataframe&#34;: elapsed_time},
                )
                index_document(doc)
                batches.task_done()
            except Empty:
                sleep(1)

    def thread_dataframe_to_csv(
        dataframes: Queue,
        done: Event,
        flow_name: str,
        labels: List[str],
        dataset_id: str,
        table_id: str,
        partition_column: str,
        partition_columns: List[str],
        prepath: Path,
        batch_data_type: str,
        eventid: str,
    ):
        while not done.is_set():
            try:
                # Get dataframe from queue
                dataframe: pd.DataFrame = dataframes.get(timeout=1)
                # Clean dataframe
                start_time = time()
                old_columns = dataframe.columns.tolist()
                dataframe.columns = remove_columns_accents(dataframe)
                new_columns_dict = dict(zip(old_columns, dataframe.columns.tolist()))
                dataframe = clean_dataframe(dataframe)
                elapsed_time = time() - start_time
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=&#34;clean_dataframe&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={&#34;clean_dataframe&#34;: elapsed_time},
                )
                index_document(doc)
                # Dump dataframe to file
                start_time = time()
                if partition_column:
                    dataframe, date_partition_columns = parse_date_columns(
                        dataframe, new_columns_dict[partition_column]
                    )
                    partitions = date_partition_columns + [
                        new_columns_dict[col] for col in partition_columns[1:]
                    ]
                    to_partitions(
                        data=dataframe,
                        partition_columns=partitions,
                        savepath=prepath,
                        data_type=batch_data_type,
                    )
                elif batch_data_type == &#34;csv&#34;:
                    dataframe_to_csv(dataframe, prepath / f&#34;{eventid}-{uuid4()}.csv&#34;)
                elif batch_data_type == &#34;parquet&#34;:
                    dataframe_to_parquet(
                        dataframe, prepath / f&#34;{eventid}-{uuid4()}.parquet&#34;
                    )
                elapsed_time = time() - start_time
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=f&#34;batch_to_{batch_data_type}&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={f&#34;batch_to_{batch_data_type}&#34;: elapsed_time},
                )
                index_document(doc)
                dataframes.task_done()
            except Empty:
                sleep(1)

    # Initialize threads
    done = Event()
    eventid = datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)
    worker_batch_to_dataframe = Thread(
        target=thread_batch_to_dataframe,
        args=(
            batches,
            dataframes,
            done,
            columns,
            flow_name,
            labels,
            dataset_id,
            table_id,
        ),
    )
    worker_dataframe_to_csv = Thread(
        target=thread_dataframe_to_csv,
        args=(
            dataframes,
            done,
            flow_name,
            labels,
            dataset_id,
            table_id,
            partition_column,
            partition_columns,
            prepath,
            batch_data_type,
            eventid,
        ),
    )
    worker_batch_to_dataframe.start()
    worker_dataframe_to_csv.start()

    # Dump batches
    start_fetch_batch = time()
    batch = database.fetch_batch(batch_size)
    time_fetch_batch = time() - start_fetch_batch
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;fetch_batch&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;fetch_batch&#34;: time_fetch_batch},
    )
    index_document(doc)
    idx = 0
    while len(batch) &gt; 0:
        if idx % 100 == 0:
            log(f&#34;Dumping batch {idx} with size {len(batch)}&#34;)
        # Add current batch to queue
        batches.put(batch)
        # Get next batch
        start_fetch_batch = time()
        batch = database.fetch_batch(batch_size)
        time_fetch_batch = time() - start_fetch_batch
        doc = format_document(
            flow_name=flow_name,
            labels=labels,
            event_type=&#34;fetch_batch&#34;,
            dataset_id=dataset_id,
            table_id=table_id,
            metrics={&#34;fetch_batch&#34;: time_fetch_batch},
        )
        index_document(doc)
        idx += 1

    log(&#34;Waiting for batches queue...&#34;)
    start_sleep = 1
    max_sleep = 300
    while batches.unfinished_tasks &gt; 0:
        sleep(start_sleep)
        start_sleep = min(start_sleep * 2, max_sleep)
        log(
            f&#34;Waiting for {batches.unfinished_tasks} batches to be parsed as dataframes...&#34;
        )
    batches.join()
    start_sleep = 1
    log(&#34;Waiting for dataframes queue...&#34;)
    while dataframes.unfinished_tasks &gt; 0:
        sleep(start_sleep)
        start_sleep = min(start_sleep * 2, max_sleep)
        log(f&#34;Waiting for {dataframes.unfinished_tasks} dataframes to be dumped...&#34;)
    dataframes.join()
    done.set()

    log(&#34;Waiting for threads to finish...&#34;)
    worker_batch_to_dataframe.join()
    worker_dataframe_to_csv.join()

    log(
        f&#34;Successfully dumped {idx} batches with size {len(batch)}, total of {idx*batch_size}&#34;
    )

    return prepath, idx</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.utils.dump_db.tasks.database_execute"><code class="name flex">
<span>def <span class="ident">database_execute</span></span>(<span>database: <a title="pipelines.utils.dump_db.db.Database" href="db.html#pipelines.utils.dump_db.db.Database">Database</a>, query: str, wait=None, flow_name: str = None, labels: List[str] = None, dataset_id: str = None, table_id: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a query on the database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>database</code></strong></dt>
<dd>The database object.</dd>
<dt><strong><code>query</code></strong></dt>
<dd>The query to execute.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_execute(
    database: Database,
    query: str,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
) -&gt; None:
    &#34;&#34;&#34;
    Executes a query on the database.

    Args:
        database: The database object.
        query: The query to execute.
    &#34;&#34;&#34;
    start_time = time()
    log(f&#34;Executing query: {query}&#34;)
    database.execute_query(query)
    time_elapsed = time() - start_time
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;db_execute&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;db_execute&#34;: time_elapsed},
    )
    index_document(doc)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_db.tasks.database_fetch"><code class="name flex">
<span>def <span class="ident">database_fetch</span></span>(<span>database: <a title="pipelines.utils.dump_db.db.Database" href="db.html#pipelines.utils.dump_db.db.Database">Database</a>, batch_size: str, wait=None, flow_name: str = None, labels: List[str] = None, dataset_id: str = None, table_id: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches the results of a query on the database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_fetch(
    database: Database,
    batch_size: str,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
):
    &#34;&#34;&#34;
    Fetches the results of a query on the database.
    &#34;&#34;&#34;
    start_time = time()
    if batch_size == &#34;all&#34;:
        log(f&#34;columns: {database.get_columns()}&#34;)
        log(f&#34;All rows: { database.fetch_all()}&#34;)
    else:
        try:
            batch_size_no = int(batch_size)
        except ValueError as error:
            raise ValueError(f&#34;Invalid batch size: {batch_size}&#34;) from error
        log(f&#34;columns: {database.get_columns()}&#34;)
        log(f&#34;{batch_size_no} rows: {database.fetch_batch(batch_size_no)}&#34;)
    time_elapsed = time() - start_time
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;db_fetch&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;db_fetch&#34;: time_elapsed},
    )
    index_document(doc)</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_db.tasks.database_get"><code class="name flex">
<span>def <span class="ident">database_get</span></span>(<span>database_type: str, hostname: str, port: int, user: str, password: str, database: str, wait=None) ‑> <a title="pipelines.utils.dump_db.db.Database" href="db.html#pipelines.utils.dump_db.db.Database">Database</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns a database object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>database_type</code></strong></dt>
<dd>The type of the database.</dd>
<dt><strong><code>hostname</code></strong></dt>
<dd>The hostname of the database.</dd>
<dt><strong><code>port</code></strong></dt>
<dd>The port of the database.</dd>
<dt><strong><code>user</code></strong></dt>
<dd>The username of the database.</dd>
<dt><strong><code>password</code></strong></dt>
<dd>The password of the database.</dd>
<dt><strong><code>database</code></strong></dt>
<dd>The database name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A database object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def database_get(
    database_type: str,
    hostname: str,
    port: int,
    user: str,
    password: str,
    database: str,
    wait=None,  # pylint: disable=unused-argument
) -&gt; Database:
    &#34;&#34;&#34;
    Returns a database object.

    Args:
        database_type: The type of the database.
        hostname: The hostname of the database.
        port: The port of the database.
        user: The username of the database.
        password: The password of the database.
        database: The database name.

    Returns:
        A database object.
    &#34;&#34;&#34;
    if database_type not in DATABASE_MAPPING:
        raise ValueError(f&#34;Unknown database type: {database_type}&#34;)
    return DATABASE_MAPPING[database_type](
        hostname=hostname,
        port=port,
        user=user,
        password=password,
        database=database,
    )</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_db.tasks.dump_batches_to_file"><code class="name flex">
<span>def <span class="ident">dump_batches_to_file</span></span>(<span>database: <a title="pipelines.utils.dump_db.db.Database" href="db.html#pipelines.utils.dump_db.db.Database">Database</a>, batch_size: int, prepath: Union[str, pathlib.Path], partition_columns: List[str] = None, batch_data_type: str = 'csv', wait=None, flow_name: str = None, labels: List[str] = None, dataset_id: str = None, table_id: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Dumps batches of data to FILE.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
    nout=2,
)
def dump_batches_to_file(  # pylint: disable=too-many-locals,too-many-statements
    database: Database,
    batch_size: int,
    prepath: Union[str, Path],
    partition_columns: List[str] = None,
    batch_data_type: str = &#34;csv&#34;,
    wait=None,  # pylint: disable=unused-argument
    flow_name: str = None,
    labels: List[str] = None,
    dataset_id: str = None,
    table_id: str = None,
) -&gt; Path:
    &#34;&#34;&#34;
    Dumps batches of data to FILE.
    &#34;&#34;&#34;
    # Get columns
    columns = database.get_columns()
    log(f&#34;Got columns: {columns}&#34;)

    new_query_cols = build_query_new_columns(table_columns=columns)
    log(&#34;New query columns without accents:&#34;)
    log(f&#34;{new_query_cols}&#34;)

    prepath = Path(prepath)
    log(f&#34;Got prepath: {prepath}&#34;)

    if not partition_columns or partition_columns[0] == &#34;&#34;:
        partition_column = None
    else:
        partition_column = partition_columns[0]

    if not partition_column:
        log(&#34;NO partition column specified! Writing unique files&#34;)
    else:
        log(f&#34;Partition column: {partition_column} FOUND!! Write to partitioned files&#34;)

    # Initialize queues
    batches = Queue()
    dataframes = Queue()

    # Define thread functions
    def thread_batch_to_dataframe(
        batches: Queue,
        dataframes: Queue,
        done: Event,
        columns: List[str],
        flow_name: str,
        labels: List[str],
        dataset_id: str,
        table_id: str,
    ):
        while not done.is_set():
            try:
                batch = batches.get(timeout=1)
                start_time = time()
                dataframe = batch_to_dataframe(batch, columns)
                elapsed_time = time() - start_time
                dataframes.put(dataframe)
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=&#34;batch_to_dataframe&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={&#34;batch_to_dataframe&#34;: elapsed_time},
                )
                index_document(doc)
                batches.task_done()
            except Empty:
                sleep(1)

    def thread_dataframe_to_csv(
        dataframes: Queue,
        done: Event,
        flow_name: str,
        labels: List[str],
        dataset_id: str,
        table_id: str,
        partition_column: str,
        partition_columns: List[str],
        prepath: Path,
        batch_data_type: str,
        eventid: str,
    ):
        while not done.is_set():
            try:
                # Get dataframe from queue
                dataframe: pd.DataFrame = dataframes.get(timeout=1)
                # Clean dataframe
                start_time = time()
                old_columns = dataframe.columns.tolist()
                dataframe.columns = remove_columns_accents(dataframe)
                new_columns_dict = dict(zip(old_columns, dataframe.columns.tolist()))
                dataframe = clean_dataframe(dataframe)
                elapsed_time = time() - start_time
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=&#34;clean_dataframe&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={&#34;clean_dataframe&#34;: elapsed_time},
                )
                index_document(doc)
                # Dump dataframe to file
                start_time = time()
                if partition_column:
                    dataframe, date_partition_columns = parse_date_columns(
                        dataframe, new_columns_dict[partition_column]
                    )
                    partitions = date_partition_columns + [
                        new_columns_dict[col] for col in partition_columns[1:]
                    ]
                    to_partitions(
                        data=dataframe,
                        partition_columns=partitions,
                        savepath=prepath,
                        data_type=batch_data_type,
                    )
                elif batch_data_type == &#34;csv&#34;:
                    dataframe_to_csv(dataframe, prepath / f&#34;{eventid}-{uuid4()}.csv&#34;)
                elif batch_data_type == &#34;parquet&#34;:
                    dataframe_to_parquet(
                        dataframe, prepath / f&#34;{eventid}-{uuid4()}.parquet&#34;
                    )
                elapsed_time = time() - start_time
                doc = format_document(
                    flow_name=flow_name,
                    labels=labels,
                    event_type=f&#34;batch_to_{batch_data_type}&#34;,
                    dataset_id=dataset_id,
                    table_id=table_id,
                    metrics={f&#34;batch_to_{batch_data_type}&#34;: elapsed_time},
                )
                index_document(doc)
                dataframes.task_done()
            except Empty:
                sleep(1)

    # Initialize threads
    done = Event()
    eventid = datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)
    worker_batch_to_dataframe = Thread(
        target=thread_batch_to_dataframe,
        args=(
            batches,
            dataframes,
            done,
            columns,
            flow_name,
            labels,
            dataset_id,
            table_id,
        ),
    )
    worker_dataframe_to_csv = Thread(
        target=thread_dataframe_to_csv,
        args=(
            dataframes,
            done,
            flow_name,
            labels,
            dataset_id,
            table_id,
            partition_column,
            partition_columns,
            prepath,
            batch_data_type,
            eventid,
        ),
    )
    worker_batch_to_dataframe.start()
    worker_dataframe_to_csv.start()

    # Dump batches
    start_fetch_batch = time()
    batch = database.fetch_batch(batch_size)
    time_fetch_batch = time() - start_fetch_batch
    doc = format_document(
        flow_name=flow_name,
        labels=labels,
        event_type=&#34;fetch_batch&#34;,
        dataset_id=dataset_id,
        table_id=table_id,
        metrics={&#34;fetch_batch&#34;: time_fetch_batch},
    )
    index_document(doc)
    idx = 0
    while len(batch) &gt; 0:
        if idx % 100 == 0:
            log(f&#34;Dumping batch {idx} with size {len(batch)}&#34;)
        # Add current batch to queue
        batches.put(batch)
        # Get next batch
        start_fetch_batch = time()
        batch = database.fetch_batch(batch_size)
        time_fetch_batch = time() - start_fetch_batch
        doc = format_document(
            flow_name=flow_name,
            labels=labels,
            event_type=&#34;fetch_batch&#34;,
            dataset_id=dataset_id,
            table_id=table_id,
            metrics={&#34;fetch_batch&#34;: time_fetch_batch},
        )
        index_document(doc)
        idx += 1

    log(&#34;Waiting for batches queue...&#34;)
    start_sleep = 1
    max_sleep = 300
    while batches.unfinished_tasks &gt; 0:
        sleep(start_sleep)
        start_sleep = min(start_sleep * 2, max_sleep)
        log(
            f&#34;Waiting for {batches.unfinished_tasks} batches to be parsed as dataframes...&#34;
        )
    batches.join()
    start_sleep = 1
    log(&#34;Waiting for dataframes queue...&#34;)
    while dataframes.unfinished_tasks &gt; 0:
        sleep(start_sleep)
        start_sleep = min(start_sleep * 2, max_sleep)
        log(f&#34;Waiting for {dataframes.unfinished_tasks} dataframes to be dumped...&#34;)
    dataframes.join()
    done.set()

    log(&#34;Waiting for threads to finish...&#34;)
    worker_batch_to_dataframe.join()
    worker_dataframe_to_csv.join()

    log(
        f&#34;Successfully dumped {idx} batches with size {len(batch)}, total of {idx*batch_size}&#34;
    )

    return prepath, idx</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_db.tasks.format_partitioned_query"><code class="name flex">
<span>def <span class="ident">format_partitioned_query</span></span>(<span>query: str, dataset_id: str, table_id: str, database_type: str, partition_columns: List[str] = None, lower_bound_date: str = None, date_format: str = None, wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Formats a query for fetching partitioned data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.TASK_MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.TASK_RETRY_DELAY.value),
)
def format_partitioned_query(
    query: str,
    dataset_id: str,
    table_id: str,
    database_type: str,
    partition_columns: List[str] = None,
    lower_bound_date: str = None,
    date_format: str = None,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;
    Formats a query for fetching partitioned data.
    &#34;&#34;&#34;
    # If no partition column is specified, return the query as is.
    if not partition_columns or partition_columns[0] == &#34;&#34;:
        log(&#34;NO partition column specified. Returning query as is&#34;)
        return query

    partition_column = partition_columns[0]

    # Check if the table already exists in BigQuery.
    table = bd.Table(dataset_id, table_id)

    # If it doesn&#39;t, return the query as is, so we can fetch the whole table.
    if not table.table_exists(mode=&#34;staging&#34;):
        log(&#34;NO tables was found. Returning query as is&#34;)
        return query

    blobs = get_storage_blobs(dataset_id, table_id)

    # extract only partitioned folders
    storage_partitions_dict = parser_blobs_to_partition_dict(blobs)
    # get last partition date
    last_partition_date = extract_last_partition_date(
        storage_partitions_dict, date_format
    )

    if lower_bound_date == &#34;current_year&#34;:
        lower_bound_date = datetime.now().replace(month=1, day=1).strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_year: {lower_bound_date}&#34;)
    elif lower_bound_date == &#34;current_month&#34;:
        lower_bound_date = datetime.now().replace(day=1).strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_month: {lower_bound_date}&#34;)
    elif lower_bound_date == &#34;current_day&#34;:
        lower_bound_date = datetime.now().strftime(&#34;%Y-%m-%d&#34;)
        log(f&#34;Using lower_bound_date current_day: {lower_bound_date}&#34;)

    if lower_bound_date:
        last_date = min(str(lower_bound_date), str(last_partition_date))
        log(f&#34;Using lower_bound_date: {last_date}&#34;)

    else:
        last_date = str(last_partition_date)
        log(f&#34;Using last_date from storage: {last_date}&#34;)

    # Using the last partition date, get the partitioned query.
    # `aux_name` must be unique and start with a letter, for better compatibility with
    # multiple DBMSs.
    aux_name = f&#34;a{uuid4().hex}&#34;[:8]

    log(
        f&#34;Partitioned DETECTED: {partition_column}, retuning a NEW QUERY &#34;
        &#34;with partitioned columns and filters&#34;
    )
    if database_type == &#34;oracle&#34;:
        oracle_date_format = &#34;YYYY-MM-DD&#34; if date_format == &#34;%Y-%m-%d&#34; else date_format

        return f&#34;&#34;&#34;
        with {aux_name} as ({query})
        select * from {aux_name}
        where {partition_column} &gt;= TO_DATE(&#39;{last_date}&#39;, &#39;{oracle_date_format}&#39;)
        &#34;&#34;&#34;

    return f&#34;&#34;&#34;
    with {aux_name} as ({query})
    select * from {aux_name}
    where {partition_column} &gt;= &#39;{last_date}&#39;
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="pipelines.utils.dump_db.tasks.parse_comma_separated_string_to_list"><code class="name flex">
<span>def <span class="ident">parse_comma_separated_string_to_list</span></span>(<span>text: str) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Parses a comma separated string to a list.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong></dt>
<dd>The text to parse.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of strings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def parse_comma_separated_string_to_list(text: str) -&gt; List[str]:
    &#34;&#34;&#34;
    Parses a comma separated string to a list.

    Args:
        text: The text to parse.

    Returns:
        A list of strings.
    &#34;&#34;&#34;
    if text is None or not text:
        return []
    # Remove extras.
    text = text.replace(&#34;\n&#34;, &#34;&#34;)
    text = text.replace(&#34;\r&#34;, &#34;&#34;)
    text = text.replace(&#34;\t&#34;, &#34;&#34;)
    while &#34;,,&#34; in text:
        text = text.replace(&#34;,,&#34;, &#34;,&#34;)
    while text.endswith(&#34;,&#34;):
        text = text[:-1]
    result = [x.strip() for x in text.split(&#34;,&#34;)]
    result = [item for item in result if item != &#34;&#34; and item is not None]
    return result</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.utils.dump_db" href="index.html">pipelines.utils.dump_db</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.utils.dump_db.tasks.database_execute" href="#pipelines.utils.dump_db.tasks.database_execute">database_execute</a></code></li>
<li><code><a title="pipelines.utils.dump_db.tasks.database_fetch" href="#pipelines.utils.dump_db.tasks.database_fetch">database_fetch</a></code></li>
<li><code><a title="pipelines.utils.dump_db.tasks.database_get" href="#pipelines.utils.dump_db.tasks.database_get">database_get</a></code></li>
<li><code><a title="pipelines.utils.dump_db.tasks.dump_batches_to_file" href="#pipelines.utils.dump_db.tasks.dump_batches_to_file">dump_batches_to_file</a></code></li>
<li><code><a title="pipelines.utils.dump_db.tasks.format_partitioned_query" href="#pipelines.utils.dump_db.tasks.format_partitioned_query">format_partitioned_query</a></code></li>
<li><code><a title="pipelines.utils.dump_db.tasks.parse_comma_separated_string_to_list" href="#pipelines.utils.dump_db.tasks.parse_comma_separated_string_to_list">parse_comma_separated_string_to_list</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>