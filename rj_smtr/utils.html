<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.rj_smtr.utils API documentation</title>
<meta name="description" content="General purpose functions for rj_smtr" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.rj_smtr.utils</code></h1>
</header>
<section id="section-intro">
<p>General purpose functions for rj_smtr</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
General purpose functions for rj_smtr
&#34;&#34;&#34;

from ftplib import FTP
from pathlib import Path

import basedosdados as bd
from basedosdados import Table
import pandas as pd
from pipelines.rj_smtr.implicit_ftp import ImplicitFtpTls

from pipelines.utils.utils import log
from pipelines.utils.utils import (
    get_vault_secret,
    send_discord_message,
    get_redis_client,
)
from pipelines.rj_smtr.constants import constants

# Set BD config to run on cloud #
bd.config.from_file = True


def log_critical(message: str, secret_path: str = constants.CRITICAL_SECRET_PATH.value):
    &#34;&#34;&#34;Logs message to critical discord channel specified

    Args:
        message (str): Message to post on the channel
        secret_path (str, optional): Secret path storing the webhook to critical channel.
        Defaults to constants.CRITICAL_SECRETPATH.value.

    &#34;&#34;&#34;
    url = get_vault_secret(secret_path=secret_path)[&#34;data&#34;][&#34;url&#34;]
    return send_discord_message(message=message, webhook_url=url)


def create_or_append_table(
    dataset_id: str, table_id: str, path: str, partitions: str = None
):
    &#34;&#34;&#34;Conditionally create table or append data to its relative GCS folder.

    Args:
        dataset_id (str): target dataset_id on BigQuery
        table_id (str): target table_id on BigQuery
        path (str): Path to .csv data file
    &#34;&#34;&#34;
    tb_obj = Table(table_id=table_id, dataset_id=dataset_id)
    if not tb_obj.table_exists(&#34;staging&#34;):
        log(&#34;Table does not exist in STAGING, creating table...&#34;)
        dirpath = path.split(partitions)[0]
        tb_obj.create(
            path=dirpath,
            if_table_exists=&#34;pass&#34;,
            if_storage_data_exists=&#34;replace&#34;,
            if_table_config_exists=&#34;replace&#34;,
        )
        log(&#34;Table created in STAGING&#34;)
    else:
        log(&#34;Table already exists in STAGING, appending to it...&#34;)
        tb_obj.append(
            filepath=path, if_exists=&#34;replace&#34;, timeout=600, partitions=partitions
        )
        log(&#34;Appended to table on STAGING successfully.&#34;)


def generate_df_and_save(data: dict, fname: Path):
    &#34;&#34;&#34;Save DataFrame as csv

    Args:
        data (dict): dict with the data which to build the DataFrame
        fname (Path): _description_
    &#34;&#34;&#34;
    # Generate dataframe
    dataframe = pd.DataFrame()
    dataframe[data[&#34;key_column&#34;]] = [
        piece[data[&#34;key_column&#34;]] for piece in data[&#34;data&#34;]
    ]
    dataframe[&#34;content&#34;] = list(data[&#34;data&#34;])

    # Save dataframe to CSV
    dataframe.to_csv(fname, index=False)


def bq_project(kind: str = &#34;bigquery_prod&#34;):
    &#34;&#34;&#34;Get the set BigQuery project_id

    Args:
        kind (str, optional): Which client to get the project name from.
        Options are &#39;bigquery_staging&#39;, &#39;bigquery_prod&#39; and &#39;storage_staging&#39;
        Defaults to &#39;bigquery_prod&#39;.

    Returns:
        str: the requested project_id
    &#34;&#34;&#34;
    return bd.upload.base.Base().client[kind].project


def get_table_min_max_value(  # pylint: disable=R0913
    query_project_id: str,
    dataset_id: str,
    table_id: str,
    field_name: str,
    kind: str,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;Query a table to get the maximum value for the chosen field.
    Useful to incrementally materialize tables via DBT

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        field_name (str): column name to query
        kind (str): which value to get. Accepts min and max
    &#34;&#34;&#34;
    query = f&#34;&#34;&#34;
        SELECT
            {kind}({field_name})
        FROM {query_project_id}.{dataset_id}.{table_id}
    &#34;&#34;&#34;
    result = bd.read_sql(query=query, billing_project_id=bq_project())

    return result.iloc[0][0]


def get_last_run_timestamp(dataset_id: str, table_id: str, mode: str = &#34;prod&#34;):
    &#34;&#34;&#34;
    Query redis to retrive the time for when the last materialization
    ran.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        eg: if you have a model defined in the file &lt;filename&gt;.sql,
        the table_id should be &lt;filename&gt;
        mode (str):

    Returns:
        Union[str, None]: _description_
    &#34;&#34;&#34;
    redis_client = get_redis_client()
    key = dataset_id + &#34;.&#34; + table_id
    log(f&#34;Fetching key {key} from redis, working on mode {mode}&#34;)
    if mode == &#34;dev&#34;:
        key = f&#34;{mode}.{key}&#34;
    runs = redis_client.get(key)
    # if runs is None:
    #     redis_client.set(key, &#34;&#34;)
    try:
        last_run_timestamp = runs[&#34;last_run_timestamp&#34;]
    except KeyError:
        return None
    except TypeError:
        return None
    log(f&#34;Got value {last_run_timestamp}&#34;)
    return last_run_timestamp


def map_dict_keys(data: dict, mapping: dict) -&gt; None:
    &#34;&#34;&#34;
    Map old keys to new keys in a dict.
    &#34;&#34;&#34;
    for old_key, new_key in mapping.items():
        data[new_key] = data.pop(old_key)
    return data


def connect_ftp(
    secret_path: str = constants.FTPS_SECRET_PATH.value, secure: bool = True
):
    &#34;&#34;&#34;Connect to FTP

    Returns:
        ImplicitFTP_TLS: ftp client
    &#34;&#34;&#34;

    ftp_data = get_vault_secret(secret_path)[&#34;data&#34;]
    if secure:
        ftp_client = ImplicitFtpTls()
    else:
        ftp_client = FTP()
    ftp_client.connect(host=ftp_data[&#34;host&#34;], port=int(ftp_data[&#34;port&#34;]))
    ftp_client.login(user=ftp_data[&#34;username&#34;], passwd=ftp_data[&#34;pwd&#34;])
    if secure:
        ftp_client.prot_p()
    return ftp_client


def safe_cast(val, to_type, default=None):
    &#34;&#34;&#34;
    Safe cast value.
    &#34;&#34;&#34;
    try:
        return to_type(val)
    except ValueError:
        return default</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.rj_smtr.utils.bq_project"><code class="name flex">
<span>def <span class="ident">bq_project</span></span>(<span>kind: str = 'bigquery_prod')</span>
</code></dt>
<dd>
<div class="desc"><p>Get the set BigQuery project_id</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Which client to get the project name from.</dd>
</dl>
<p>Options are 'bigquery_staging', 'bigquery_prod' and 'storage_staging'
Defaults to 'bigquery_prod'.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>the requested project_id</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bq_project(kind: str = &#34;bigquery_prod&#34;):
    &#34;&#34;&#34;Get the set BigQuery project_id

    Args:
        kind (str, optional): Which client to get the project name from.
        Options are &#39;bigquery_staging&#39;, &#39;bigquery_prod&#39; and &#39;storage_staging&#39;
        Defaults to &#39;bigquery_prod&#39;.

    Returns:
        str: the requested project_id
    &#34;&#34;&#34;
    return bd.upload.base.Base().client[kind].project</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.connect_ftp"><code class="name flex">
<span>def <span class="ident">connect_ftp</span></span>(<span>secret_path: str = 'smtr_rdo_ftps', secure: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Connect to FTP</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImplicitFTP_TLS</code></dt>
<dd>ftp client</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def connect_ftp(
    secret_path: str = constants.FTPS_SECRET_PATH.value, secure: bool = True
):
    &#34;&#34;&#34;Connect to FTP

    Returns:
        ImplicitFTP_TLS: ftp client
    &#34;&#34;&#34;

    ftp_data = get_vault_secret(secret_path)[&#34;data&#34;]
    if secure:
        ftp_client = ImplicitFtpTls()
    else:
        ftp_client = FTP()
    ftp_client.connect(host=ftp_data[&#34;host&#34;], port=int(ftp_data[&#34;port&#34;]))
    ftp_client.login(user=ftp_data[&#34;username&#34;], passwd=ftp_data[&#34;pwd&#34;])
    if secure:
        ftp_client.prot_p()
    return ftp_client</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.create_or_append_table"><code class="name flex">
<span>def <span class="ident">create_or_append_table</span></span>(<span>dataset_id: str, table_id: str, path: str, partitions: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Conditionally create table or append data to its relative GCS folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>target dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>target table_id on BigQuery</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to .csv data file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_or_append_table(
    dataset_id: str, table_id: str, path: str, partitions: str = None
):
    &#34;&#34;&#34;Conditionally create table or append data to its relative GCS folder.

    Args:
        dataset_id (str): target dataset_id on BigQuery
        table_id (str): target table_id on BigQuery
        path (str): Path to .csv data file
    &#34;&#34;&#34;
    tb_obj = Table(table_id=table_id, dataset_id=dataset_id)
    if not tb_obj.table_exists(&#34;staging&#34;):
        log(&#34;Table does not exist in STAGING, creating table...&#34;)
        dirpath = path.split(partitions)[0]
        tb_obj.create(
            path=dirpath,
            if_table_exists=&#34;pass&#34;,
            if_storage_data_exists=&#34;replace&#34;,
            if_table_config_exists=&#34;replace&#34;,
        )
        log(&#34;Table created in STAGING&#34;)
    else:
        log(&#34;Table already exists in STAGING, appending to it...&#34;)
        tb_obj.append(
            filepath=path, if_exists=&#34;replace&#34;, timeout=600, partitions=partitions
        )
        log(&#34;Appended to table on STAGING successfully.&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.generate_df_and_save"><code class="name flex">
<span>def <span class="ident">generate_df_and_save</span></span>(<span>data: dict, fname: pathlib.Path)</span>
</code></dt>
<dd>
<div class="desc"><p>Save DataFrame as csv</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>dict</code></dt>
<dd>dict with the data which to build the DataFrame</dd>
<dt><strong><code>fname</code></strong> :&ensp;<code>Path</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_df_and_save(data: dict, fname: Path):
    &#34;&#34;&#34;Save DataFrame as csv

    Args:
        data (dict): dict with the data which to build the DataFrame
        fname (Path): _description_
    &#34;&#34;&#34;
    # Generate dataframe
    dataframe = pd.DataFrame()
    dataframe[data[&#34;key_column&#34;]] = [
        piece[data[&#34;key_column&#34;]] for piece in data[&#34;data&#34;]
    ]
    dataframe[&#34;content&#34;] = list(data[&#34;data&#34;])

    # Save dataframe to CSV
    dataframe.to_csv(fname, index=False)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.get_last_run_timestamp"><code class="name flex">
<span>def <span class="ident">get_last_run_timestamp</span></span>(<span>dataset_id: str, table_id: str, mode: str = 'prod')</span>
</code></dt>
<dd>
<div class="desc"><p>Query redis to retrive the time for when the last materialization
ran.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model filename on the queries repo.</dd>
<dt><strong><code>eg</code></strong></dt>
<dd>if you have a model defined in the file <filename>.sql,</dd>
</dl>
<p>the table_id should be <filename>
mode (str):</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[str, None]</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_run_timestamp(dataset_id: str, table_id: str, mode: str = &#34;prod&#34;):
    &#34;&#34;&#34;
    Query redis to retrive the time for when the last materialization
    ran.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        eg: if you have a model defined in the file &lt;filename&gt;.sql,
        the table_id should be &lt;filename&gt;
        mode (str):

    Returns:
        Union[str, None]: _description_
    &#34;&#34;&#34;
    redis_client = get_redis_client()
    key = dataset_id + &#34;.&#34; + table_id
    log(f&#34;Fetching key {key} from redis, working on mode {mode}&#34;)
    if mode == &#34;dev&#34;:
        key = f&#34;{mode}.{key}&#34;
    runs = redis_client.get(key)
    # if runs is None:
    #     redis_client.set(key, &#34;&#34;)
    try:
        last_run_timestamp = runs[&#34;last_run_timestamp&#34;]
    except KeyError:
        return None
    except TypeError:
        return None
    log(f&#34;Got value {last_run_timestamp}&#34;)
    return last_run_timestamp</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.get_table_min_max_value"><code class="name flex">
<span>def <span class="ident">get_table_min_max_value</span></span>(<span>query_project_id: str, dataset_id: str, table_id: str, field_name: str, kind: str, wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Query a table to get the maximum value for the chosen field.
Useful to incrementally materialize tables via DBT</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>field_name</code></strong> :&ensp;<code>str</code></dt>
<dd>column name to query</dd>
<dt><strong><code>kind</code></strong> :&ensp;<code>str</code></dt>
<dd>which value to get. Accepts min and max</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_table_min_max_value(  # pylint: disable=R0913
    query_project_id: str,
    dataset_id: str,
    table_id: str,
    field_name: str,
    kind: str,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;Query a table to get the maximum value for the chosen field.
    Useful to incrementally materialize tables via DBT

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        field_name (str): column name to query
        kind (str): which value to get. Accepts min and max
    &#34;&#34;&#34;
    query = f&#34;&#34;&#34;
        SELECT
            {kind}({field_name})
        FROM {query_project_id}.{dataset_id}.{table_id}
    &#34;&#34;&#34;
    result = bd.read_sql(query=query, billing_project_id=bq_project())

    return result.iloc[0][0]</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.log_critical"><code class="name flex">
<span>def <span class="ident">log_critical</span></span>(<span>message: str, secret_path: str = 'critical_webhook')</span>
</code></dt>
<dd>
<div class="desc"><p>Logs message to critical discord channel specified</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message</code></strong> :&ensp;<code>str</code></dt>
<dd>Message to post on the channel</dd>
<dt><strong><code>secret_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Secret path storing the webhook to critical channel.</dd>
</dl>
<p>Defaults to constants.CRITICAL_SECRETPATH.value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_critical(message: str, secret_path: str = constants.CRITICAL_SECRET_PATH.value):
    &#34;&#34;&#34;Logs message to critical discord channel specified

    Args:
        message (str): Message to post on the channel
        secret_path (str, optional): Secret path storing the webhook to critical channel.
        Defaults to constants.CRITICAL_SECRETPATH.value.

    &#34;&#34;&#34;
    url = get_vault_secret(secret_path=secret_path)[&#34;data&#34;][&#34;url&#34;]
    return send_discord_message(message=message, webhook_url=url)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.map_dict_keys"><code class="name flex">
<span>def <span class="ident">map_dict_keys</span></span>(<span>data: dict, mapping: dict) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Map old keys to new keys in a dict.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_dict_keys(data: dict, mapping: dict) -&gt; None:
    &#34;&#34;&#34;
    Map old keys to new keys in a dict.
    &#34;&#34;&#34;
    for old_key, new_key in mapping.items():
        data[new_key] = data.pop(old_key)
    return data</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.utils.safe_cast"><code class="name flex">
<span>def <span class="ident">safe_cast</span></span>(<span>val, to_type, default=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Safe cast value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def safe_cast(val, to_type, default=None):
    &#34;&#34;&#34;
    Safe cast value.
    &#34;&#34;&#34;
    try:
        return to_type(val)
    except ValueError:
        return default</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.rj_smtr" href="index.html">pipelines.rj_smtr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.rj_smtr.utils.bq_project" href="#pipelines.rj_smtr.utils.bq_project">bq_project</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.connect_ftp" href="#pipelines.rj_smtr.utils.connect_ftp">connect_ftp</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.create_or_append_table" href="#pipelines.rj_smtr.utils.create_or_append_table">create_or_append_table</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.generate_df_and_save" href="#pipelines.rj_smtr.utils.generate_df_and_save">generate_df_and_save</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.get_last_run_timestamp" href="#pipelines.rj_smtr.utils.get_last_run_timestamp">get_last_run_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.get_table_min_max_value" href="#pipelines.rj_smtr.utils.get_table_min_max_value">get_table_min_max_value</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.log_critical" href="#pipelines.rj_smtr.utils.log_critical">log_critical</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.map_dict_keys" href="#pipelines.rj_smtr.utils.map_dict_keys">map_dict_keys</a></code></li>
<li><code><a title="pipelines.rj_smtr.utils.safe_cast" href="#pipelines.rj_smtr.utils.safe_cast">safe_cast</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>