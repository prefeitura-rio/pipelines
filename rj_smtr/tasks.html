<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipelines.rj_smtr.tasks API documentation</title>
<meta name="description" content="Tasks for rj_smtr" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.rj_smtr.tasks</code></h1>
</header>
<section id="section-intro">
<p>Tasks for rj_smtr</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# pylint: disable=W0703, W0511
&#34;&#34;&#34;
Tasks for rj_smtr
&#34;&#34;&#34;
from datetime import datetime, timedelta
import json
import os
from pathlib import Path
import traceback
from typing import Dict, List
import io

from basedosdados import Storage, Table
import basedosdados as bd
from dbt_client import DbtClient
import pandas as pd
import pendulum
from prefect import task
from pytz import timezone
import requests

from pipelines.rj_smtr.constants import constants
from pipelines.rj_smtr.utils import (
    create_or_append_table,
    bq_project,
    get_table_min_max_value,
    get_last_run_timestamp,
    log_critical,
)
from pipelines.utils.execute_dbt_model.utils import get_dbt_client
from pipelines.utils.utils import log, get_redis_client, get_vault_secret

from pipelines.utils.tasks import get_now_date

###############
#
# DBT
#
###############


@task
def get_local_dbt_client(host: str, port: int):
    &#34;&#34;&#34;Set a DBT client for running CLI commands. Requires
    building container image for your queries repository.

    Args:
        host (str): hostname. When running locally, usually &#39;localhost&#39;
        port (int): the port number in which the DBT rpc is running

    Returns:
        DbtClient: object used to run DBT commands.
    &#34;&#34;&#34;
    return get_dbt_client(host=host, port=port)


@task(max_retries=3, retry_delay=timedelta(seconds=10))
def build_incremental_model(  # pylint: disable=too-many-arguments
    dbt_client: DbtClient,
    dataset_id: str,
    base_table_id: str,
    mat_table_id: str,
    field_name: str = &#34;data_versao&#34;,
    refresh: bool = False,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;
        Utility task for backfilling table in predetermined steps.
        Assumes the step sizes will be defined on the .sql file.

    Args:
        dbt_client (DbtClient): DBT interface object
        dataset_id (str): Dataset id on BigQuery
        base_table_id (str): Base table from which to materialize (usually, an external table)
        mat_table_id (str): Target table id for materialization
        field_name (str, optional): Key field (column) for dbt incremental filters.
        Defaults to &#34;data_versao&#34;.
        refresh (bool, optional): If True, rebuild the table from scratch. Defaults to False.
        wait (NoneType, optional): Placeholder parameter, used to wait previous tasks finish.
        Defaults to None.

    Returns:
        bool: whether the table was fully built or not.
    &#34;&#34;&#34;

    query_project_id = bq_project()
    last_mat_date = get_table_min_max_value(
        query_project_id, dataset_id, mat_table_id, field_name, &#34;max&#34;
    )
    last_base_date = get_table_min_max_value(
        query_project_id, dataset_id, base_table_id, field_name, &#34;max&#34;
    )
    log(
        f&#34;&#34;&#34;
    Base table last version: {last_base_date}
    Materialized table last version: {last_mat_date}
    &#34;&#34;&#34;
    )
    run_command = f&#34;run --select models/{dataset_id}/{mat_table_id}.sql&#34;

    if refresh:
        log(&#34;Running in full refresh mode&#34;)
        log(f&#34;DBT will run the following command:\n{run_command+&#39; --full-refresh&#39;}&#34;)
        dbt_client.cli(run_command + &#34; --full-refresh&#34;, sync=True)
        last_mat_date = get_table_min_max_value(
            query_project_id, dataset_id, mat_table_id, field_name, &#34;max&#34;
        )

    if last_base_date &gt; last_mat_date:
        log(&#34;Running interval step materialization&#34;)
        log(f&#34;DBT will run the following command:\n{run_command}&#34;)
        while last_base_date &gt; last_mat_date:
            running = dbt_client.cli(run_command, sync=True)
            last_mat_date = get_table_min_max_value(
                query_project_id,
                dataset_id,
                mat_table_id,
                field_name,
                &#34;max&#34;,
                wait=running,
            )
            log(f&#34;After this step, materialized table last version is: {last_mat_date}&#34;)
            if last_mat_date == last_base_date:
                log(&#34;Materialized table reached base table version!&#34;)
                return True
    log(&#34;Did not run interval step materialization...&#34;)
    return False


###############
#
# Local file managment
#
###############


@task
def get_current_timestamp(
    timestamp: datetime = None, truncate_minute: bool = True
) -&gt; datetime:
    &#34;&#34;&#34;
    Get current timestamp for flow run.
    &#34;&#34;&#34;
    if not timestamp:
        timestamp = datetime.now(tz=timezone(constants.TIMEZONE.value))
    if truncate_minute:
        return timestamp.replace(second=0, microsecond=0)
    return timestamp


@task
def create_date_hour_partition(timestamp: datetime) -&gt; str:
    &#34;&#34;&#34;
    Get date hour Hive partition structure from timestamp.
    &#34;&#34;&#34;
    return f&#34;data={timestamp.strftime(&#39;%Y-%m-%d&#39;)}/hora={timestamp.strftime(&#39;%H&#39;)}&#34;


@task
def create_date_partition(timestamp: datetime) -&gt; str:
    &#34;&#34;&#34;
    Get date hour Hive partition structure from timestamp.
    &#34;&#34;&#34;
    return f&#34;data={timestamp.date()}&#34;


@task
def parse_timestamp_to_string(timestamp: datetime, pattern=&#34;%Y-%m-%d-%H-%M-%S&#34;) -&gt; str:
    &#34;&#34;&#34;
    Parse timestamp to string pattern.
    &#34;&#34;&#34;
    return timestamp.strftime(pattern)


@task
def create_current_date_hour_partition(capture_time=None):
    &#34;&#34;&#34;Create partitioned directory structure to save data locally based
    on capture time.

    Args:
        capture_time(pendulum.datetime.DateTime, optional):
            if recapturing data, will create partitions based
            on the failed timestamps being recaptured

    Returns:
        dict: &#34;filename&#34; contains the name which to upload the csv, &#34;partitions&#34; contains
        the partitioned directory path
    &#34;&#34;&#34;
    if capture_time is None:
        capture_time = datetime.now(tz=constants.TIMEZONE.value).replace(
            minute=0, second=0, microsecond=0
        )
    date = capture_time.strftime(&#34;%Y-%m-%d&#34;)
    hour = capture_time.strftime(&#34;%H&#34;)

    return {
        &#34;filename&#34;: capture_time.strftime(&#34;%Y-%m-%d-%H-%M-%S&#34;),
        &#34;partitions&#34;: f&#34;data={date}/hora={hour}&#34;,
        &#34;timestamp&#34;: capture_time,
    }


@task
def create_local_partition_path(
    dataset_id: str, table_id: str, filename: str, partitions: str = None
) -&gt; str:
    &#34;&#34;&#34;
    Create the full path sctructure which to save data locally before
    upload.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        filename (str, optional): Single csv name
        partitions (str, optional): Partitioned directory structure, ie &#34;ano=2022/mes=03/data=01&#34;
    Returns:
        str: String path having `mode` and `filetype` to be replaced afterwards,
    either to save raw or staging files.
    &#34;&#34;&#34;
    data_folder = os.getenv(&#34;DATA_FOLDER&#34;, &#34;data&#34;)
    file_path = f&#34;{os.getcwd()}/{data_folder}/{{mode}}/{dataset_id}/{table_id}&#34;
    file_path += f&#34;/{partitions}/{filename}.{{filetype}}&#34;
    log(f&#34;Creating file path: {file_path}&#34;)
    return file_path


@task
def save_raw_local(file_path: str, status: dict, mode: str = &#34;raw&#34;) -&gt; str:
    &#34;&#34;&#34;
    Saves json response from API to .json file.
    Args:
        file_path (str): Path which to save raw file
        status (dict): Must contain keys
          * data: json returned from API
          * error: error catched from API request
        mode (str, optional): Folder to save locally, later folder which to upload to GCS.
    Returns:
        str: Path to the saved file
    &#34;&#34;&#34;
    _file_path = file_path.format(mode=mode, filetype=&#34;json&#34;)
    Path(_file_path).parent.mkdir(parents=True, exist_ok=True)
    if status[&#34;error&#34;] is None:
        json.dump(status[&#34;data&#34;], Path(_file_path).open(&#34;w&#34;, encoding=&#34;utf-8&#34;))
        log(f&#34;Raw data saved to: {_file_path}&#34;)
    return _file_path


@task
def save_treated_local(file_path: str, status: dict, mode: str = &#34;staging&#34;) -&gt; str:
    &#34;&#34;&#34;
    Save treated file to CSV.

    Args:
        file_path (str): Path which to save treated file
        status (dict): Must contain keys
          * `data`: dataframe returned from treatement
          * `error`: error catched from data treatement
        mode (str, optional): Folder to save locally, later folder which to upload to GCS.

    Returns:
        str: Path to the saved file
    &#34;&#34;&#34;
    _file_path = file_path.format(mode=mode, filetype=&#34;csv&#34;)
    Path(_file_path).parent.mkdir(parents=True, exist_ok=True)
    if status[&#34;error&#34;] is None:
        status[&#34;data&#34;].to_csv(_file_path, index=False)
        log(f&#34;Treated data saved to: {_file_path}&#34;)
    return _file_path


###############
#
# Extract data
#
###############
@task(nout=3)
def query_logs(
    dataset_id: str, table_id: str, datetime_filter=None, max_recaptures: int = 60
):
    &#34;&#34;&#34;
    Queries capture logs to check for errors

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        datetime_filter (pendulum.datetime.DateTime, optional):
        filter passed to query. This task will query the logs table
        for the last 1 day before datetime_filter

    Returns:
        list: containing timestamps for which the capture failed
    &#34;&#34;&#34;

    if not datetime_filter:
        datetime_filter = pendulum.now(constants.TIMEZONE.value).replace(
            second=0, microsecond=0
        )

    query = f&#34;&#34;&#34;
    with t as (
    select
        datetime(timestamp_array) as timestamp_array
    from
        unnest(GENERATE_TIMESTAMP_ARRAY(
            timestamp_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;, interval 1 day),
            timestamp(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;),
            interval 1 minute)
        ) as timestamp_array
    where timestamp_array &lt; &#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;
    ),
    logs as (
        select
            *,
            timestamp_trunc(timestamp_captura, minute) as timestamp_array
        from
            rj-smtr.{dataset_id}.{table_id}_logs
        where
            data between
                date(datetime_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;,
                interval 1 day))
                and date(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;)
        and
            timestamp_captura between
                datetime_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;, interval 1 day)
                and &#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;
        order by timestamp_captura
    )
    select
        case
            when logs.timestamp_captura is not null then logs.timestamp_captura
            else t.timestamp_array
        end as timestamp_captura,
        logs.erro
    from
        t
    left join
        logs
    on
        logs.timestamp_array = t.timestamp_array
    where
        logs.sucesso is not True
    order by
        timestamp_captura
    &#34;&#34;&#34;
    log(f&#34;Run query to check logs:\n{query}&#34;)
    results = bd.read_sql(query=query, billing_project_id=bq_project())
    if len(results) &gt; 0:
        results[&#34;timestamp_captura&#34;] = (
            pd.to_datetime(results[&#34;timestamp_captura&#34;])
            .dt.tz_localize(constants.TIMEZONE.value)
            .to_list()
        )
        log(f&#34;Recapture data for the following {len(results)} timestamps:\n{results}&#34;)
        if len(results) &gt; max_recaptures:
            message = f&#34;&#34;&#34;
            [SPPO - Recaptures]
            Encontradas {len(results)} timestamps para serem recapturadas.
            Essa run processará as seguintes:
            #####
            {results[:max_recaptures]}
            #####
            Sobraram as seguintes para serem recapturadas na próxima run:
            #####
            {results[max_recaptures:]}
            #####
            &#34;&#34;&#34;
            log_critical(message)
            results = results[:max_recaptures]
        return True, results[&#34;timestamp_captura&#34;].to_list(), results[&#34;erro&#34;].to_list()
    return False, [], []


@task
def get_raw(  # pylint: disable=R0912
    url: str, headers: str = None, filetype: str = &#34;json&#34;, csv_args: dict = None
) -&gt; Dict:
    &#34;&#34;&#34;
    Request data from URL API

    Args:
        url (str): URL to send request
        headers (str, optional): Path to headers guardeded on Vault, if needed.
        filetype (str, optional): Filetype to be formatted (supported only: json, csv and txt)
        csv_args (dict, optional): Arguments for read_csv, if needed
    Returns:
        dict: Conatining keys
          * `data` (json): data result
          * `error` (str): catched error, if any. Otherwise, returns None
    &#34;&#34;&#34;
    data = None
    error = None

    try:
        if headers is not None:
            headers = get_vault_secret(headers)[&#34;data&#34;]

        response = requests.get(
            url, headers=headers, timeout=constants.MAX_TIMEOUT_SECONDS.value
        )

        if response.ok:  # status code is less than 400
            if filetype == &#34;json&#34;:
                data = response.json()

                # todo: move to data check on specfic API # pylint: disable=W0102
                if isinstance(data, dict) and &#34;DescricaoErro&#34; in data.keys():
                    error = data[&#34;DescricaoErro&#34;]

            elif filetype in (&#34;txt&#34;, &#34;csv&#34;):
                if csv_args is None:
                    csv_args = {}
                data = pd.read_csv(io.StringIO(response.text), **csv_args).to_dict(
                    orient=&#34;records&#34;
                )
            else:
                error = (
                    &#34;Unsupported raw file extension. Supported only: json, csv and txt&#34;
                )

    except Exception as exp:
        error = exp

    if error is not None:
        log(f&#34;[CATCHED] Task failed with error: \n{error}&#34;, level=&#34;error&#34;)

    return {&#34;data&#34;: data, &#34;error&#34;: error}


###############
#
# Load data
#
###############


@task
def bq_upload(
    dataset_id: str,
    table_id: str,
    filepath: str,
    raw_filepath: str = None,
    partitions: str = None,
    status: dict = None,
):  # pylint: disable=R0913
    &#34;&#34;&#34;
    Upload raw and treated data to GCS and BigQuery.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        filepath (str): Path to the saved treated .csv file
        raw_filepath (str, optional): Path to raw .json file. Defaults to None.
        partitions (str, optional): Partitioned directory structure, ie &#34;ano=2022/mes=03/data=01&#34;.
        Defaults to None.
        status (dict, optional): Dict containing `error` key from
        upstream tasks.

    Returns:
        None
    &#34;&#34;&#34;
    log(
        f&#34;&#34;&#34;
    Received inputs:
    raw_filepath = {raw_filepath}, type = {type(raw_filepath)}
    treated_filepath = {filepath}, type = {type(filepath)}
    dataset_id = {dataset_id}, type = {type(dataset_id)}
    table_id = {table_id}, type = {type(table_id)}
    partitions = {partitions}, type = {type(partitions)}
    &#34;&#34;&#34;
    )
    if status[&#34;error&#34;] is not None:
        return status[&#34;error&#34;]

    error = None
    try:
        # Upload raw to staging
        if raw_filepath:
            st_obj = Storage(table_id=table_id, dataset_id=dataset_id)
            log(
                f&#34;&#34;&#34;Uploading raw file to bucket {st_obj.bucket_name} at
                {st_obj.bucket_name}/{dataset_id}/{table_id}&#34;&#34;&#34;
            )
            st_obj.upload(
                path=raw_filepath,
                partitions=partitions,
                mode=&#34;raw&#34;,
                if_exists=&#34;replace&#34;,
            )

        # Creates and publish table if it does not exist, append to it otherwise
        create_or_append_table(
            dataset_id=dataset_id,
            table_id=table_id,
            path=filepath,
            partitions=partitions,
        )
    except Exception:
        error = traceback.format_exc()
        log(f&#34;[CATCHED] Task failed with error: \n{error}&#34;, level=&#34;error&#34;)

    return error


@task
def bq_upload_from_dict(paths: dict, dataset_id: str, partition_levels: int = 1):
    &#34;&#34;&#34;Upload multiple tables from a dict structured as {table_id: csv_path}.
        Present use case assumes table partitioned once. Adjust the parameter
        &#39;partition_levels&#39; to best suit new uses.
        i.e. if your csv is saved as:
            &lt;table_id&gt;/date=&lt;run_date&gt;/&lt;filename&gt;.csv
        it has 1 level of partition.
        if your csv file is saved as:
            &lt;table_id&gt;/date=&lt;run_date&gt;/hour=&lt;run_hour&gt;/&lt;filename&gt;.csv
        it has 2 levels of partition

    Args:
        paths (dict): _description_
        dataset_id (str): _description_

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    for key in paths.keys():
        log(&#34;#&#34; * 80)
        log(f&#34;KEY = {key}&#34;)
        tb_dir = paths[key].parent
        # climb up the partition directories to reach the table dir
        for i in range(partition_levels):  # pylint: disable=unused-variable
            tb_dir = tb_dir.parent
        log(f&#34;tb_dir = {tb_dir}&#34;)
        create_or_append_table(dataset_id=dataset_id, table_id=key, path=tb_dir)

    log(f&#34;Returning -&gt; {tb_dir.parent}&#34;)

    return tb_dir.parent


@task
def upload_logs_to_bq(  # pylint: disable=R0913
    dataset_id: str,
    parent_table_id: str,
    timestamp: str,
    error: str = None,
    previous_error: str = None,
    recapture: bool = False,
):
    &#34;&#34;&#34;
    Upload execution status table to BigQuery.
    Table is uploaded to the same dataset, named {parent_table_id}_logs.
    If passing status_dict, should not pass timestamp and error.

    Args:
        dataset_id (str): dataset_id on BigQuery
        parent_table_id (str): Parent table id related to the status table
        timestamp (str): ISO formatted timestamp string
        error (str, optional): String associated with error caught during execution
    Returns:
        None
    &#34;&#34;&#34;
    table_id = parent_table_id + &#34;_logs&#34;
    # Create partition directory
    filename = f&#34;{table_id}_{timestamp.isoformat()}&#34;
    partition = f&#34;data={timestamp.date()}&#34;
    filepath = Path(
        f&#34;&#34;&#34;data/staging/{dataset_id}/{table_id}/{partition}/{filename}.csv&#34;&#34;&#34;
    )
    filepath.parent.mkdir(exist_ok=True, parents=True)
    # Create dataframe to be uploaded
    if not error and recapture is True:
        # if the recapture is succeeded, update the column erro
        dataframe = pd.DataFrame(
            {
                &#34;timestamp_captura&#34;: [timestamp],
                &#34;sucesso&#34;: [True],
                &#34;erro&#34;: [f&#34;[recapturado]{previous_error}&#34;],
            }
        )
        log(f&#34;Recapturing {timestamp} with previous error:\n{error}&#34;)
    else:
        # not recapturing or error during flow execution
        dataframe = pd.DataFrame(
            {
                &#34;timestamp_captura&#34;: [timestamp],
                &#34;sucesso&#34;: [error is None],
                &#34;erro&#34;: [error],
            }
        )
    # Save data local
    dataframe.to_csv(filepath, index=False)
    # Upload to Storage
    create_or_append_table(
        dataset_id=dataset_id,
        table_id=table_id,
        path=filepath.as_posix(),
        partitions=partition,
    )
    if error is not None:
        raise Exception(f&#34;Pipeline failed with error: {error}&#34;)


@task(
    checkpoint=False,
    max_retries=constants.MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.RETRY_DELAY.value),
)
def get_materialization_date_range(  # pylint: disable=R0913
    dataset_id: str,
    table_id: str,
    raw_dataset_id: str,
    raw_table_id: str,
    table_date_column_name: str = None,
    mode: str = &#34;prod&#34;,
    delay_hours: int = 0,
):
    &#34;&#34;&#34;
    Task for generating dict with variables to be passed to the
    --vars argument on DBT.
    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        eg: if you have a model defined in the file &lt;filename&gt;.sql,
        the table_id should be &lt;filename&gt;
        table_date_column_name (Optional, str): if it&#39;s the first time this
        is ran, will query the table for the maximum value on this field.
        If rebuild is true, will query the table for the minimum value
        on this field.
        rebuild (Optional, bool): if true, queries the minimum date value on the
        table and return a date range from that value to the datetime.now() time
        delay(Optional, int): hours delayed from now time for materialization range
    Returns:
        dict: containing date_range_start and date_range_end
    &#34;&#34;&#34;
    timestr = &#34;%Y-%m-%dT%H:%M:%S&#34;
    # get start from redis
    last_run = get_last_run_timestamp(
        dataset_id=dataset_id, table_id=table_id, mode=mode
    )
    # if there&#39;s no timestamp set on redis, get max timestamp on source table
    if last_run is None:
        if Table(dataset_id=dataset_id, table_id=table_id).table_exists(&#34;prod&#34;):
            last_run = get_table_min_max_value(
                query_project_id=bq_project(),
                dataset_id=dataset_id,
                table_id=table_id,
                field_name=table_date_column_name,
                kind=&#34;max&#34;,
            )
        else:
            last_run = get_table_min_max_value(
                query_project_id=bq_project(),
                dataset_id=raw_dataset_id,
                table_id=raw_table_id,
                field_name=table_date_column_name,
                kind=&#34;max&#34;,
            )
    else:
        last_run = datetime.strptime(last_run, timestr)

    # set start to last run hour (H)
    start_ts = last_run.replace(minute=0, second=0, microsecond=0).strftime(timestr)

    # set end to now - delay
    now_ts = pendulum.now(constants.TIMEZONE.value).replace(
        tzinfo=None, minute=0, second=0, microsecond=0
    )
    end_ts = (
        (now_ts - timedelta(hours=delay_hours))
        .replace(minute=0, second=0, microsecond=0)
        .strftime(timestr)
    )
    date_range = {&#34;date_range_start&#34;: start_ts, &#34;date_range_end&#34;: end_ts}
    log(f&#34;Got date_range as: {date_range}&#34;)
    return date_range


@task
def set_last_run_timestamp(
    dataset_id: str, table_id: str, timestamp: str, mode: str = &#34;prod&#34;, wait=None
):  # pylint: disable=unused-argument
    &#34;&#34;&#34;
    Set the `last_run_timestamp` key for the dataset_id/table_id pair
    to datetime.now() time. Used after running a materialization to set the
    stage for the next to come

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        timestamp: Last run timestamp end.
        wait (Any, optional): Used for defining dependencies inside the flow,
        in general, pass the output of the task which should be run imediately
        before this. Defaults to None.

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    redis_client = get_redis_client()
    key = dataset_id + &#34;.&#34; + table_id
    if mode == &#34;dev&#34;:
        key = f&#34;{mode}.{key}&#34;
    content = redis_client.get(key)
    content[&#34;last_run_timestamp&#34;] = timestamp
    redis_client.set(key, content)
    return True


@task
def delay_now_time(timestamp: str, delay_minutes=6):
    &#34;&#34;&#34;Return timestamp string delayed by &lt;delay_minutes&gt;

    Args:
        timestamp (str): Isoformat timestamp string
        delay_minutes (int, optional): Minutes to delay timestamp by Defaults to 6.

    Returns:
        str : timestamp string formatted as &#34;%Y-%m-%dT%H-%M-%S&#34;
    &#34;&#34;&#34;
    ts_obj = datetime.fromisoformat(timestamp)
    ts_obj = ts_obj - timedelta(minutes=delay_minutes)
    return ts_obj.strftime(&#34;%Y-%m-%dT%H-%M-%S&#34;)


@task
def fetch_dataset_sha(dataset_id: str):
    &#34;&#34;&#34;Fetches the SHA of a branch from Github&#34;&#34;&#34;
    url = &#34;https://api.github.com/repos/prefeitura-rio/queries-rj-smtr&#34;
    url += f&#34;/commits?queries-rj-smtr/rj_smtr/{dataset_id}&#34;
    response = requests.get(url)

    if response.status_code != 200:
        return None

    dataset_version = response.json()[0][&#34;sha&#34;]
    return {&#34;version&#34;: dataset_version}


@task
def get_run_dates(date_range_start: str, date_range_end: str) -&gt; List:
    &#34;&#34;&#34;
    Generates a list of dates between date_range_start and date_range_end.
    &#34;&#34;&#34;
    if (date_range_start is False) or (date_range_end is False):
        dates = [{&#34;run_date&#34;: get_now_date.run()}]
    else:
        dates = [
            {&#34;run_date&#34;: d.strftime(&#34;%Y-%m-%d&#34;)}
            for d in pd.date_range(start=date_range_start, end=date_range_end)
        ]
    log(f&#34;Will run the following dates: {dates}&#34;)
    return dates


@task
def get_join_dict(dict_list: list, new_dict: dict) -&gt; List:
    &#34;&#34;&#34;
    Updates a list of dictionaries with a new dictionary.
    &#34;&#34;&#34;
    for dict_temp in dict_list:
        dict_temp.update(new_dict)

    log(f&#34;get_join_dict: {dict_list}&#34;)
    return dict_list


@task(checkpoint=False)
def get_previous_date(days):
    &#34;&#34;&#34;
    Returns the date of {days} days ago in YYYY-MM-DD.
    &#34;&#34;&#34;
    now = pendulum.now(pendulum.timezone(&#34;America/Sao_Paulo&#34;)).subtract(days=days)

    return now.to_date_string()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.rj_smtr.tasks.bq_upload"><code class="name flex">
<span>def <span class="ident">bq_upload</span></span>(<span>dataset_id: str, table_id: str, filepath: str, raw_filepath: str = None, partitions: str = None, status: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload raw and treated data to GCS and BigQuery.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved treated .csv file</dd>
<dt><strong><code>raw_filepath</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to raw .json file. Defaults to None.</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Partitioned directory structure, ie "ano=2022/mes=03/data=01".</dd>
<dt>Defaults to None.</dt>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dict containing <code>error</code> key from</dd>
</dl>
<p>upstream tasks.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def bq_upload(
    dataset_id: str,
    table_id: str,
    filepath: str,
    raw_filepath: str = None,
    partitions: str = None,
    status: dict = None,
):  # pylint: disable=R0913
    &#34;&#34;&#34;
    Upload raw and treated data to GCS and BigQuery.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        filepath (str): Path to the saved treated .csv file
        raw_filepath (str, optional): Path to raw .json file. Defaults to None.
        partitions (str, optional): Partitioned directory structure, ie &#34;ano=2022/mes=03/data=01&#34;.
        Defaults to None.
        status (dict, optional): Dict containing `error` key from
        upstream tasks.

    Returns:
        None
    &#34;&#34;&#34;
    log(
        f&#34;&#34;&#34;
    Received inputs:
    raw_filepath = {raw_filepath}, type = {type(raw_filepath)}
    treated_filepath = {filepath}, type = {type(filepath)}
    dataset_id = {dataset_id}, type = {type(dataset_id)}
    table_id = {table_id}, type = {type(table_id)}
    partitions = {partitions}, type = {type(partitions)}
    &#34;&#34;&#34;
    )
    if status[&#34;error&#34;] is not None:
        return status[&#34;error&#34;]

    error = None
    try:
        # Upload raw to staging
        if raw_filepath:
            st_obj = Storage(table_id=table_id, dataset_id=dataset_id)
            log(
                f&#34;&#34;&#34;Uploading raw file to bucket {st_obj.bucket_name} at
                {st_obj.bucket_name}/{dataset_id}/{table_id}&#34;&#34;&#34;
            )
            st_obj.upload(
                path=raw_filepath,
                partitions=partitions,
                mode=&#34;raw&#34;,
                if_exists=&#34;replace&#34;,
            )

        # Creates and publish table if it does not exist, append to it otherwise
        create_or_append_table(
            dataset_id=dataset_id,
            table_id=table_id,
            path=filepath,
            partitions=partitions,
        )
    except Exception:
        error = traceback.format_exc()
        log(f&#34;[CATCHED] Task failed with error: \n{error}&#34;, level=&#34;error&#34;)

    return error</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.bq_upload_from_dict"><code class="name flex">
<span>def <span class="ident">bq_upload_from_dict</span></span>(<span>paths: dict, dataset_id: str, partition_levels: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload multiple tables from a dict structured as {table_id: csv_path}.
Present use case assumes table partitioned once. Adjust the parameter
'partition_levels' to best suit new uses.
i.e. if your csv is saved as:
<table_id>/date=<run_date>/<filename>.csv
it has 1 level of partition.
if your csv file is saved as:
<table_id>/date=<run_date>/hour=<run_hour>/<filename>.csv
it has 2 levels of partition</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>paths</code></strong> :&ensp;<code>dict</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def bq_upload_from_dict(paths: dict, dataset_id: str, partition_levels: int = 1):
    &#34;&#34;&#34;Upload multiple tables from a dict structured as {table_id: csv_path}.
        Present use case assumes table partitioned once. Adjust the parameter
        &#39;partition_levels&#39; to best suit new uses.
        i.e. if your csv is saved as:
            &lt;table_id&gt;/date=&lt;run_date&gt;/&lt;filename&gt;.csv
        it has 1 level of partition.
        if your csv file is saved as:
            &lt;table_id&gt;/date=&lt;run_date&gt;/hour=&lt;run_hour&gt;/&lt;filename&gt;.csv
        it has 2 levels of partition

    Args:
        paths (dict): _description_
        dataset_id (str): _description_

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    for key in paths.keys():
        log(&#34;#&#34; * 80)
        log(f&#34;KEY = {key}&#34;)
        tb_dir = paths[key].parent
        # climb up the partition directories to reach the table dir
        for i in range(partition_levels):  # pylint: disable=unused-variable
            tb_dir = tb_dir.parent
        log(f&#34;tb_dir = {tb_dir}&#34;)
        create_or_append_table(dataset_id=dataset_id, table_id=key, path=tb_dir)

    log(f&#34;Returning -&gt; {tb_dir.parent}&#34;)

    return tb_dir.parent</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.build_incremental_model"><code class="name flex">
<span>def <span class="ident">build_incremental_model</span></span>(<span>dbt_client: dbt_client.dbt_client.DbtClient, dataset_id: str, base_table_id: str, mat_table_id: str, field_name: str = 'data_versao', refresh: bool = False, wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility task for backfilling table in predetermined steps.
Assumes the step sizes will be defined on the .sql file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dbt_client</code></strong> :&ensp;<code>DbtClient</code></dt>
<dd>DBT interface object</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Dataset id on BigQuery</dd>
<dt><strong><code>base_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Base table from which to materialize (usually, an external table)</dd>
<dt><strong><code>mat_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Target table id for materialization</dd>
<dt><strong><code>field_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Key field (column) for dbt incremental filters.</dd>
<dt>Defaults to "data_versao".</dt>
<dt><strong><code>refresh</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, rebuild the table from scratch. Defaults to False.</dd>
<dt><strong><code>wait</code></strong> :&ensp;<code>NoneType</code>, optional</dt>
<dd>Placeholder parameter, used to wait previous tasks finish.</dd>
</dl>
<p>Defaults to None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>whether the table was fully built or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(max_retries=3, retry_delay=timedelta(seconds=10))
def build_incremental_model(  # pylint: disable=too-many-arguments
    dbt_client: DbtClient,
    dataset_id: str,
    base_table_id: str,
    mat_table_id: str,
    field_name: str = &#34;data_versao&#34;,
    refresh: bool = False,
    wait=None,  # pylint: disable=unused-argument
):
    &#34;&#34;&#34;
        Utility task for backfilling table in predetermined steps.
        Assumes the step sizes will be defined on the .sql file.

    Args:
        dbt_client (DbtClient): DBT interface object
        dataset_id (str): Dataset id on BigQuery
        base_table_id (str): Base table from which to materialize (usually, an external table)
        mat_table_id (str): Target table id for materialization
        field_name (str, optional): Key field (column) for dbt incremental filters.
        Defaults to &#34;data_versao&#34;.
        refresh (bool, optional): If True, rebuild the table from scratch. Defaults to False.
        wait (NoneType, optional): Placeholder parameter, used to wait previous tasks finish.
        Defaults to None.

    Returns:
        bool: whether the table was fully built or not.
    &#34;&#34;&#34;

    query_project_id = bq_project()
    last_mat_date = get_table_min_max_value(
        query_project_id, dataset_id, mat_table_id, field_name, &#34;max&#34;
    )
    last_base_date = get_table_min_max_value(
        query_project_id, dataset_id, base_table_id, field_name, &#34;max&#34;
    )
    log(
        f&#34;&#34;&#34;
    Base table last version: {last_base_date}
    Materialized table last version: {last_mat_date}
    &#34;&#34;&#34;
    )
    run_command = f&#34;run --select models/{dataset_id}/{mat_table_id}.sql&#34;

    if refresh:
        log(&#34;Running in full refresh mode&#34;)
        log(f&#34;DBT will run the following command:\n{run_command+&#39; --full-refresh&#39;}&#34;)
        dbt_client.cli(run_command + &#34; --full-refresh&#34;, sync=True)
        last_mat_date = get_table_min_max_value(
            query_project_id, dataset_id, mat_table_id, field_name, &#34;max&#34;
        )

    if last_base_date &gt; last_mat_date:
        log(&#34;Running interval step materialization&#34;)
        log(f&#34;DBT will run the following command:\n{run_command}&#34;)
        while last_base_date &gt; last_mat_date:
            running = dbt_client.cli(run_command, sync=True)
            last_mat_date = get_table_min_max_value(
                query_project_id,
                dataset_id,
                mat_table_id,
                field_name,
                &#34;max&#34;,
                wait=running,
            )
            log(f&#34;After this step, materialized table last version is: {last_mat_date}&#34;)
            if last_mat_date == last_base_date:
                log(&#34;Materialized table reached base table version!&#34;)
                return True
    log(&#34;Did not run interval step materialization...&#34;)
    return False</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_current_date_hour_partition"><code class="name flex">
<span>def <span class="ident">create_current_date_hour_partition</span></span>(<span>capture_time=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create partitioned directory structure to save data locally based
on capture time.</p>
<h2 id="args">Args</h2>
<p>capture_time(pendulum.datetime.DateTime, optional):
if recapturing data, will create partitions based
on the failed timestamps being recaptured</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>"filename" contains the name which to upload the csv, "partitions" contains</dd>
</dl>
<p>the partitioned directory path</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def create_current_date_hour_partition(capture_time=None):
    &#34;&#34;&#34;Create partitioned directory structure to save data locally based
    on capture time.

    Args:
        capture_time(pendulum.datetime.DateTime, optional):
            if recapturing data, will create partitions based
            on the failed timestamps being recaptured

    Returns:
        dict: &#34;filename&#34; contains the name which to upload the csv, &#34;partitions&#34; contains
        the partitioned directory path
    &#34;&#34;&#34;
    if capture_time is None:
        capture_time = datetime.now(tz=constants.TIMEZONE.value).replace(
            minute=0, second=0, microsecond=0
        )
    date = capture_time.strftime(&#34;%Y-%m-%d&#34;)
    hour = capture_time.strftime(&#34;%H&#34;)

    return {
        &#34;filename&#34;: capture_time.strftime(&#34;%Y-%m-%d-%H-%M-%S&#34;),
        &#34;partitions&#34;: f&#34;data={date}/hora={hour}&#34;,
        &#34;timestamp&#34;: capture_time,
    }</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_date_hour_partition"><code class="name flex">
<span>def <span class="ident">create_date_hour_partition</span></span>(<span>timestamp: datetime.datetime) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Get date hour Hive partition structure from timestamp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def create_date_hour_partition(timestamp: datetime) -&gt; str:
    &#34;&#34;&#34;
    Get date hour Hive partition structure from timestamp.
    &#34;&#34;&#34;
    return f&#34;data={timestamp.strftime(&#39;%Y-%m-%d&#39;)}/hora={timestamp.strftime(&#39;%H&#39;)}&#34;</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_date_partition"><code class="name flex">
<span>def <span class="ident">create_date_partition</span></span>(<span>timestamp: datetime.datetime) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Get date hour Hive partition structure from timestamp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def create_date_partition(timestamp: datetime) -&gt; str:
    &#34;&#34;&#34;
    Get date hour Hive partition structure from timestamp.
    &#34;&#34;&#34;
    return f&#34;data={timestamp.date()}&#34;</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_local_partition_path"><code class="name flex">
<span>def <span class="ident">create_local_partition_path</span></span>(<span>dataset_id: str, table_id: str, filename: str, partitions: str = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Create the full path sctructure which to save data locally before
upload.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Single csv name</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Partitioned directory structure, ie "ano=2022/mes=03/data=01"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>String path having <code>mode</code> and <code>filetype</code> to be replaced afterwards,</dd>
</dl>
<p>either to save raw or staging files.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def create_local_partition_path(
    dataset_id: str, table_id: str, filename: str, partitions: str = None
) -&gt; str:
    &#34;&#34;&#34;
    Create the full path sctructure which to save data locally before
    upload.

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        filename (str, optional): Single csv name
        partitions (str, optional): Partitioned directory structure, ie &#34;ano=2022/mes=03/data=01&#34;
    Returns:
        str: String path having `mode` and `filetype` to be replaced afterwards,
    either to save raw or staging files.
    &#34;&#34;&#34;
    data_folder = os.getenv(&#34;DATA_FOLDER&#34;, &#34;data&#34;)
    file_path = f&#34;{os.getcwd()}/{data_folder}/{{mode}}/{dataset_id}/{table_id}&#34;
    file_path += f&#34;/{partitions}/{filename}.{{filetype}}&#34;
    log(f&#34;Creating file path: {file_path}&#34;)
    return file_path</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.delay_now_time"><code class="name flex">
<span>def <span class="ident">delay_now_time</span></span>(<span>timestamp: str, delay_minutes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>Return timestamp string delayed by <delay_minutes></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>str</code></dt>
<dd>Isoformat timestamp string</dd>
<dt><strong><code>delay_minutes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Minutes to delay timestamp by Defaults to 6.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str </code></dt>
<dd>timestamp string formatted as "%Y-%m-%dT%H-%M-%S"</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def delay_now_time(timestamp: str, delay_minutes=6):
    &#34;&#34;&#34;Return timestamp string delayed by &lt;delay_minutes&gt;

    Args:
        timestamp (str): Isoformat timestamp string
        delay_minutes (int, optional): Minutes to delay timestamp by Defaults to 6.

    Returns:
        str : timestamp string formatted as &#34;%Y-%m-%dT%H-%M-%S&#34;
    &#34;&#34;&#34;
    ts_obj = datetime.fromisoformat(timestamp)
    ts_obj = ts_obj - timedelta(minutes=delay_minutes)
    return ts_obj.strftime(&#34;%Y-%m-%dT%H-%M-%S&#34;)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.fetch_dataset_sha"><code class="name flex">
<span>def <span class="ident">fetch_dataset_sha</span></span>(<span>dataset_id: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches the SHA of a branch from Github</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def fetch_dataset_sha(dataset_id: str):
    &#34;&#34;&#34;Fetches the SHA of a branch from Github&#34;&#34;&#34;
    url = &#34;https://api.github.com/repos/prefeitura-rio/queries-rj-smtr&#34;
    url += f&#34;/commits?queries-rj-smtr/rj_smtr/{dataset_id}&#34;
    response = requests.get(url)

    if response.status_code != 200:
        return None

    dataset_version = response.json()[0][&#34;sha&#34;]
    return {&#34;version&#34;: dataset_version}</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_current_timestamp"><code class="name flex">
<span>def <span class="ident">get_current_timestamp</span></span>(<span>timestamp: datetime.datetime = None, truncate_minute: bool = True) ‑> datetime.datetime</span>
</code></dt>
<dd>
<div class="desc"><p>Get current timestamp for flow run.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_current_timestamp(
    timestamp: datetime = None, truncate_minute: bool = True
) -&gt; datetime:
    &#34;&#34;&#34;
    Get current timestamp for flow run.
    &#34;&#34;&#34;
    if not timestamp:
        timestamp = datetime.now(tz=timezone(constants.TIMEZONE.value))
    if truncate_minute:
        return timestamp.replace(second=0, microsecond=0)
    return timestamp</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_join_dict"><code class="name flex">
<span>def <span class="ident">get_join_dict</span></span>(<span>dict_list: list, new_dict: dict) ‑> List</span>
</code></dt>
<dd>
<div class="desc"><p>Updates a list of dictionaries with a new dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_join_dict(dict_list: list, new_dict: dict) -&gt; List:
    &#34;&#34;&#34;
    Updates a list of dictionaries with a new dictionary.
    &#34;&#34;&#34;
    for dict_temp in dict_list:
        dict_temp.update(new_dict)

    log(f&#34;get_join_dict: {dict_list}&#34;)
    return dict_list</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_local_dbt_client"><code class="name flex">
<span>def <span class="ident">get_local_dbt_client</span></span>(<span>host: str, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Set a DBT client for running CLI commands. Requires
building container image for your queries repository.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>host</code></strong> :&ensp;<code>str</code></dt>
<dd>hostname. When running locally, usually 'localhost'</dd>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>the port number in which the DBT rpc is running</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DbtClient</code></dt>
<dd>object used to run DBT commands.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_local_dbt_client(host: str, port: int):
    &#34;&#34;&#34;Set a DBT client for running CLI commands. Requires
    building container image for your queries repository.

    Args:
        host (str): hostname. When running locally, usually &#39;localhost&#39;
        port (int): the port number in which the DBT rpc is running

    Returns:
        DbtClient: object used to run DBT commands.
    &#34;&#34;&#34;
    return get_dbt_client(host=host, port=port)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_materialization_date_range"><code class="name flex">
<span>def <span class="ident">get_materialization_date_range</span></span>(<span>dataset_id: str, table_id: str, raw_dataset_id: str, raw_table_id: str, table_date_column_name: str = None, mode: str = 'prod', delay_hours: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Task for generating dict with variables to be passed to the
&ndash;vars argument on DBT.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model filename on the queries repo.</dd>
<dt><strong><code>eg</code></strong></dt>
<dd>if you have a model defined in the file <filename>.sql,</dd>
<dt>the table_id should be <filename></dt>
<dt><strong><code>table_date_column_name</code></strong> :&ensp;<code>Optional, str</code></dt>
<dd>if it's the first time this</dd>
<dt>is ran, will query the table for the maximum value on this field.</dt>
<dt>If rebuild is true, will query the table for the minimum value</dt>
<dt>on this field.</dt>
<dt><strong><code>rebuild</code></strong> :&ensp;<code>Optional, bool</code></dt>
<dd>if true, queries the minimum date value on the</dd>
</dl>
<p>table and return a date range from that value to the datetime.now() time
delay(Optional, int): hours delayed from now time for materialization range</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>containing date_range_start and date_range_end</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(
    checkpoint=False,
    max_retries=constants.MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.RETRY_DELAY.value),
)
def get_materialization_date_range(  # pylint: disable=R0913
    dataset_id: str,
    table_id: str,
    raw_dataset_id: str,
    raw_table_id: str,
    table_date_column_name: str = None,
    mode: str = &#34;prod&#34;,
    delay_hours: int = 0,
):
    &#34;&#34;&#34;
    Task for generating dict with variables to be passed to the
    --vars argument on DBT.
    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        eg: if you have a model defined in the file &lt;filename&gt;.sql,
        the table_id should be &lt;filename&gt;
        table_date_column_name (Optional, str): if it&#39;s the first time this
        is ran, will query the table for the maximum value on this field.
        If rebuild is true, will query the table for the minimum value
        on this field.
        rebuild (Optional, bool): if true, queries the minimum date value on the
        table and return a date range from that value to the datetime.now() time
        delay(Optional, int): hours delayed from now time for materialization range
    Returns:
        dict: containing date_range_start and date_range_end
    &#34;&#34;&#34;
    timestr = &#34;%Y-%m-%dT%H:%M:%S&#34;
    # get start from redis
    last_run = get_last_run_timestamp(
        dataset_id=dataset_id, table_id=table_id, mode=mode
    )
    # if there&#39;s no timestamp set on redis, get max timestamp on source table
    if last_run is None:
        if Table(dataset_id=dataset_id, table_id=table_id).table_exists(&#34;prod&#34;):
            last_run = get_table_min_max_value(
                query_project_id=bq_project(),
                dataset_id=dataset_id,
                table_id=table_id,
                field_name=table_date_column_name,
                kind=&#34;max&#34;,
            )
        else:
            last_run = get_table_min_max_value(
                query_project_id=bq_project(),
                dataset_id=raw_dataset_id,
                table_id=raw_table_id,
                field_name=table_date_column_name,
                kind=&#34;max&#34;,
            )
    else:
        last_run = datetime.strptime(last_run, timestr)

    # set start to last run hour (H)
    start_ts = last_run.replace(minute=0, second=0, microsecond=0).strftime(timestr)

    # set end to now - delay
    now_ts = pendulum.now(constants.TIMEZONE.value).replace(
        tzinfo=None, minute=0, second=0, microsecond=0
    )
    end_ts = (
        (now_ts - timedelta(hours=delay_hours))
        .replace(minute=0, second=0, microsecond=0)
        .strftime(timestr)
    )
    date_range = {&#34;date_range_start&#34;: start_ts, &#34;date_range_end&#34;: end_ts}
    log(f&#34;Got date_range as: {date_range}&#34;)
    return date_range</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_previous_date"><code class="name flex">
<span>def <span class="ident">get_previous_date</span></span>(<span>days)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the date of {days} days ago in YYYY-MM-DD.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(checkpoint=False)
def get_previous_date(days):
    &#34;&#34;&#34;
    Returns the date of {days} days ago in YYYY-MM-DD.
    &#34;&#34;&#34;
    now = pendulum.now(pendulum.timezone(&#34;America/Sao_Paulo&#34;)).subtract(days=days)

    return now.to_date_string()</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_raw"><code class="name flex">
<span>def <span class="ident">get_raw</span></span>(<span>url: str, headers: str = None, filetype: str = 'json', csv_args: dict = None) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Request data from URL API</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code></dt>
<dd>URL to send request</dd>
<dt><strong><code>headers</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to headers guardeded on Vault, if needed.</dd>
<dt><strong><code>filetype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Filetype to be formatted (supported only: json, csv and txt)</dd>
<dt><strong><code>csv_args</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Arguments for read_csv, if needed</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Conatining keys
* <code>data</code> (json): data result
* <code>error</code> (str): catched error, if any. Otherwise, returns None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_raw(  # pylint: disable=R0912
    url: str, headers: str = None, filetype: str = &#34;json&#34;, csv_args: dict = None
) -&gt; Dict:
    &#34;&#34;&#34;
    Request data from URL API

    Args:
        url (str): URL to send request
        headers (str, optional): Path to headers guardeded on Vault, if needed.
        filetype (str, optional): Filetype to be formatted (supported only: json, csv and txt)
        csv_args (dict, optional): Arguments for read_csv, if needed
    Returns:
        dict: Conatining keys
          * `data` (json): data result
          * `error` (str): catched error, if any. Otherwise, returns None
    &#34;&#34;&#34;
    data = None
    error = None

    try:
        if headers is not None:
            headers = get_vault_secret(headers)[&#34;data&#34;]

        response = requests.get(
            url, headers=headers, timeout=constants.MAX_TIMEOUT_SECONDS.value
        )

        if response.ok:  # status code is less than 400
            if filetype == &#34;json&#34;:
                data = response.json()

                # todo: move to data check on specfic API # pylint: disable=W0102
                if isinstance(data, dict) and &#34;DescricaoErro&#34; in data.keys():
                    error = data[&#34;DescricaoErro&#34;]

            elif filetype in (&#34;txt&#34;, &#34;csv&#34;):
                if csv_args is None:
                    csv_args = {}
                data = pd.read_csv(io.StringIO(response.text), **csv_args).to_dict(
                    orient=&#34;records&#34;
                )
            else:
                error = (
                    &#34;Unsupported raw file extension. Supported only: json, csv and txt&#34;
                )

    except Exception as exp:
        error = exp

    if error is not None:
        log(f&#34;[CATCHED] Task failed with error: \n{error}&#34;, level=&#34;error&#34;)

    return {&#34;data&#34;: data, &#34;error&#34;: error}</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_run_dates"><code class="name flex">
<span>def <span class="ident">get_run_dates</span></span>(<span>date_range_start: str, date_range_end: str) ‑> List</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a list of dates between date_range_start and date_range_end.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def get_run_dates(date_range_start: str, date_range_end: str) -&gt; List:
    &#34;&#34;&#34;
    Generates a list of dates between date_range_start and date_range_end.
    &#34;&#34;&#34;
    if (date_range_start is False) or (date_range_end is False):
        dates = [{&#34;run_date&#34;: get_now_date.run()}]
    else:
        dates = [
            {&#34;run_date&#34;: d.strftime(&#34;%Y-%m-%d&#34;)}
            for d in pd.date_range(start=date_range_start, end=date_range_end)
        ]
    log(f&#34;Will run the following dates: {dates}&#34;)
    return dates</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.parse_timestamp_to_string"><code class="name flex">
<span>def <span class="ident">parse_timestamp_to_string</span></span>(<span>timestamp: datetime.datetime, pattern='%Y-%m-%d-%H-%M-%S') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Parse timestamp to string pattern.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def parse_timestamp_to_string(timestamp: datetime, pattern=&#34;%Y-%m-%d-%H-%M-%S&#34;) -&gt; str:
    &#34;&#34;&#34;
    Parse timestamp to string pattern.
    &#34;&#34;&#34;
    return timestamp.strftime(pattern)</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.query_logs"><code class="name flex">
<span>def <span class="ident">query_logs</span></span>(<span>dataset_id: str, table_id: str, datetime_filter=None, max_recaptures: int = 60)</span>
</code></dt>
<dd>
<div class="desc"><p>Queries capture logs to check for errors</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
</dl>
<p>datetime_filter (pendulum.datetime.DateTime, optional):
filter passed to query. This task will query the logs table
for the last 1 day before datetime_filter</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>containing timestamps for which the capture failed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(nout=3)
def query_logs(
    dataset_id: str, table_id: str, datetime_filter=None, max_recaptures: int = 60
):
    &#34;&#34;&#34;
    Queries capture logs to check for errors

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): table_id on BigQuery
        datetime_filter (pendulum.datetime.DateTime, optional):
        filter passed to query. This task will query the logs table
        for the last 1 day before datetime_filter

    Returns:
        list: containing timestamps for which the capture failed
    &#34;&#34;&#34;

    if not datetime_filter:
        datetime_filter = pendulum.now(constants.TIMEZONE.value).replace(
            second=0, microsecond=0
        )

    query = f&#34;&#34;&#34;
    with t as (
    select
        datetime(timestamp_array) as timestamp_array
    from
        unnest(GENERATE_TIMESTAMP_ARRAY(
            timestamp_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;, interval 1 day),
            timestamp(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;),
            interval 1 minute)
        ) as timestamp_array
    where timestamp_array &lt; &#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;
    ),
    logs as (
        select
            *,
            timestamp_trunc(timestamp_captura, minute) as timestamp_array
        from
            rj-smtr.{dataset_id}.{table_id}_logs
        where
            data between
                date(datetime_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;,
                interval 1 day))
                and date(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;)
        and
            timestamp_captura between
                datetime_sub(&#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;, interval 1 day)
                and &#39;{datetime_filter.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)}&#39;
        order by timestamp_captura
    )
    select
        case
            when logs.timestamp_captura is not null then logs.timestamp_captura
            else t.timestamp_array
        end as timestamp_captura,
        logs.erro
    from
        t
    left join
        logs
    on
        logs.timestamp_array = t.timestamp_array
    where
        logs.sucesso is not True
    order by
        timestamp_captura
    &#34;&#34;&#34;
    log(f&#34;Run query to check logs:\n{query}&#34;)
    results = bd.read_sql(query=query, billing_project_id=bq_project())
    if len(results) &gt; 0:
        results[&#34;timestamp_captura&#34;] = (
            pd.to_datetime(results[&#34;timestamp_captura&#34;])
            .dt.tz_localize(constants.TIMEZONE.value)
            .to_list()
        )
        log(f&#34;Recapture data for the following {len(results)} timestamps:\n{results}&#34;)
        if len(results) &gt; max_recaptures:
            message = f&#34;&#34;&#34;
            [SPPO - Recaptures]
            Encontradas {len(results)} timestamps para serem recapturadas.
            Essa run processará as seguintes:
            #####
            {results[:max_recaptures]}
            #####
            Sobraram as seguintes para serem recapturadas na próxima run:
            #####
            {results[max_recaptures:]}
            #####
            &#34;&#34;&#34;
            log_critical(message)
            results = results[:max_recaptures]
        return True, results[&#34;timestamp_captura&#34;].to_list(), results[&#34;erro&#34;].to_list()
    return False, [], []</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.save_raw_local"><code class="name flex">
<span>def <span class="ident">save_raw_local</span></span>(<span>file_path: str, status: dict, mode: str = 'raw') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Saves json response from API to .json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path which to save raw file</dd>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code></dt>
<dd>Must contain keys
* data: json returned from API
* error: error catched from API request</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Folder to save locally, later folder which to upload to GCS.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Path to the saved file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def save_raw_local(file_path: str, status: dict, mode: str = &#34;raw&#34;) -&gt; str:
    &#34;&#34;&#34;
    Saves json response from API to .json file.
    Args:
        file_path (str): Path which to save raw file
        status (dict): Must contain keys
          * data: json returned from API
          * error: error catched from API request
        mode (str, optional): Folder to save locally, later folder which to upload to GCS.
    Returns:
        str: Path to the saved file
    &#34;&#34;&#34;
    _file_path = file_path.format(mode=mode, filetype=&#34;json&#34;)
    Path(_file_path).parent.mkdir(parents=True, exist_ok=True)
    if status[&#34;error&#34;] is None:
        json.dump(status[&#34;data&#34;], Path(_file_path).open(&#34;w&#34;, encoding=&#34;utf-8&#34;))
        log(f&#34;Raw data saved to: {_file_path}&#34;)
    return _file_path</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.save_treated_local"><code class="name flex">
<span>def <span class="ident">save_treated_local</span></span>(<span>file_path: str, status: dict, mode: str = 'staging') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Save treated file to CSV.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path which to save treated file</dd>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code></dt>
<dd>Must contain keys
* <code>data</code>: dataframe returned from treatement
* <code>error</code>: error catched from data treatement</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Folder to save locally, later folder which to upload to GCS.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Path to the saved file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def save_treated_local(file_path: str, status: dict, mode: str = &#34;staging&#34;) -&gt; str:
    &#34;&#34;&#34;
    Save treated file to CSV.

    Args:
        file_path (str): Path which to save treated file
        status (dict): Must contain keys
          * `data`: dataframe returned from treatement
          * `error`: error catched from data treatement
        mode (str, optional): Folder to save locally, later folder which to upload to GCS.

    Returns:
        str: Path to the saved file
    &#34;&#34;&#34;
    _file_path = file_path.format(mode=mode, filetype=&#34;csv&#34;)
    Path(_file_path).parent.mkdir(parents=True, exist_ok=True)
    if status[&#34;error&#34;] is None:
        status[&#34;data&#34;].to_csv(_file_path, index=False)
        log(f&#34;Treated data saved to: {_file_path}&#34;)
    return _file_path</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.set_last_run_timestamp"><code class="name flex">
<span>def <span class="ident">set_last_run_timestamp</span></span>(<span>dataset_id: str, table_id: str, timestamp: str, mode: str = 'prod', wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the <code>last_run_timestamp</code> key for the dataset_id/table_id pair
to datetime.now() time. Used after running a materialization to set the
stage for the next to come</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model filename on the queries repo.</dd>
<dt><strong><code>timestamp</code></strong></dt>
<dd>Last run timestamp end.</dd>
<dt><strong><code>wait</code></strong> :&ensp;<code>Any</code>, optional</dt>
<dd>Used for defining dependencies inside the flow,</dd>
</dl>
<p>in general, pass the output of the task which should be run imediately
before this. Defaults to None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def set_last_run_timestamp(
    dataset_id: str, table_id: str, timestamp: str, mode: str = &#34;prod&#34;, wait=None
):  # pylint: disable=unused-argument
    &#34;&#34;&#34;
    Set the `last_run_timestamp` key for the dataset_id/table_id pair
    to datetime.now() time. Used after running a materialization to set the
    stage for the next to come

    Args:
        dataset_id (str): dataset_id on BigQuery
        table_id (str): model filename on the queries repo.
        timestamp: Last run timestamp end.
        wait (Any, optional): Used for defining dependencies inside the flow,
        in general, pass the output of the task which should be run imediately
        before this. Defaults to None.

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    redis_client = get_redis_client()
    key = dataset_id + &#34;.&#34; + table_id
    if mode == &#34;dev&#34;:
        key = f&#34;{mode}.{key}&#34;
    content = redis_client.get(key)
    content[&#34;last_run_timestamp&#34;] = timestamp
    redis_client.set(key, content)
    return True</code></pre>
</details>
</dd>
<dt id="pipelines.rj_smtr.tasks.upload_logs_to_bq"><code class="name flex">
<span>def <span class="ident">upload_logs_to_bq</span></span>(<span>dataset_id: str, parent_table_id: str, timestamp: str, error: str = None, previous_error: str = None, recapture: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload execution status table to BigQuery.
Table is uploaded to the same dataset, named {parent_table_id}_logs.
If passing status_dict, should not pass timestamp and error.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>parent_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Parent table id related to the status table</dd>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>str</code></dt>
<dd>ISO formatted timestamp string</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String associated with error caught during execution</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task
def upload_logs_to_bq(  # pylint: disable=R0913
    dataset_id: str,
    parent_table_id: str,
    timestamp: str,
    error: str = None,
    previous_error: str = None,
    recapture: bool = False,
):
    &#34;&#34;&#34;
    Upload execution status table to BigQuery.
    Table is uploaded to the same dataset, named {parent_table_id}_logs.
    If passing status_dict, should not pass timestamp and error.

    Args:
        dataset_id (str): dataset_id on BigQuery
        parent_table_id (str): Parent table id related to the status table
        timestamp (str): ISO formatted timestamp string
        error (str, optional): String associated with error caught during execution
    Returns:
        None
    &#34;&#34;&#34;
    table_id = parent_table_id + &#34;_logs&#34;
    # Create partition directory
    filename = f&#34;{table_id}_{timestamp.isoformat()}&#34;
    partition = f&#34;data={timestamp.date()}&#34;
    filepath = Path(
        f&#34;&#34;&#34;data/staging/{dataset_id}/{table_id}/{partition}/{filename}.csv&#34;&#34;&#34;
    )
    filepath.parent.mkdir(exist_ok=True, parents=True)
    # Create dataframe to be uploaded
    if not error and recapture is True:
        # if the recapture is succeeded, update the column erro
        dataframe = pd.DataFrame(
            {
                &#34;timestamp_captura&#34;: [timestamp],
                &#34;sucesso&#34;: [True],
                &#34;erro&#34;: [f&#34;[recapturado]{previous_error}&#34;],
            }
        )
        log(f&#34;Recapturing {timestamp} with previous error:\n{error}&#34;)
    else:
        # not recapturing or error during flow execution
        dataframe = pd.DataFrame(
            {
                &#34;timestamp_captura&#34;: [timestamp],
                &#34;sucesso&#34;: [error is None],
                &#34;erro&#34;: [error],
            }
        )
    # Save data local
    dataframe.to_csv(filepath, index=False)
    # Upload to Storage
    create_or_append_table(
        dataset_id=dataset_id,
        table_id=table_id,
        path=filepath.as_posix(),
        partitions=partition,
    )
    if error is not None:
        raise Exception(f&#34;Pipeline failed with error: {error}&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.rj_smtr" href="index.html">pipelines.rj_smtr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.rj_smtr.tasks.bq_upload" href="#pipelines.rj_smtr.tasks.bq_upload">bq_upload</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.bq_upload_from_dict" href="#pipelines.rj_smtr.tasks.bq_upload_from_dict">bq_upload_from_dict</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.build_incremental_model" href="#pipelines.rj_smtr.tasks.build_incremental_model">build_incremental_model</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_current_date_hour_partition" href="#pipelines.rj_smtr.tasks.create_current_date_hour_partition">create_current_date_hour_partition</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_date_hour_partition" href="#pipelines.rj_smtr.tasks.create_date_hour_partition">create_date_hour_partition</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_date_partition" href="#pipelines.rj_smtr.tasks.create_date_partition">create_date_partition</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_local_partition_path" href="#pipelines.rj_smtr.tasks.create_local_partition_path">create_local_partition_path</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.delay_now_time" href="#pipelines.rj_smtr.tasks.delay_now_time">delay_now_time</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.fetch_dataset_sha" href="#pipelines.rj_smtr.tasks.fetch_dataset_sha">fetch_dataset_sha</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_current_timestamp" href="#pipelines.rj_smtr.tasks.get_current_timestamp">get_current_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_join_dict" href="#pipelines.rj_smtr.tasks.get_join_dict">get_join_dict</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_local_dbt_client" href="#pipelines.rj_smtr.tasks.get_local_dbt_client">get_local_dbt_client</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_materialization_date_range" href="#pipelines.rj_smtr.tasks.get_materialization_date_range">get_materialization_date_range</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_previous_date" href="#pipelines.rj_smtr.tasks.get_previous_date">get_previous_date</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_raw" href="#pipelines.rj_smtr.tasks.get_raw">get_raw</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_run_dates" href="#pipelines.rj_smtr.tasks.get_run_dates">get_run_dates</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.parse_timestamp_to_string" href="#pipelines.rj_smtr.tasks.parse_timestamp_to_string">parse_timestamp_to_string</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.query_logs" href="#pipelines.rj_smtr.tasks.query_logs">query_logs</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.save_raw_local" href="#pipelines.rj_smtr.tasks.save_raw_local">save_raw_local</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.save_treated_local" href="#pipelines.rj_smtr.tasks.save_treated_local">save_treated_local</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.set_last_run_timestamp" href="#pipelines.rj_smtr.tasks.set_last_run_timestamp">set_last_run_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.upload_logs_to_bq" href="#pipelines.rj_smtr.tasks.upload_logs_to_bq">upload_logs_to_bq</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>