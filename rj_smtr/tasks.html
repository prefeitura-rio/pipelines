<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>pipelines.rj_smtr.tasks API documentation</title>
<meta name="description" content="Tasks for rj_smtr">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelines.rj_smtr.tasks</code></h1>
</header>
<section id="section-intro">
<p>Tasks for rj_smtr</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipelines.rj_smtr.tasks.bq_upload"><code class="name flex">
<span>def <span class="ident">bq_upload</span></span>(<span>dataset_id:Â str, table_id:Â str, filepath:Â str, raw_filepath:Â strÂ =Â None, partitions:Â strÂ =Â None, status:Â dictÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload raw and treated data to GCS and BigQuery.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved treated .csv file</dd>
<dt><strong><code>raw_filepath</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to raw .json file. Defaults to None.</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Partitioned directory structure, ie "ano=2022/mes=03/data=01".</dd>
<dt>Defaults to None.</dt>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dict containing <code>error</code> key from</dd>
</dl>
<p>upstream tasks.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.bq_upload_from_dict"><code class="name flex">
<span>def <span class="ident">bq_upload_from_dict</span></span>(<span>paths:Â dict, dataset_id:Â str, partition_levels:Â intÂ =Â 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload multiple tables from a dict structured as {table_id: csv_path}.
Present use case assumes table partitioned once. Adjust the parameter
'partition_levels' to best suit new uses.
i.e. if your csv is saved as:
<table_id>/date=<run_date>/<filename>.csv
it has 1 level of partition.
if your csv file is saved as:
<table_id>/date=<run_date>/hour=<run_hour>/<filename>.csv
it has 2 levels of partition</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>paths</code></strong> :&ensp;<code>dict</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd><em>description</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.build_incremental_model"><code class="name flex">
<span>def <span class="ident">build_incremental_model</span></span>(<span>dbt_client:Â dbt_client.dbt_client.DbtClient, dataset_id:Â str, base_table_id:Â str, mat_table_id:Â str, field_name:Â strÂ =Â 'data_versao', refresh:Â boolÂ =Â False, wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility task for backfilling table in predetermined steps.
Assumes the step sizes will be defined on the .sql file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dbt_client</code></strong> :&ensp;<code>DbtClient</code></dt>
<dd>DBT interface object</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Dataset id on BigQuery</dd>
<dt><strong><code>base_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Base table from which to materialize (usually, an external table)</dd>
<dt><strong><code>mat_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Target table id for materialization</dd>
<dt><strong><code>field_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Key field (column) for dbt incremental filters.</dd>
<dt>Defaults to "data_versao".</dt>
<dt><strong><code>refresh</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, rebuild the table from scratch. Defaults to False.</dd>
<dt><strong><code>wait</code></strong> :&ensp;<code>NoneType</code>, optional</dt>
<dd>Placeholder parameter, used to wait previous tasks finish.</dd>
</dl>
<p>Defaults to None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>whether the table was fully built or not.</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.check_mapped_query_logs_output"><code class="name flex">
<span>def <span class="ident">check_mapped_query_logs_output</span></span>(<span>query_logs_output:Â list[tuple]) â€‘>Â bool</span>
</code></dt>
<dd>
<div class="desc"><p>Task to check if there is recaptures pending</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>query_logs_output</code></strong> :&ensp;<code>list[tuple]</code></dt>
<dd>the return from a mapped query_logs execution</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if there is recaptures to do, otherwise False</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.coalesce_task"><code class="name flex">
<span>def <span class="ident">coalesce_task</span></span>(<span>value_list:Â Iterable)</span>
</code></dt>
<dd>
<div class="desc"><p>Task to get the first non None value of a list</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value_list</code></strong> :&ensp;<code>Iterable</code></dt>
<dd>a iterable object with the values</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>any</code></dt>
<dd>value_list's first non None item</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_date_hour_partition"><code class="name flex">
<span>def <span class="ident">create_date_hour_partition</span></span>(<span>timestamp:Â datetime.datetime, partition_date_name:Â strÂ =Â 'data', partition_date_only:Â boolÂ =Â False) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Create a date (and hour) Hive partition structure from timestamp.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>datetime</code></dt>
<dd>timestamp to be used as reference</dd>
<dt><strong><code>partition_date_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>partition name. Defaults to "data".</dd>
<dt><strong><code>partition_date_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether to add hour partition or not</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>partition string</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_dbt_run_vars"><code class="name flex">
<span>def <span class="ident">create_dbt_run_vars</span></span>(<span>dataset_id:Â str, dbt_vars:Â dict, table_id:Â str, raw_dataset_id:Â str, raw_table_id:Â str, mode:Â str, timestamp:Â datetime.datetime) â€‘>Â tuple[list[dict],Â typing.Union[list[dict],Â dict,Â NoneType],Â bool]</span>
</code></dt>
<dd>
<div class="desc"><p>Create the variables to be used in dbt materialization based on a dict</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>the dataset_id to get the variables</dd>
<dt><strong><code>dbt_vars</code></strong> :&ensp;<code>dict</code></dt>
<dd>dict containing the parameters</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>the table_id get the date_range variable</dd>
<dt><strong><code>raw_dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>the raw_dataset_id get the date_range variable</dd>
<dt><strong><code>raw_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>the raw_table_id get the date_range variable</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>the mode to get the date_range variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[dict]</code></dt>
<dd>the variables to be used in DBT</dd>
<dt><code>Union[list[dict], dict, None]</code></dt>
<dd>the date variable (date_range or run_date)</dd>
<dt><code>bool</code></dt>
<dd>a flag that indicates if the date_range variable came from Redis</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_local_partition_path"><code class="name flex">
<span>def <span class="ident">create_local_partition_path</span></span>(<span>dataset_id:Â str, table_id:Â str, filename:Â str, partitions:Â strÂ =Â None) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Create the full path sctructure which to save data locally before
upload.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Single csv name</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Partitioned directory structure, ie "ano=2022/mes=03/data=01"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>String path having <code>mode</code> and <code>filetype</code> to be replaced afterwards,</dd>
</dl>
<p>either to save raw or staging files.</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.create_request_params"><code class="name flex">
<span>def <span class="ident">create_request_params</span></span>(<span>extract_params:Â dict, table_id:Â str, dataset_id:Â str, timestamp:Â datetime.datetime, interval_minutes:Â int) â€‘>Â tuple[str,Â str]</span>
</code></dt>
<dd>
<div class="desc"><p>Task to create request params</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>extract_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>extract parameters</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>datetime</code></dt>
<dd>timestamp for flow run</dd>
<dt><strong><code>interval_minutes</code></strong> :&ensp;<code>int</code></dt>
<dd>interval in minutes between each capture</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>request_params</code></dt>
<dd>host, database and query to request data</dd>
<dt><code>request_url</code></dt>
<dd>url to request data</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.delay_now_time"><code class="name flex">
<span>def <span class="ident">delay_now_time</span></span>(<span>timestamp:Â str, delay_minutes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>Return timestamp string delayed by <delay_minutes></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>str</code></dt>
<dd>Isoformat timestamp string</dd>
<dt><strong><code>delay_minutes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Minutes to delay timestamp by Defaults to 6.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str </code></dt>
<dd>timestamp string formatted as "%Y-%m-%dT%H-%M-%S"</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.fetch_dataset_sha"><code class="name flex">
<span>def <span class="ident">fetch_dataset_sha</span></span>(<span>dataset_id:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches the SHA of a branch from Github</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_current_timestamp"><code class="name flex">
<span>def <span class="ident">get_current_timestamp</span></span>(<span>timestamp=None, truncate_minute:Â boolÂ =Â True, return_str:Â boolÂ =Â False) â€‘>Â Union[datetime.datetime,Â str]</span>
</code></dt>
<dd>
<div class="desc"><p>Get current timestamp for flow run.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong></dt>
<dd>timestamp to be used as reference (optionally, it can be a string)</dd>
<dt><strong><code>truncate_minute</code></strong></dt>
<dd>whether to truncate the timestamp to the minute or not</dd>
<dt><strong><code>return_str</code></strong></dt>
<dd>if True, the return will be an isoformatted datetime string
otherwise it returns a datetime object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[datetime, str]</code></dt>
<dd>timestamp for flow run</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_join_dict"><code class="name flex">
<span>def <span class="ident">get_join_dict</span></span>(<span>dict_list:Â list, new_dict:Â dict) â€‘>Â List</span>
</code></dt>
<dd>
<div class="desc"><p>Updates a list of dictionaries with a new dictionary.</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_local_dbt_client"><code class="name flex">
<span>def <span class="ident">get_local_dbt_client</span></span>(<span>host:Â str, port:Â int)</span>
</code></dt>
<dd>
<div class="desc"><p>Set a DBT client for running CLI commands. Requires
building container image for your queries repository.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>host</code></strong> :&ensp;<code>str</code></dt>
<dd>hostname. When running locally, usually 'localhost'</dd>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>the port number in which the DBT rpc is running</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DbtClient</code></dt>
<dd>object used to run DBT commands.</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_materialization_date_range"><code class="name flex">
<span>def <span class="ident">get_materialization_date_range</span></span>(<span>dataset_id:Â str, table_id:Â str, raw_dataset_id:Â str, raw_table_id:Â str, table_run_datetime_column_name:Â strÂ =Â None, mode:Â strÂ =Â 'prod', delay_hours:Â intÂ =Â 0, end_ts:Â datetime.datetimeÂ =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Task for generating dict with variables to be passed to the
&ndash;vars argument on DBT.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model filename on the queries repo.</dd>
<dt><strong><code>eg</code></strong></dt>
<dd>if you have a model defined in the file <filename>.sql,</dd>
<dt>the table_id should be <filename></dt>
<dt><strong><code>table_date_column_name</code></strong> :&ensp;<code>Optional, str</code></dt>
<dd>if it's the first time this</dd>
<dt>is ran, will query the table for the maximum value on this field.</dt>
<dt>If rebuild is true, will query the table for the minimum value</dt>
<dt>on this field.</dt>
<dt><strong><code>rebuild</code></strong> :&ensp;<code>Optional, bool</code></dt>
<dd>if true, queries the minimum date value on the</dd>
</dl>
<p>table and return a date range from that value to the datetime.now() time
delay(Optional, int): hours delayed from now time for materialization range
end_ts(Optional, datetime): date range's final date</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>containing date_range_start and date_range_end</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_previous_date"><code class="name flex">
<span>def <span class="ident">get_previous_date</span></span>(<span>days)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the date of {days} days ago in YYYY-MM-DD.</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_raw"><code class="name flex">
<span>def <span class="ident">get_raw</span></span>(<span>url:Â str, headers:Â strÂ =Â None, filetype:Â strÂ =Â 'json', csv_args:Â dictÂ =Â None, params:Â dictÂ =Â None) â€‘>Â Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Request data from URL API</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code></dt>
<dd>URL to send request</dd>
<dt><strong><code>headers</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to headers guardeded on Vault, if needed.</dd>
<dt><strong><code>filetype</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Filetype to be formatted (supported only: json, csv and txt)</dd>
<dt><strong><code>csv_args</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Arguments for read_csv, if needed</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Params to be sent on request</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Containing keys
* <code>data</code> (json): data result
* <code>error</code> (str): catched error, if any. Otherwise, returns None</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_raw_from_sources"><code class="name flex">
<span>def <span class="ident">get_raw_from_sources</span></span>(<span>source_type:Â str, local_filepath:Â str, source_path:Â strÂ =Â None, dataset_id:Â strÂ =Â None, table_id:Â strÂ =Â None, secret_path:Â strÂ =Â None, request_params:Â dictÂ =Â None) â€‘>Â tuple[str,Â str]</span>
</code></dt>
<dd>
<div class="desc"><p>Task to get raw data from sources</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source_type</code></strong> :&ensp;<code>str</code></dt>
<dd>source type</dd>
<dt><strong><code>local_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>local filepath</dd>
<dt><strong><code>source_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>source path. Defaults to None.</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>dataset_id on BigQuery. Defaults to None.</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>table_id on BigQuery. Defaults to None.</dd>
<dt><strong><code>secret_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>secret path. Defaults to None.</dd>
<dt><strong><code>request_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>request parameters. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>error</code></dt>
<dd>error catched from upstream tasks</dd>
<dt><code>filepath</code></dt>
<dd>filepath to raw data</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_rounded_timestamp"><code class="name flex">
<span>def <span class="ident">get_rounded_timestamp</span></span>(<span>timestamp:Â Union[str,Â datetime.datetime,Â ForwardRef(None)]Â =Â None, interval_minutes:Â Optional[int]Â =Â None) â€‘>Â datetime.datetime</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate rounded timestamp for flow run.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>Union[str, datetime, None]</code></dt>
<dd>timestamp to be used as reference</dd>
<dt><strong><code>interval_minutes</code></strong> :&ensp;<code>Union[int, None]</code>, optional</dt>
<dd>interval in minutes between each recapture</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>datetime</code></dt>
<dd>timestamp for flow run</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_run_dates"><code class="name flex">
<span>def <span class="ident">get_run_dates</span></span>(<span>date_range_start:Â str, date_range_end:Â str, day_datetime:Â datetime.datetimeÂ =Â None) â€‘>Â List</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a list of dates between date_range_start and date_range_end.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>date_range_start</code></strong> :&ensp;<code>str</code></dt>
<dd>the start date to create the date range</dd>
<dt><strong><code>date_range_end</code></strong> :&ensp;<code>str</code></dt>
<dd>the end date to create the date range</dd>
<dt><strong><code>day_datetime</code></strong> :&ensp;<code>datetime, Optional</code></dt>
<dd>a timestamp to use as run_date
if the range start or end is False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>the list of run_dates</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.get_scheduled_start_times"><code class="name flex">
<span>def <span class="ident">get_scheduled_start_times</span></span>(<span>timestamp:Â datetime.datetime, parameters:Â list, intervals:Â Optional[None]Â =Â None)</span>
</code></dt>
<dd>
<div class="desc"><p>Task to get start times to schedule flows</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>datetime</code></dt>
<dd>initial flow run timestamp</dd>
<dt><strong><code>parameters</code></strong> :&ensp;<code>list</code></dt>
<dd>parameters for the flow</dd>
<dt><strong><code>intervals</code></strong> :&ensp;<code>Union[None, dict]</code>, optional</dt>
<dd>intervals between each flow run. Defaults to None.
Optionally, you can pass specific intervals for some table_ids.
Suggests to pass intervals based on previous table observed execution times.
Defaults to dict(default=timedelta(minutes=2)).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[datetime]</code></dt>
<dd>list of scheduled start times</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.parse_timestamp_to_string"><code class="name flex">
<span>def <span class="ident">parse_timestamp_to_string</span></span>(<span>timestamp:Â datetime.datetime, pattern='%Y-%m-%d-%H-%M-%S') â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Parse timestamp to string pattern.</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.query_logs"><code class="name flex">
<span>def <span class="ident">query_logs</span></span>(<span>dataset_id:Â str, table_id:Â str, datetime_filter=None, max_recaptures:Â intÂ =Â 90, interval_minutes:Â intÂ =Â 1, recapture_window_days:Â intÂ =Â 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Queries capture logs to check for errors</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt>datetime_filter (pendulum.datetime.DateTime, optional):</dt>
<dt>filter passed to query. This task will query the logs table</dt>
<dt>for the last n (n = recapture_window_days) days before datetime_filter</dt>
<dt><strong><code>max_recaptures</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>maximum number of recaptures to be done</dd>
<dt><strong><code>interval_minutes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>interval in minutes between each recapture</dd>
<dt><strong><code>recapture_window_days</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of days to query for erros</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>lists</code></dt>
<dd>errors (bool),</dd>
</dl>
<p>timestamps (list of pendulum.datetime.DateTime),
previous_errors (list of previous errors)</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.save_raw_local"><code class="name flex">
<span>def <span class="ident">save_raw_local</span></span>(<span>file_path:Â str, status:Â dict, mode:Â strÂ =Â 'raw') â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Saves json response from API to .json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path which to save raw file</dd>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code></dt>
<dd>Must contain keys
* data: json returned from API
* error: error catched from API request</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Folder to save locally, later folder which to upload to GCS.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Path to the saved file</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.save_treated_local"><code class="name flex">
<span>def <span class="ident">save_treated_local</span></span>(<span>file_path:Â str, status:Â dict, mode:Â strÂ =Â 'staging') â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Save treated file to CSV.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path which to save treated file</dd>
<dt><strong><code>status</code></strong> :&ensp;<code>dict</code></dt>
<dd>Must contain keys
* <code>data</code>: dataframe returned from treatement
* <code>error</code>: error catched from data treatement</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Folder to save locally, later folder which to upload to GCS.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Path to the saved file</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.set_last_run_timestamp"><code class="name flex">
<span>def <span class="ident">set_last_run_timestamp</span></span>(<span>dataset_id:Â str, table_id:Â str, timestamp:Â str, mode:Â strÂ =Â 'prod', wait=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the <code>last_run_timestamp</code> key for the dataset_id/table_id pair
to datetime.now() time. Used after running a materialization to set the
stage for the next to come</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model filename on the queries repo.</dd>
<dt><strong><code>timestamp</code></strong></dt>
<dd>Last run timestamp end.</dd>
<dt><strong><code>wait</code></strong> :&ensp;<code>Any</code>, optional</dt>
<dd>Used for defining dependencies inside the flow,</dd>
</dl>
<p>in general, pass the output of the task which should be run imediately
before this. Defaults to None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.transform_raw_to_nested_structure"><code class="name flex">
<span>def <span class="ident">transform_raw_to_nested_structure</span></span>(<span>raw_filepath:Â str, filepath:Â str, error:Â str, timestamp:Â datetime.datetime, primary_key:Â listÂ =Â None, flag_private_data:Â boolÂ =Â False, reader_args:Â dictÂ =Â None) â€‘>Â tuple[str,Â str]</span>
</code></dt>
<dd>
<div class="desc"><p>Task to transform raw data to nested structure</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>raw_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved raw .json file</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved treated .csv file</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>str</code></dt>
<dd>Error catched from upstream tasks</dd>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>datetime</code></dt>
<dd>timestamp for flow run</dd>
<dt><strong><code>primary_key</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Primary key to be used on nested structure</dd>
<dt><strong><code>flag_private_data</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Flag to indicate if the task should log the data</dd>
<dt><strong><code>reader_args</code></strong> :&ensp;<code>dict</code></dt>
<dd>arguments to pass to pandas.read_csv or read_json</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Error traceback</dd>
<dt><code>str</code></dt>
<dd>Path to the saved treated .csv file</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.unpack_mapped_results_nout2"><code class="name flex">
<span>def <span class="ident">unpack_mapped_results_nout2</span></span>(<span>mapped_results:Â Iterable) â€‘>Â tuple[list[typing.Any],Â list[typing.Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Task to unpack the results from an nout=2 tasks in 2 lists when it is mapped</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mapped_results</code></strong> :&ensp;<code>Iterable</code></dt>
<dd>The mapped task return</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[list[Any], list[Any]]</code></dt>
<dd>The task original return splited in 2 lists:
- 1st list being all the first return
- 2nd list being all the second return</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.upload_logs_to_bq"><code class="name flex">
<span>def <span class="ident">upload_logs_to_bq</span></span>(<span>dataset_id:Â str, parent_table_id:Â str, timestamp:Â str, error:Â strÂ =Â None, previous_error:Â strÂ =Â None, recapture:Â boolÂ =Â False)</span>
</code></dt>
<dd>
<div class="desc"><p>Upload execution status table to BigQuery.
Table is uploaded to the same dataset, named {parent_table_id}_logs.
If passing status_dict, should not pass timestamp and error.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>parent_table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Parent table id related to the status table</dd>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>str</code></dt>
<dd>ISO formatted timestamp string</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>String associated with error caught during execution</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.upload_raw_data_to_gcs"><code class="name flex">
<span>def <span class="ident">upload_raw_data_to_gcs</span></span>(<span>error:Â str, raw_filepath:Â str, table_id:Â str, dataset_id:Â str, partitions:Â list, bucket_name:Â strÂ =Â None) â€‘>Â Optional[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Upload raw data to GCS.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>error</code></strong> :&ensp;<code>str</code></dt>
<dd>Error catched from upstream tasks.</dd>
<dt><strong><code>raw_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved raw .json file</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>list</code></dt>
<dd>list of partition strings</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[str, None]</code></dt>
<dd>if there is an error returns it traceback, otherwise returns None</dd>
</dl></div>
</dd>
<dt id="pipelines.rj_smtr.tasks.upload_staging_data_to_gcs"><code class="name flex">
<span>def <span class="ident">upload_staging_data_to_gcs</span></span>(<span>error:Â str, staging_filepath:Â str, timestamp:Â datetime.datetime, table_id:Â str, dataset_id:Â str, partitions:Â str, previous_error:Â strÂ =Â None, recapture:Â boolÂ =Â False, bucket_name:Â strÂ =Â None) â€‘>Â Optional[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Upload staging data to GCS.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>error</code></strong> :&ensp;<code>str</code></dt>
<dd>Error catched from upstream tasks.</dd>
<dt><strong><code>staging_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the saved treated .csv file.</dd>
<dt><strong><code>timestamp</code></strong> :&ensp;<code>datetime</code></dt>
<dd>timestamp for flow run.</dd>
<dt><strong><code>table_id</code></strong> :&ensp;<code>str</code></dt>
<dd>table_id on BigQuery.</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>str</code></dt>
<dd>dataset_id on BigQuery.</dd>
<dt><strong><code>partitions</code></strong> :&ensp;<code>str</code></dt>
<dd>partition string.</dd>
<dt><strong><code>previous_error</code></strong> :&ensp;<code>str, Optional</code></dt>
<dd>Previous error on recaptures.</dd>
<dt><strong><code>recapture</code></strong></dt>
<dd>(bool, Optional): Flag that indicates if the run is recapture or not.</dd>
<dt><strong><code>bucket_name</code></strong> :&ensp;<code>str, Optional</code></dt>
<dd>The bucket name to save the data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[str, None]</code></dt>
<dd>if there is an error returns it traceback, otherwise returns None</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.css" integrity="sha512-b+T2i3P45i1LZM7I00Ci5QquB9szqaxu+uuk5TUSGjZQ4w4n+qujQiIuvTv2BxE7WCGQCifNMksyKILDiHzsOg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.js" integrity="sha512-2B9/byNV1KKRm5nQ2RLViPFD6U4dUjDGwuW1GU+ImJh8YinPU9Zlq1GzdTMO+G2ROrB5o1qasJBy1ttYz0wCug==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelines.rj_smtr" href="index.html">pipelines.rj_smtr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipelines.rj_smtr.tasks.bq_upload" href="#pipelines.rj_smtr.tasks.bq_upload">bq_upload</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.bq_upload_from_dict" href="#pipelines.rj_smtr.tasks.bq_upload_from_dict">bq_upload_from_dict</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.build_incremental_model" href="#pipelines.rj_smtr.tasks.build_incremental_model">build_incremental_model</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.check_mapped_query_logs_output" href="#pipelines.rj_smtr.tasks.check_mapped_query_logs_output">check_mapped_query_logs_output</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.coalesce_task" href="#pipelines.rj_smtr.tasks.coalesce_task">coalesce_task</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_date_hour_partition" href="#pipelines.rj_smtr.tasks.create_date_hour_partition">create_date_hour_partition</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_dbt_run_vars" href="#pipelines.rj_smtr.tasks.create_dbt_run_vars">create_dbt_run_vars</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_local_partition_path" href="#pipelines.rj_smtr.tasks.create_local_partition_path">create_local_partition_path</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.create_request_params" href="#pipelines.rj_smtr.tasks.create_request_params">create_request_params</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.delay_now_time" href="#pipelines.rj_smtr.tasks.delay_now_time">delay_now_time</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.fetch_dataset_sha" href="#pipelines.rj_smtr.tasks.fetch_dataset_sha">fetch_dataset_sha</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_current_timestamp" href="#pipelines.rj_smtr.tasks.get_current_timestamp">get_current_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_join_dict" href="#pipelines.rj_smtr.tasks.get_join_dict">get_join_dict</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_local_dbt_client" href="#pipelines.rj_smtr.tasks.get_local_dbt_client">get_local_dbt_client</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_materialization_date_range" href="#pipelines.rj_smtr.tasks.get_materialization_date_range">get_materialization_date_range</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_previous_date" href="#pipelines.rj_smtr.tasks.get_previous_date">get_previous_date</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_raw" href="#pipelines.rj_smtr.tasks.get_raw">get_raw</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_raw_from_sources" href="#pipelines.rj_smtr.tasks.get_raw_from_sources">get_raw_from_sources</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_rounded_timestamp" href="#pipelines.rj_smtr.tasks.get_rounded_timestamp">get_rounded_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_run_dates" href="#pipelines.rj_smtr.tasks.get_run_dates">get_run_dates</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.get_scheduled_start_times" href="#pipelines.rj_smtr.tasks.get_scheduled_start_times">get_scheduled_start_times</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.parse_timestamp_to_string" href="#pipelines.rj_smtr.tasks.parse_timestamp_to_string">parse_timestamp_to_string</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.query_logs" href="#pipelines.rj_smtr.tasks.query_logs">query_logs</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.save_raw_local" href="#pipelines.rj_smtr.tasks.save_raw_local">save_raw_local</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.save_treated_local" href="#pipelines.rj_smtr.tasks.save_treated_local">save_treated_local</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.set_last_run_timestamp" href="#pipelines.rj_smtr.tasks.set_last_run_timestamp">set_last_run_timestamp</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.transform_raw_to_nested_structure" href="#pipelines.rj_smtr.tasks.transform_raw_to_nested_structure">transform_raw_to_nested_structure</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.unpack_mapped_results_nout2" href="#pipelines.rj_smtr.tasks.unpack_mapped_results_nout2">unpack_mapped_results_nout2</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.upload_logs_to_bq" href="#pipelines.rj_smtr.tasks.upload_logs_to_bq">upload_logs_to_bq</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.upload_raw_data_to_gcs" href="#pipelines.rj_smtr.tasks.upload_raw_data_to_gcs">upload_raw_data_to_gcs</a></code></li>
<li><code><a title="pipelines.rj_smtr.tasks.upload_staging_data_to_gcs" href="#pipelines.rj_smtr.tasks.upload_staging_data_to_gcs">upload_staging_data_to_gcs</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
